{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_legacy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J3b5BlsbB3U",
        "colab_type": "text"
      },
      "source": [
        "pytorch에서 제공하는 TransformerEncoder, TransformerDecoder를 이용해서 transformer 구현한 예제\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw_nvMs1cNZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
        "from torch.nn.modules import LayerNorm\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torchtext.data import Dataset\n",
        "import math\n",
        "import os\n",
        "import io\n",
        "from torchtext.utils import download_from_url\n",
        "from torchtext.vocab import Vocab\n",
        "from collections import Counter\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxg5o49cdhux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_DIM = 512\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 6\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 8\n",
        "SEQ_LEN = 50"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQqWV_dEb7zP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionEmbed(nn.Module):\n",
        "  def __init__(self, d_model, max_seq_length):\n",
        "    super().__init__()\n",
        "    self.pos_embed = nn.Embedding(max_seq_length, d_model)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\n",
        "    pos_mask = inputs.eq(1)\n",
        "    positions.masked_fill_(pos_mask, 0)\n",
        "    pos_embed = self.pos_embed(positions)\n",
        "\n",
        "    return pos_embed\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUVxZFGgcq05",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "37373bbe-e65c-4de6-e4c8-2299b9c18503"
      },
      "source": [
        "inputs = torch.randint(0, 100, (1, 10))\n",
        "pos_emb = PositionEmbed(MODEL_DIM, 100)\n",
        "embed = pos_emb(inputs)\n",
        "embed"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 2.0831,  1.5158, -1.0150,  ..., -0.2912,  0.3344, -0.6084],\n",
              "         [-0.7239,  0.3984, -0.2242,  ...,  1.2703,  1.4499,  1.5684],\n",
              "         [-0.3978, -0.8903,  0.7149,  ...,  1.2819,  0.4226,  1.5349],\n",
              "         ...,\n",
              "         [ 0.0064, -0.3452, -0.1066,  ..., -2.5091, -0.8501,  0.2440],\n",
              "         [ 2.0557, -0.6697,  0.9876,  ..., -1.8837, -0.1613, -0.4974],\n",
              "         [-0.0996, -0.0515,  0.5646,  ..., -1.1756, -1.2993, -2.7639]]],\n",
              "       grad_fn=<EmbeddingBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmV-BPCtw6RU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size_src, vocab_size_trg, d_model, nhead, num_layers, dim_feedforward, dropout, max_seq_length=100):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = max_seq_length\n",
        "        self.embed_src = nn.Embedding(vocab_size_src, d_model)\n",
        "        self.embed_trg = nn.Embedding(vocab_size_trg, d_model)\n",
        "        self.pos_embed = PositionEmbed(d_model, max_seq_length * 2)\n",
        "\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout=0.1, activation=\"relu\")      \n",
        "        encoder_norm = LayerNorm(d_model)\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_layers, encoder_norm)\n",
        "\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout=0.1, activation='relu')\n",
        "        decoder_norm = LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(decoder_layer, num_layers, decoder_norm)\n",
        "\n",
        "        self.fc = nn.Sequential(nn.Linear(d_model, vocab_size_trg),\n",
        "                                nn.LogSoftmax(dim=-1))\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n",
        "\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                xavier_uniform_(p)\n",
        "\n",
        "    def rearrange(self, x):\n",
        "      return torch.transpose(x, 0, 1)\n",
        "    \n",
        "    def get_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
        "        src = self.rearrange(src)  # (batch_size, seq_len) => (seq_len, batch_size)\n",
        "        tgt = self.rearrange(tgt)  # (batch_size, seq_len) => (seq_len, batch_size)\n",
        "        if src.size(1) != tgt.size(1):\n",
        "          raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
        "\n",
        "        src = self.embed_src(src) + self.pos_embed(src)\n",
        "        tgt = self.embed_trg(tgt) + self.pos_embed(tgt)\n",
        "        src_mask = self.get_mask(src.size()[0]).to(src.device)\n",
        "        tgt_mask = self.get_mask(tgt.size()[0]).to(tgt.device)\n",
        "        if src_key_padding_mask is not None:\n",
        "          memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "        else:\n",
        "          memory_key_padding_mask = None\n",
        "\n",
        "        self.encoder_output = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        decoder_output = self.decoder(tgt, self.encoder_output, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "        decoder_output = self.fc(decoder_output)\n",
        "        decoder_output = self.rearrange(decoder_output)  # (seq_len, batch_size, vocab_size) = > (batch_size, seq_len, vocab_size)\n",
        "        return decoder_output\n",
        "\n",
        "    def predict(self, src, src_key_padding_mask=None):\n",
        "      src = self.rearrange(src)  # (batch_size, seq_len) => (seq_len, batch_size)\n",
        "      tgt = torch.ones([1, src.size()[1]], dtype=torch.long).to(src.device)\n",
        "\n",
        "      src = self.embed_src(src) + self.pos_embed(src)\n",
        "      tgt = self.embed_trg(tgt) + self.pos_embed(tgt)\n",
        "      src_mask = self.get_mask(src.size()[0]).to(src.device)\n",
        "      if src_key_padding_mask is not None:\n",
        "        memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "      else:\n",
        "        memory_key_padding_mask = None\n",
        "\n",
        "      self.encoder_output = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
        "      decoder_input = tgt\n",
        "      for i in range(self.seq_len):\n",
        "        tgt_mask = self.get_mask(decoder_input.size()[0]).to(tgt.device)\n",
        "        decoder_output = self.decoder(decoder_input, self.encoder_output, tgt_mask=tgt_mask, memory_key_padding_mask=memory_key_padding_mask)  # (seq_len, batch_size, d_model)\n",
        "        decoder_output = self.fc(decoder_output)\n",
        "        decoder_max = torch.argmax(decoder_output, dim=-1)\n",
        "        decoder_max = self.embed_trg(decoder_max) + self.pos_embed(decoder_max)\n",
        "        decoder_input = torch.cat([decoder_input, decoder_max[-1:, :, :]])\n",
        "    \n",
        "      decoder_output = self.rearrange(decoder_output)  # (seq_len, batch_size, vocab_size) = > (batch_size, batch_size, vocab_size)\n",
        "      return decoder_output\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsoHPrzzsoHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download(url, out_path=None):\n",
        "  filename = os.path.basename(url)\n",
        "  if out_path is None:\n",
        "    path = '.data'\n",
        "  else:\n",
        "    path = out_path\n",
        "  zpath = os.path.join(path, filename)\n",
        "  if not os.path.isfile(zpath):\n",
        "      if not os.path.exists(os.path.dirname(zpath)):\n",
        "          os.makedirs(os.path.dirname(zpath))\n",
        "      print('downloading {}'.format(filename))\n",
        "      download_from_url(url, zpath)\n",
        "  zroot, ext = os.path.splitext(zpath)\n",
        "  _, ext_inner = os.path.splitext(zroot)\n",
        "  if ext == '.zip':\n",
        "      with zipfile.ZipFile(zpath, 'r') as zfile:\n",
        "          print('extracting')\n",
        "          zfile.extractall(path)\n",
        "  # tarfile cannot handle bare .gz files\n",
        "  elif ext == '.tgz' or ext == '.gz' and ext_inner == '.tar':\n",
        "      with tarfile.open(zpath, 'r:gz') as tar:\n",
        "          dirs = [member for member in tar.getmembers()]\n",
        "          tar.extractall(path=path, members=dirs)\n",
        "  elif ext == '.gz':\n",
        "      with gzip.open(zpath, 'rb') as gz:\n",
        "          with open(zroot, 'wb') as uncompressed:\n",
        "              shutil.copyfileobj(gz, uncompressed)\n",
        "\n",
        "  return os.path.dirname(zpath)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wz4Vx1_uVPa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "02767a8f-9e6f-452d-bce3-1bac2abd5e54"
      },
      "source": [
        "train_url = 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz'\n",
        "path = download(train_url)\n",
        "src_corpus  = os.path.join(path, 'train.de')\n",
        "trg_corpus  = os.path.join(path, 'train.en')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:04<00:00, 249kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KBGkx0_XFYy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "41de5de5-1fd6-4d9c-d62e-74d3c1f510f9"
      },
      "source": [
        "val_url = 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz'\n",
        "path = download(val_url)\n",
        "val_src_corpus  = os.path.join(path, 'val.de')\n",
        "val_trg_corpus  = os.path.join(path, 'val.en')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 80.9kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb0V5FQDuyQW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb1e6cdb-50f2-4259-bb04-87b4483456f4"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rGZ8kJZwJ8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CorpusDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Defines a dataset for machine translation.\"\"\"\n",
        "    def __init__(self, src_path, trg_path, seq_len):\n",
        "      super().__init__()\n",
        "      self.src_lines = self.get_lines(src_path)\n",
        "      self.trg_lines = self.get_lines(trg_path)\n",
        "\n",
        "      assert len(self.src_lines) == len(self.trg_lines)\n",
        "\n",
        "      self.src_vocab = self.build_vocab(self.src_lines)\n",
        "      self.trg_vocab = self.build_vocab(self.trg_lines)\n",
        "      self.seq_len = seq_len\n",
        "      self.sos = self.src_vocab.stoi['<sos>']\n",
        "      self.eos = self.src_vocab.stoi['<eos>']\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.src_lines)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "      src_sent = self.src_lines[item]\n",
        "      trg_sent = self.trg_lines[item]\n",
        "\n",
        "      words = src_sent.split()\n",
        "      tokens = self.word2tokens(self.src_vocab, words) \n",
        "      trg_words  = trg_sent.split()\n",
        "      trg_tokens = self.word2tokens(self.trg_vocab, trg_words, self.eos)\n",
        "      trg_tokens = [self.sos] + trg_tokens\n",
        "      return (torch.tensor(tokens).to(device), torch.tensor(trg_tokens).to(device))\n",
        "\n",
        "    def build_vocab(self, lines):\n",
        "      counter = Counter()\n",
        "      for line in lines:\n",
        "        tokens = line.split()\n",
        "        counter.update(tokens)\n",
        "      vocab = Vocab(counter, min_freq=1, specials=['<pad>', '<sos>', '<eos>', '<unk>'])\n",
        "\n",
        "      return vocab\n",
        "\n",
        "    def get_lines(self, file_path):\n",
        "      lines = open(file_path, mode='r', encoding='utf-8').readlines()\n",
        "      lines = [line.strip() for line in lines ]\n",
        "\n",
        "      return lines\n",
        "\n",
        "    def word2tokens(self, vocab, words, eos=None):\n",
        "      tokens  = []\n",
        "      for word in words:\n",
        "        token = vocab.stoi[word]\n",
        "        tokens.append(token)\n",
        "\n",
        "      if eos is not None:\n",
        "        tokens = tokens + [eos]\n",
        "      padding = [vocab.stoi['<pad>'] for _ in range(self.seq_len - len(tokens))]\n",
        "      tokens = tokens + padding\n",
        "      return tokens\n",
        "\n",
        "    def tokens2word(self, vocab, tokens):\n",
        "      words = []\n",
        "      for token in tokens:\n",
        "        try:\n",
        "          word = vocab.itos[token]\n",
        "        except Exception:\n",
        "          word = '<unk>'\n",
        "        words.append(word)\n",
        "\n",
        "      return words\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk-X1tBzcoHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokens2words(tokens, dataset, vocab):\n",
        "  words = dataset.tokens2word(vocab, tokens)\n",
        "  for i, word in enumerate(words):\n",
        "    if word == '<eos>':\n",
        "      words = words[:i+1]\n",
        "      break\n",
        "  words = ' '.join(words) \n",
        "  words = words.replace('<pad>', '')\n",
        "  return words"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7kDJoPLMfWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "6b930144-d306-4fb4-db1b-263f6ac3f01d"
      },
      "source": [
        "dataset = CorpusDataset(src_corpus, trg_corpus, seq_len=SEQ_LEN)\n",
        "dataloader = DataLoader(dataset,batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "for (src, tgt) in dataloader:\n",
        "  print(src[0])\n",
        "  print('src=', tokens2words(src[0], dataset, dataset.src_vocab))\n",
        "  print(tgt[0])\n",
        "  print('tgt=', tokens2words(tgt[0], dataset, dataset.trg_vocab))\n",
        "  break"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([    4,    10,    28,    44,     5, 15975,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "       device='cuda:0')\n",
            "src= Ein Mann steht neben einem Obstkarren.                                            \n",
            "tensor([  1,   5,  11,   9,  32,  44,   4, 650, 714,   2,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0], device='cuda:0')\n",
            "tgt= <sos> A man is standing by a fruit cart. <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI_fOXhOXbY7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "ad3ddd82-a0f9-45ee-83e4-dc01d6320c6b"
      },
      "source": [
        "val_dataset = CorpusDataset(val_src_corpus, val_trg_corpus, seq_len=SEQ_LEN)\n",
        "val_dataloader = DataLoader(val_dataset,batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "for src, tgt in val_dataloader:\n",
        "  print(src[0])\n",
        "  print(tgt[0])\n",
        "  break"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  5,  34,   8,   4, 275, 380,  73, 116, 487,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0], device='cuda:0')\n",
            "tensor([   1,    5,   36,    6,    4,  288, 1454,   52, 2322,    2,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfGdRlFg4q0-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd04ef40-d57d-4fdc-8bad-4a14c7c569cf"
      },
      "source": [
        "VOCAB_SIZE_SRC = len(dataset.src_vocab)\n",
        "VOCAB_SIZE_TRG = len(dataset.trg_vocab)\n",
        "VOCAB_SIZE_SRC, VOCAB_SIZE_TRG"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24893, 15460)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oz6KZbbxo2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Transformer(vocab_size_src=VOCAB_SIZE_SRC,\n",
        "                    vocab_size_trg=VOCAB_SIZE_TRG,\n",
        "                    d_model = MODEL_DIM,\n",
        "                    nhead = NUM_HEADS,\n",
        "                    num_layers = NUM_LAYERS,\n",
        "                    dim_feedforward = 2048,\n",
        "                    dropout = 0.1,\n",
        "                    max_seq_length=SEQ_LEN)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpzcIPUmGL8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD_IDX = dataset.src_vocab.stoi['<pad>']\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZowGI9uA8px",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "6f6ca5fd-4842-4694-fb75-676fed270962"
      },
      "source": [
        "for src, tgt in dataloader:\n",
        "  src_key_padding_mask = (src == PAD_IDX).to(src.device)\n",
        "  print(src[0])\n",
        "  print(src_key_padding_mask[0])\n",
        "  \n",
        "  break"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  19, 1541,    8, 2197,  107,    9,   23,  305,   52,   27,  412,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0], device='cuda:0')\n",
            "tensor([False, False, False, False, False, False, False, False, False, False,\n",
            "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aikmoqm1MkzR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "10ecfff3-2af0-448d-a1c5-a0f1d248c660"
      },
      "source": [
        "model.eval()\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "with torch.no_grad():\n",
        "  for src, tgt in dataloader:\n",
        "    src_key_padding_mask = (src == PAD_IDX).to(src.device)\n",
        "    outputs = model.predict(src, src_key_padding_mask=src_key_padding_mask)\n",
        "    print(outputs[0])\n",
        "    break"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ -9.5506,  -9.6070,  -9.4486,  ...,  -9.2964, -10.1216,  -9.5001],\n",
            "        [ -9.6494,  -9.3505,  -9.3675,  ...,  -9.2720, -10.0462,  -9.3168],\n",
            "        [ -9.6610,  -9.2874,  -9.3991,  ...,  -9.0884, -10.2051,  -9.1475],\n",
            "        ...,\n",
            "        [ -9.7248,  -9.0629,  -9.2523,  ...,  -9.1900, -10.0133,  -9.0814],\n",
            "        [ -9.7250,  -9.0620,  -9.2519,  ...,  -9.1894, -10.0123,  -9.0812],\n",
            "        [ -9.7253,  -9.0611,  -9.2515,  ...,  -9.1888, -10.0114,  -9.0810]],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZlgSG6mpX0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "  torch.manual_seed(2147483647)\n",
        "  if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "  model.train()\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    for step , (src, tgt) in enumerate(dataloader):\n",
        "      optimizer.zero_grad()\n",
        "      # src_key_padding_mask = (src == PAD_IDX).to(src.device)\n",
        "      # tgt_key_padding_mask = (tgt[:, :-1] == PAD_IDX).to(tgt.device)\n",
        "      src_key_padding_mask=None\n",
        "      tgt_key_padding_mask=None\n",
        "      outputs = model(src, tgt[:, :-1], src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
        "      outputs = torch.transpose(outputs, 1, 2)  # (batch, seq_len, vocab_size)  => (batch, vocab_size, seq_len)\n",
        "      loss = criterion(outputs, tgt[:, 1:])\n",
        "      print('epoch: {}, step: {}, loss: {:.3f}'.format(epoch + 1, step + 1, loss))\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "      optimizer.step()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCGsCiOG2ir_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c9517821-17e8-4b78-c422-5a9185b9d647"
      },
      "source": [
        "train()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, step: 1, loss: 9.879\n",
            "epoch: 1, step: 2, loss: 6.006\n",
            "epoch: 1, step: 3, loss: 3.969\n",
            "epoch: 1, step: 4, loss: 3.130\n",
            "epoch: 1, step: 5, loss: 2.961\n",
            "epoch: 1, step: 6, loss: 3.091\n",
            "epoch: 1, step: 7, loss: 2.987\n",
            "epoch: 1, step: 8, loss: 2.846\n",
            "epoch: 1, step: 9, loss: 2.771\n",
            "epoch: 1, step: 10, loss: 2.817\n",
            "epoch: 1, step: 11, loss: 2.874\n",
            "epoch: 1, step: 12, loss: 2.700\n",
            "epoch: 1, step: 13, loss: 2.785\n",
            "epoch: 1, step: 14, loss: 2.759\n",
            "epoch: 1, step: 15, loss: 2.596\n",
            "epoch: 1, step: 16, loss: 2.677\n",
            "epoch: 1, step: 17, loss: 2.622\n",
            "epoch: 1, step: 18, loss: 2.585\n",
            "epoch: 1, step: 19, loss: 2.482\n",
            "epoch: 1, step: 20, loss: 2.426\n",
            "epoch: 1, step: 21, loss: 2.263\n",
            "epoch: 1, step: 22, loss: 2.268\n",
            "epoch: 1, step: 23, loss: 2.076\n",
            "epoch: 1, step: 24, loss: 2.122\n",
            "epoch: 1, step: 25, loss: 2.086\n",
            "epoch: 1, step: 26, loss: 2.097\n",
            "epoch: 1, step: 27, loss: 2.081\n",
            "epoch: 1, step: 28, loss: 1.981\n",
            "epoch: 1, step: 29, loss: 2.038\n",
            "epoch: 1, step: 30, loss: 2.069\n",
            "epoch: 1, step: 31, loss: 2.011\n",
            "epoch: 1, step: 32, loss: 1.994\n",
            "epoch: 1, step: 33, loss: 1.997\n",
            "epoch: 1, step: 34, loss: 1.951\n",
            "epoch: 1, step: 35, loss: 1.924\n",
            "epoch: 1, step: 36, loss: 1.905\n",
            "epoch: 1, step: 37, loss: 1.982\n",
            "epoch: 1, step: 38, loss: 1.974\n",
            "epoch: 1, step: 39, loss: 1.774\n",
            "epoch: 1, step: 40, loss: 1.833\n",
            "epoch: 1, step: 41, loss: 1.737\n",
            "epoch: 1, step: 42, loss: 1.798\n",
            "epoch: 1, step: 43, loss: 1.830\n",
            "epoch: 1, step: 44, loss: 1.763\n",
            "epoch: 1, step: 45, loss: 1.722\n",
            "epoch: 1, step: 46, loss: 1.715\n",
            "epoch: 1, step: 47, loss: 1.787\n",
            "epoch: 1, step: 48, loss: 1.728\n",
            "epoch: 1, step: 49, loss: 1.663\n",
            "epoch: 1, step: 50, loss: 1.724\n",
            "epoch: 1, step: 51, loss: 1.665\n",
            "epoch: 1, step: 52, loss: 1.670\n",
            "epoch: 1, step: 53, loss: 1.622\n",
            "epoch: 1, step: 54, loss: 1.557\n",
            "epoch: 1, step: 55, loss: 1.632\n",
            "epoch: 1, step: 56, loss: 1.718\n",
            "epoch: 1, step: 57, loss: 1.728\n",
            "epoch: 1, step: 58, loss: 1.644\n",
            "epoch: 1, step: 59, loss: 1.711\n",
            "epoch: 1, step: 60, loss: 1.665\n",
            "epoch: 1, step: 61, loss: 1.684\n",
            "epoch: 1, step: 62, loss: 1.725\n",
            "epoch: 1, step: 63, loss: 1.632\n",
            "epoch: 1, step: 64, loss: 1.578\n",
            "epoch: 1, step: 65, loss: 1.628\n",
            "epoch: 1, step: 66, loss: 1.713\n",
            "epoch: 1, step: 67, loss: 1.490\n",
            "epoch: 1, step: 68, loss: 1.606\n",
            "epoch: 1, step: 69, loss: 1.538\n",
            "epoch: 1, step: 70, loss: 1.543\n",
            "epoch: 1, step: 71, loss: 1.643\n",
            "epoch: 1, step: 72, loss: 1.664\n",
            "epoch: 1, step: 73, loss: 1.520\n",
            "epoch: 1, step: 74, loss: 1.544\n",
            "epoch: 1, step: 75, loss: 1.491\n",
            "epoch: 1, step: 76, loss: 1.564\n",
            "epoch: 1, step: 77, loss: 1.432\n",
            "epoch: 1, step: 78, loss: 1.603\n",
            "epoch: 1, step: 79, loss: 1.487\n",
            "epoch: 1, step: 80, loss: 1.550\n",
            "epoch: 1, step: 81, loss: 1.427\n",
            "epoch: 1, step: 82, loss: 1.482\n",
            "epoch: 1, step: 83, loss: 1.565\n",
            "epoch: 1, step: 84, loss: 1.573\n",
            "epoch: 1, step: 85, loss: 1.599\n",
            "epoch: 1, step: 86, loss: 1.472\n",
            "epoch: 1, step: 87, loss: 1.452\n",
            "epoch: 1, step: 88, loss: 1.646\n",
            "epoch: 1, step: 89, loss: 1.565\n",
            "epoch: 1, step: 90, loss: 1.594\n",
            "epoch: 1, step: 91, loss: 1.534\n",
            "epoch: 1, step: 92, loss: 1.423\n",
            "epoch: 1, step: 93, loss: 1.526\n",
            "epoch: 1, step: 94, loss: 1.526\n",
            "epoch: 1, step: 95, loss: 1.494\n",
            "epoch: 1, step: 96, loss: 1.538\n",
            "epoch: 1, step: 97, loss: 1.549\n",
            "epoch: 1, step: 98, loss: 1.526\n",
            "epoch: 1, step: 99, loss: 1.438\n",
            "epoch: 1, step: 100, loss: 1.443\n",
            "epoch: 1, step: 101, loss: 1.450\n",
            "epoch: 1, step: 102, loss: 1.403\n",
            "epoch: 1, step: 103, loss: 1.585\n",
            "epoch: 1, step: 104, loss: 1.456\n",
            "epoch: 1, step: 105, loss: 1.588\n",
            "epoch: 1, step: 106, loss: 1.455\n",
            "epoch: 1, step: 107, loss: 1.447\n",
            "epoch: 1, step: 108, loss: 1.546\n",
            "epoch: 1, step: 109, loss: 1.517\n",
            "epoch: 1, step: 110, loss: 1.472\n",
            "epoch: 1, step: 111, loss: 1.475\n",
            "epoch: 1, step: 112, loss: 1.540\n",
            "epoch: 1, step: 113, loss: 1.524\n",
            "epoch: 1, step: 114, loss: 1.468\n",
            "epoch: 1, step: 115, loss: 1.417\n",
            "epoch: 1, step: 116, loss: 1.477\n",
            "epoch: 1, step: 117, loss: 1.337\n",
            "epoch: 1, step: 118, loss: 1.572\n",
            "epoch: 1, step: 119, loss: 1.472\n",
            "epoch: 1, step: 120, loss: 1.383\n",
            "epoch: 1, step: 121, loss: 1.402\n",
            "epoch: 1, step: 122, loss: 1.469\n",
            "epoch: 1, step: 123, loss: 1.581\n",
            "epoch: 1, step: 124, loss: 1.519\n",
            "epoch: 1, step: 125, loss: 1.487\n",
            "epoch: 1, step: 126, loss: 1.369\n",
            "epoch: 1, step: 127, loss: 1.400\n",
            "epoch: 1, step: 128, loss: 1.412\n",
            "epoch: 1, step: 129, loss: 1.363\n",
            "epoch: 1, step: 130, loss: 1.452\n",
            "epoch: 1, step: 131, loss: 1.400\n",
            "epoch: 1, step: 132, loss: 1.501\n",
            "epoch: 1, step: 133, loss: 1.561\n",
            "epoch: 1, step: 134, loss: 1.405\n",
            "epoch: 1, step: 135, loss: 1.538\n",
            "epoch: 1, step: 136, loss: 1.352\n",
            "epoch: 1, step: 137, loss: 1.436\n",
            "epoch: 1, step: 138, loss: 1.392\n",
            "epoch: 1, step: 139, loss: 1.284\n",
            "epoch: 1, step: 140, loss: 1.537\n",
            "epoch: 1, step: 141, loss: 1.438\n",
            "epoch: 1, step: 142, loss: 1.397\n",
            "epoch: 1, step: 143, loss: 1.519\n",
            "epoch: 1, step: 144, loss: 1.361\n",
            "epoch: 1, step: 145, loss: 1.292\n",
            "epoch: 1, step: 146, loss: 1.407\n",
            "epoch: 1, step: 147, loss: 1.497\n",
            "epoch: 1, step: 148, loss: 1.393\n",
            "epoch: 1, step: 149, loss: 1.455\n",
            "epoch: 1, step: 150, loss: 1.488\n",
            "epoch: 1, step: 151, loss: 1.415\n",
            "epoch: 1, step: 152, loss: 1.501\n",
            "epoch: 1, step: 153, loss: 1.354\n",
            "epoch: 1, step: 154, loss: 1.421\n",
            "epoch: 1, step: 155, loss: 1.404\n",
            "epoch: 1, step: 156, loss: 1.433\n",
            "epoch: 1, step: 157, loss: 1.475\n",
            "epoch: 1, step: 158, loss: 1.392\n",
            "epoch: 1, step: 159, loss: 1.390\n",
            "epoch: 1, step: 160, loss: 1.307\n",
            "epoch: 1, step: 161, loss: 1.408\n",
            "epoch: 1, step: 162, loss: 1.409\n",
            "epoch: 1, step: 163, loss: 1.342\n",
            "epoch: 1, step: 164, loss: 1.384\n",
            "epoch: 1, step: 165, loss: 1.412\n",
            "epoch: 1, step: 166, loss: 1.357\n",
            "epoch: 1, step: 167, loss: 1.322\n",
            "epoch: 1, step: 168, loss: 1.434\n",
            "epoch: 1, step: 169, loss: 1.376\n",
            "epoch: 1, step: 170, loss: 1.288\n",
            "epoch: 1, step: 171, loss: 1.409\n",
            "epoch: 1, step: 172, loss: 1.404\n",
            "epoch: 1, step: 173, loss: 1.458\n",
            "epoch: 1, step: 174, loss: 1.335\n",
            "epoch: 1, step: 175, loss: 1.415\n",
            "epoch: 1, step: 176, loss: 1.347\n",
            "epoch: 1, step: 177, loss: 1.294\n",
            "epoch: 1, step: 178, loss: 1.343\n",
            "epoch: 1, step: 179, loss: 1.340\n",
            "epoch: 1, step: 180, loss: 1.462\n",
            "epoch: 1, step: 181, loss: 1.295\n",
            "epoch: 1, step: 182, loss: 1.367\n",
            "epoch: 1, step: 183, loss: 1.332\n",
            "epoch: 1, step: 184, loss: 1.355\n",
            "epoch: 1, step: 185, loss: 1.318\n",
            "epoch: 1, step: 186, loss: 1.384\n",
            "epoch: 1, step: 187, loss: 1.389\n",
            "epoch: 1, step: 188, loss: 1.259\n",
            "epoch: 1, step: 189, loss: 1.384\n",
            "epoch: 1, step: 190, loss: 1.438\n",
            "epoch: 1, step: 191, loss: 1.443\n",
            "epoch: 1, step: 192, loss: 1.325\n",
            "epoch: 1, step: 193, loss: 1.345\n",
            "epoch: 1, step: 194, loss: 1.403\n",
            "epoch: 1, step: 195, loss: 1.373\n",
            "epoch: 1, step: 196, loss: 1.390\n",
            "epoch: 1, step: 197, loss: 1.414\n",
            "epoch: 1, step: 198, loss: 1.332\n",
            "epoch: 1, step: 199, loss: 1.404\n",
            "epoch: 1, step: 200, loss: 1.448\n",
            "epoch: 1, step: 201, loss: 1.356\n",
            "epoch: 1, step: 202, loss: 1.270\n",
            "epoch: 1, step: 203, loss: 1.309\n",
            "epoch: 1, step: 204, loss: 1.384\n",
            "epoch: 1, step: 205, loss: 1.358\n",
            "epoch: 1, step: 206, loss: 1.424\n",
            "epoch: 1, step: 207, loss: 1.249\n",
            "epoch: 1, step: 208, loss: 1.396\n",
            "epoch: 1, step: 209, loss: 1.232\n",
            "epoch: 1, step: 210, loss: 1.305\n",
            "epoch: 1, step: 211, loss: 1.364\n",
            "epoch: 1, step: 212, loss: 1.299\n",
            "epoch: 1, step: 213, loss: 1.252\n",
            "epoch: 1, step: 214, loss: 1.327\n",
            "epoch: 1, step: 215, loss: 1.342\n",
            "epoch: 1, step: 216, loss: 1.328\n",
            "epoch: 1, step: 217, loss: 1.314\n",
            "epoch: 1, step: 218, loss: 1.367\n",
            "epoch: 1, step: 219, loss: 1.277\n",
            "epoch: 1, step: 220, loss: 1.248\n",
            "epoch: 1, step: 221, loss: 1.403\n",
            "epoch: 1, step: 222, loss: 1.364\n",
            "epoch: 1, step: 223, loss: 1.225\n",
            "epoch: 1, step: 224, loss: 1.333\n",
            "epoch: 1, step: 225, loss: 1.358\n",
            "epoch: 1, step: 226, loss: 1.260\n",
            "epoch: 1, step: 227, loss: 1.381\n",
            "epoch: 1, step: 228, loss: 1.277\n",
            "epoch: 1, step: 229, loss: 1.324\n",
            "epoch: 1, step: 230, loss: 1.242\n",
            "epoch: 1, step: 231, loss: 1.177\n",
            "epoch: 1, step: 232, loss: 1.292\n",
            "epoch: 1, step: 233, loss: 1.317\n",
            "epoch: 1, step: 234, loss: 1.259\n",
            "epoch: 1, step: 235, loss: 1.370\n",
            "epoch: 1, step: 236, loss: 1.335\n",
            "epoch: 1, step: 237, loss: 1.255\n",
            "epoch: 1, step: 238, loss: 1.363\n",
            "epoch: 1, step: 239, loss: 1.342\n",
            "epoch: 1, step: 240, loss: 1.287\n",
            "epoch: 1, step: 241, loss: 1.279\n",
            "epoch: 1, step: 242, loss: 1.272\n",
            "epoch: 1, step: 243, loss: 1.225\n",
            "epoch: 1, step: 244, loss: 1.432\n",
            "epoch: 1, step: 245, loss: 1.327\n",
            "epoch: 1, step: 246, loss: 1.298\n",
            "epoch: 1, step: 247, loss: 1.392\n",
            "epoch: 1, step: 248, loss: 1.305\n",
            "epoch: 1, step: 249, loss: 1.290\n",
            "epoch: 1, step: 250, loss: 1.228\n",
            "epoch: 1, step: 251, loss: 1.255\n",
            "epoch: 1, step: 252, loss: 1.341\n",
            "epoch: 1, step: 253, loss: 1.304\n",
            "epoch: 1, step: 254, loss: 1.302\n",
            "epoch: 1, step: 255, loss: 1.312\n",
            "epoch: 1, step: 256, loss: 1.275\n",
            "epoch: 1, step: 257, loss: 1.241\n",
            "epoch: 1, step: 258, loss: 1.296\n",
            "epoch: 1, step: 259, loss: 1.298\n",
            "epoch: 1, step: 260, loss: 1.205\n",
            "epoch: 1, step: 261, loss: 1.261\n",
            "epoch: 1, step: 262, loss: 1.230\n",
            "epoch: 1, step: 263, loss: 1.364\n",
            "epoch: 1, step: 264, loss: 1.220\n",
            "epoch: 1, step: 265, loss: 1.189\n",
            "epoch: 1, step: 266, loss: 1.326\n",
            "epoch: 1, step: 267, loss: 1.278\n",
            "epoch: 1, step: 268, loss: 1.374\n",
            "epoch: 1, step: 269, loss: 1.249\n",
            "epoch: 1, step: 270, loss: 1.130\n",
            "epoch: 1, step: 271, loss: 1.217\n",
            "epoch: 1, step: 272, loss: 1.230\n",
            "epoch: 1, step: 273, loss: 1.170\n",
            "epoch: 1, step: 274, loss: 1.211\n",
            "epoch: 1, step: 275, loss: 1.446\n",
            "epoch: 1, step: 276, loss: 1.259\n",
            "epoch: 1, step: 277, loss: 1.237\n",
            "epoch: 1, step: 278, loss: 1.238\n",
            "epoch: 1, step: 279, loss: 1.268\n",
            "epoch: 1, step: 280, loss: 1.204\n",
            "epoch: 1, step: 281, loss: 1.325\n",
            "epoch: 1, step: 282, loss: 1.212\n",
            "epoch: 1, step: 283, loss: 1.294\n",
            "epoch: 1, step: 284, loss: 1.264\n",
            "epoch: 1, step: 285, loss: 1.201\n",
            "epoch: 1, step: 286, loss: 1.265\n",
            "epoch: 1, step: 287, loss: 1.218\n",
            "epoch: 1, step: 288, loss: 1.280\n",
            "epoch: 1, step: 289, loss: 1.248\n",
            "epoch: 1, step: 290, loss: 1.282\n",
            "epoch: 1, step: 291, loss: 1.120\n",
            "epoch: 1, step: 292, loss: 1.182\n",
            "epoch: 1, step: 293, loss: 1.273\n",
            "epoch: 1, step: 294, loss: 1.209\n",
            "epoch: 1, step: 295, loss: 1.297\n",
            "epoch: 1, step: 296, loss: 1.397\n",
            "epoch: 1, step: 297, loss: 1.221\n",
            "epoch: 1, step: 298, loss: 1.201\n",
            "epoch: 1, step: 299, loss: 1.165\n",
            "epoch: 1, step: 300, loss: 1.274\n",
            "epoch: 1, step: 301, loss: 1.180\n",
            "epoch: 1, step: 302, loss: 1.235\n",
            "epoch: 1, step: 303, loss: 1.149\n",
            "epoch: 1, step: 304, loss: 1.067\n",
            "epoch: 1, step: 305, loss: 1.358\n",
            "epoch: 1, step: 306, loss: 1.175\n",
            "epoch: 1, step: 307, loss: 1.311\n",
            "epoch: 1, step: 308, loss: 1.259\n",
            "epoch: 1, step: 309, loss: 1.155\n",
            "epoch: 1, step: 310, loss: 1.224\n",
            "epoch: 1, step: 311, loss: 1.152\n",
            "epoch: 1, step: 312, loss: 1.224\n",
            "epoch: 1, step: 313, loss: 1.310\n",
            "epoch: 1, step: 314, loss: 1.109\n",
            "epoch: 1, step: 315, loss: 1.148\n",
            "epoch: 1, step: 316, loss: 1.215\n",
            "epoch: 1, step: 317, loss: 1.385\n",
            "epoch: 1, step: 318, loss: 1.133\n",
            "epoch: 1, step: 319, loss: 1.158\n",
            "epoch: 1, step: 320, loss: 1.227\n",
            "epoch: 1, step: 321, loss: 1.208\n",
            "epoch: 1, step: 322, loss: 1.260\n",
            "epoch: 1, step: 323, loss: 1.246\n",
            "epoch: 1, step: 324, loss: 1.248\n",
            "epoch: 1, step: 325, loss: 1.167\n",
            "epoch: 1, step: 326, loss: 1.189\n",
            "epoch: 1, step: 327, loss: 1.197\n",
            "epoch: 1, step: 328, loss: 1.197\n",
            "epoch: 1, step: 329, loss: 1.154\n",
            "epoch: 1, step: 330, loss: 1.213\n",
            "epoch: 1, step: 331, loss: 1.283\n",
            "epoch: 1, step: 332, loss: 1.210\n",
            "epoch: 1, step: 333, loss: 1.122\n",
            "epoch: 1, step: 334, loss: 1.190\n",
            "epoch: 1, step: 335, loss: 1.106\n",
            "epoch: 1, step: 336, loss: 1.247\n",
            "epoch: 1, step: 337, loss: 1.243\n",
            "epoch: 1, step: 338, loss: 1.228\n",
            "epoch: 1, step: 339, loss: 1.144\n",
            "epoch: 1, step: 340, loss: 1.133\n",
            "epoch: 1, step: 341, loss: 1.200\n",
            "epoch: 1, step: 342, loss: 1.185\n",
            "epoch: 1, step: 343, loss: 1.176\n",
            "epoch: 1, step: 344, loss: 1.157\n",
            "epoch: 1, step: 345, loss: 1.250\n",
            "epoch: 1, step: 346, loss: 1.125\n",
            "epoch: 1, step: 347, loss: 1.285\n",
            "epoch: 1, step: 348, loss: 1.159\n",
            "epoch: 1, step: 349, loss: 1.299\n",
            "epoch: 1, step: 350, loss: 1.274\n",
            "epoch: 1, step: 351, loss: 1.228\n",
            "epoch: 1, step: 352, loss: 1.184\n",
            "epoch: 1, step: 353, loss: 1.241\n",
            "epoch: 1, step: 354, loss: 1.285\n",
            "epoch: 1, step: 355, loss: 1.208\n",
            "epoch: 1, step: 356, loss: 1.200\n",
            "epoch: 1, step: 357, loss: 1.078\n",
            "epoch: 1, step: 358, loss: 1.296\n",
            "epoch: 1, step: 359, loss: 1.309\n",
            "epoch: 1, step: 360, loss: 1.081\n",
            "epoch: 1, step: 361, loss: 1.229\n",
            "epoch: 1, step: 362, loss: 1.242\n",
            "epoch: 1, step: 363, loss: 1.239\n",
            "epoch: 1, step: 364, loss: 1.316\n",
            "epoch: 1, step: 365, loss: 1.278\n",
            "epoch: 1, step: 366, loss: 1.139\n",
            "epoch: 1, step: 367, loss: 1.153\n",
            "epoch: 1, step: 368, loss: 1.216\n",
            "epoch: 1, step: 369, loss: 1.284\n",
            "epoch: 1, step: 370, loss: 1.131\n",
            "epoch: 1, step: 371, loss: 1.170\n",
            "epoch: 1, step: 372, loss: 1.139\n",
            "epoch: 1, step: 373, loss: 1.268\n",
            "epoch: 1, step: 374, loss: 1.236\n",
            "epoch: 1, step: 375, loss: 1.164\n",
            "epoch: 1, step: 376, loss: 1.121\n",
            "epoch: 1, step: 377, loss: 1.177\n",
            "epoch: 1, step: 378, loss: 1.237\n",
            "epoch: 1, step: 379, loss: 1.138\n",
            "epoch: 1, step: 380, loss: 1.327\n",
            "epoch: 1, step: 381, loss: 1.167\n",
            "epoch: 1, step: 382, loss: 1.166\n",
            "epoch: 1, step: 383, loss: 1.138\n",
            "epoch: 1, step: 384, loss: 1.094\n",
            "epoch: 1, step: 385, loss: 1.047\n",
            "epoch: 1, step: 386, loss: 1.134\n",
            "epoch: 1, step: 387, loss: 1.198\n",
            "epoch: 1, step: 388, loss: 1.138\n",
            "epoch: 1, step: 389, loss: 1.108\n",
            "epoch: 1, step: 390, loss: 1.171\n",
            "epoch: 1, step: 391, loss: 1.228\n",
            "epoch: 1, step: 392, loss: 1.091\n",
            "epoch: 1, step: 393, loss: 1.190\n",
            "epoch: 1, step: 394, loss: 1.210\n",
            "epoch: 1, step: 395, loss: 1.244\n",
            "epoch: 1, step: 396, loss: 1.197\n",
            "epoch: 1, step: 397, loss: 1.162\n",
            "epoch: 1, step: 398, loss: 1.224\n",
            "epoch: 1, step: 399, loss: 1.153\n",
            "epoch: 1, step: 400, loss: 1.157\n",
            "epoch: 1, step: 401, loss: 1.093\n",
            "epoch: 1, step: 402, loss: 1.141\n",
            "epoch: 1, step: 403, loss: 1.154\n",
            "epoch: 1, step: 404, loss: 1.236\n",
            "epoch: 1, step: 405, loss: 1.211\n",
            "epoch: 1, step: 406, loss: 1.042\n",
            "epoch: 1, step: 407, loss: 1.128\n",
            "epoch: 1, step: 408, loss: 1.147\n",
            "epoch: 1, step: 409, loss: 1.147\n",
            "epoch: 1, step: 410, loss: 1.127\n",
            "epoch: 1, step: 411, loss: 1.210\n",
            "epoch: 1, step: 412, loss: 1.121\n",
            "epoch: 1, step: 413, loss: 1.111\n",
            "epoch: 1, step: 414, loss: 1.021\n",
            "epoch: 1, step: 415, loss: 1.156\n",
            "epoch: 1, step: 416, loss: 1.151\n",
            "epoch: 1, step: 417, loss: 1.100\n",
            "epoch: 1, step: 418, loss: 1.179\n",
            "epoch: 1, step: 419, loss: 1.195\n",
            "epoch: 1, step: 420, loss: 1.149\n",
            "epoch: 1, step: 421, loss: 1.249\n",
            "epoch: 1, step: 422, loss: 1.207\n",
            "epoch: 1, step: 423, loss: 1.098\n",
            "epoch: 1, step: 424, loss: 1.182\n",
            "epoch: 1, step: 425, loss: 1.271\n",
            "epoch: 1, step: 426, loss: 1.148\n",
            "epoch: 1, step: 427, loss: 1.172\n",
            "epoch: 1, step: 428, loss: 1.162\n",
            "epoch: 1, step: 429, loss: 1.141\n",
            "epoch: 1, step: 430, loss: 1.178\n",
            "epoch: 1, step: 431, loss: 1.137\n",
            "epoch: 1, step: 432, loss: 1.242\n",
            "epoch: 1, step: 433, loss: 1.145\n",
            "epoch: 1, step: 434, loss: 1.223\n",
            "epoch: 1, step: 435, loss: 1.156\n",
            "epoch: 1, step: 436, loss: 1.238\n",
            "epoch: 1, step: 437, loss: 1.054\n",
            "epoch: 1, step: 438, loss: 1.168\n",
            "epoch: 1, step: 439, loss: 1.163\n",
            "epoch: 1, step: 440, loss: 1.148\n",
            "epoch: 1, step: 441, loss: 1.174\n",
            "epoch: 1, step: 442, loss: 1.235\n",
            "epoch: 1, step: 443, loss: 1.157\n",
            "epoch: 1, step: 444, loss: 1.208\n",
            "epoch: 1, step: 445, loss: 1.122\n",
            "epoch: 1, step: 446, loss: 1.204\n",
            "epoch: 1, step: 447, loss: 1.153\n",
            "epoch: 1, step: 448, loss: 1.310\n",
            "epoch: 1, step: 449, loss: 1.183\n",
            "epoch: 1, step: 450, loss: 1.245\n",
            "epoch: 1, step: 451, loss: 0.958\n",
            "epoch: 1, step: 452, loss: 1.077\n",
            "epoch: 1, step: 453, loss: 1.057\n",
            "epoch: 2, step: 1, loss: 1.160\n",
            "epoch: 2, step: 2, loss: 1.151\n",
            "epoch: 2, step: 3, loss: 1.148\n",
            "epoch: 2, step: 4, loss: 1.088\n",
            "epoch: 2, step: 5, loss: 1.106\n",
            "epoch: 2, step: 6, loss: 1.093\n",
            "epoch: 2, step: 7, loss: 1.117\n",
            "epoch: 2, step: 8, loss: 1.193\n",
            "epoch: 2, step: 9, loss: 1.175\n",
            "epoch: 2, step: 10, loss: 1.163\n",
            "epoch: 2, step: 11, loss: 1.101\n",
            "epoch: 2, step: 12, loss: 1.147\n",
            "epoch: 2, step: 13, loss: 1.052\n",
            "epoch: 2, step: 14, loss: 1.152\n",
            "epoch: 2, step: 15, loss: 1.087\n",
            "epoch: 2, step: 16, loss: 1.106\n",
            "epoch: 2, step: 17, loss: 1.095\n",
            "epoch: 2, step: 18, loss: 1.169\n",
            "epoch: 2, step: 19, loss: 1.126\n",
            "epoch: 2, step: 20, loss: 1.294\n",
            "epoch: 2, step: 21, loss: 1.118\n",
            "epoch: 2, step: 22, loss: 1.121\n",
            "epoch: 2, step: 23, loss: 1.098\n",
            "epoch: 2, step: 24, loss: 1.196\n",
            "epoch: 2, step: 25, loss: 1.099\n",
            "epoch: 2, step: 26, loss: 1.111\n",
            "epoch: 2, step: 27, loss: 1.160\n",
            "epoch: 2, step: 28, loss: 1.075\n",
            "epoch: 2, step: 29, loss: 1.079\n",
            "epoch: 2, step: 30, loss: 1.139\n",
            "epoch: 2, step: 31, loss: 1.211\n",
            "epoch: 2, step: 32, loss: 1.140\n",
            "epoch: 2, step: 33, loss: 1.144\n",
            "epoch: 2, step: 34, loss: 1.117\n",
            "epoch: 2, step: 35, loss: 1.029\n",
            "epoch: 2, step: 36, loss: 1.110\n",
            "epoch: 2, step: 37, loss: 1.107\n",
            "epoch: 2, step: 38, loss: 1.114\n",
            "epoch: 2, step: 39, loss: 1.131\n",
            "epoch: 2, step: 40, loss: 1.174\n",
            "epoch: 2, step: 41, loss: 1.069\n",
            "epoch: 2, step: 42, loss: 1.127\n",
            "epoch: 2, step: 43, loss: 1.088\n",
            "epoch: 2, step: 44, loss: 1.159\n",
            "epoch: 2, step: 45, loss: 1.072\n",
            "epoch: 2, step: 46, loss: 1.006\n",
            "epoch: 2, step: 47, loss: 1.181\n",
            "epoch: 2, step: 48, loss: 1.094\n",
            "epoch: 2, step: 49, loss: 1.103\n",
            "epoch: 2, step: 50, loss: 1.055\n",
            "epoch: 2, step: 51, loss: 1.080\n",
            "epoch: 2, step: 52, loss: 1.110\n",
            "epoch: 2, step: 53, loss: 1.096\n",
            "epoch: 2, step: 54, loss: 1.135\n",
            "epoch: 2, step: 55, loss: 1.113\n",
            "epoch: 2, step: 56, loss: 1.180\n",
            "epoch: 2, step: 57, loss: 1.037\n",
            "epoch: 2, step: 58, loss: 1.015\n",
            "epoch: 2, step: 59, loss: 1.154\n",
            "epoch: 2, step: 60, loss: 1.127\n",
            "epoch: 2, step: 61, loss: 1.110\n",
            "epoch: 2, step: 62, loss: 1.047\n",
            "epoch: 2, step: 63, loss: 1.084\n",
            "epoch: 2, step: 64, loss: 1.128\n",
            "epoch: 2, step: 65, loss: 1.134\n",
            "epoch: 2, step: 66, loss: 1.115\n",
            "epoch: 2, step: 67, loss: 1.053\n",
            "epoch: 2, step: 68, loss: 1.189\n",
            "epoch: 2, step: 69, loss: 1.074\n",
            "epoch: 2, step: 70, loss: 1.155\n",
            "epoch: 2, step: 71, loss: 1.086\n",
            "epoch: 2, step: 72, loss: 1.089\n",
            "epoch: 2, step: 73, loss: 1.111\n",
            "epoch: 2, step: 74, loss: 1.072\n",
            "epoch: 2, step: 75, loss: 1.182\n",
            "epoch: 2, step: 76, loss: 1.105\n",
            "epoch: 2, step: 77, loss: 1.166\n",
            "epoch: 2, step: 78, loss: 1.156\n",
            "epoch: 2, step: 79, loss: 1.029\n",
            "epoch: 2, step: 80, loss: 1.124\n",
            "epoch: 2, step: 81, loss: 1.081\n",
            "epoch: 2, step: 82, loss: 1.002\n",
            "epoch: 2, step: 83, loss: 1.143\n",
            "epoch: 2, step: 84, loss: 1.083\n",
            "epoch: 2, step: 85, loss: 0.957\n",
            "epoch: 2, step: 86, loss: 1.090\n",
            "epoch: 2, step: 87, loss: 1.021\n",
            "epoch: 2, step: 88, loss: 1.117\n",
            "epoch: 2, step: 89, loss: 1.126\n",
            "epoch: 2, step: 90, loss: 1.096\n",
            "epoch: 2, step: 91, loss: 1.072\n",
            "epoch: 2, step: 92, loss: 1.106\n",
            "epoch: 2, step: 93, loss: 1.117\n",
            "epoch: 2, step: 94, loss: 1.076\n",
            "epoch: 2, step: 95, loss: 1.019\n",
            "epoch: 2, step: 96, loss: 1.167\n",
            "epoch: 2, step: 97, loss: 1.131\n",
            "epoch: 2, step: 98, loss: 1.062\n",
            "epoch: 2, step: 99, loss: 1.153\n",
            "epoch: 2, step: 100, loss: 1.075\n",
            "epoch: 2, step: 101, loss: 1.065\n",
            "epoch: 2, step: 102, loss: 1.068\n",
            "epoch: 2, step: 103, loss: 1.016\n",
            "epoch: 2, step: 104, loss: 1.084\n",
            "epoch: 2, step: 105, loss: 1.131\n",
            "epoch: 2, step: 106, loss: 1.050\n",
            "epoch: 2, step: 107, loss: 1.051\n",
            "epoch: 2, step: 108, loss: 1.002\n",
            "epoch: 2, step: 109, loss: 1.029\n",
            "epoch: 2, step: 110, loss: 1.162\n",
            "epoch: 2, step: 111, loss: 1.108\n",
            "epoch: 2, step: 112, loss: 1.120\n",
            "epoch: 2, step: 113, loss: 1.034\n",
            "epoch: 2, step: 114, loss: 1.119\n",
            "epoch: 2, step: 115, loss: 1.022\n",
            "epoch: 2, step: 116, loss: 0.981\n",
            "epoch: 2, step: 117, loss: 1.169\n",
            "epoch: 2, step: 118, loss: 1.060\n",
            "epoch: 2, step: 119, loss: 1.015\n",
            "epoch: 2, step: 120, loss: 0.994\n",
            "epoch: 2, step: 121, loss: 1.046\n",
            "epoch: 2, step: 122, loss: 1.096\n",
            "epoch: 2, step: 123, loss: 1.072\n",
            "epoch: 2, step: 124, loss: 0.979\n",
            "epoch: 2, step: 125, loss: 1.091\n",
            "epoch: 2, step: 126, loss: 1.031\n",
            "epoch: 2, step: 127, loss: 1.102\n",
            "epoch: 2, step: 128, loss: 1.097\n",
            "epoch: 2, step: 129, loss: 1.004\n",
            "epoch: 2, step: 130, loss: 1.064\n",
            "epoch: 2, step: 131, loss: 0.978\n",
            "epoch: 2, step: 132, loss: 1.167\n",
            "epoch: 2, step: 133, loss: 1.014\n",
            "epoch: 2, step: 134, loss: 0.961\n",
            "epoch: 2, step: 135, loss: 1.162\n",
            "epoch: 2, step: 136, loss: 1.116\n",
            "epoch: 2, step: 137, loss: 1.110\n",
            "epoch: 2, step: 138, loss: 1.050\n",
            "epoch: 2, step: 139, loss: 1.072\n",
            "epoch: 2, step: 140, loss: 1.103\n",
            "epoch: 2, step: 141, loss: 1.130\n",
            "epoch: 2, step: 142, loss: 1.065\n",
            "epoch: 2, step: 143, loss: 1.140\n",
            "epoch: 2, step: 144, loss: 1.130\n",
            "epoch: 2, step: 145, loss: 1.043\n",
            "epoch: 2, step: 146, loss: 1.149\n",
            "epoch: 2, step: 147, loss: 1.022\n",
            "epoch: 2, step: 148, loss: 1.089\n",
            "epoch: 2, step: 149, loss: 1.091\n",
            "epoch: 2, step: 150, loss: 1.146\n",
            "epoch: 2, step: 151, loss: 1.116\n",
            "epoch: 2, step: 152, loss: 1.018\n",
            "epoch: 2, step: 153, loss: 1.200\n",
            "epoch: 2, step: 154, loss: 1.096\n",
            "epoch: 2, step: 155, loss: 1.213\n",
            "epoch: 2, step: 156, loss: 1.100\n",
            "epoch: 2, step: 157, loss: 1.076\n",
            "epoch: 2, step: 158, loss: 1.026\n",
            "epoch: 2, step: 159, loss: 0.998\n",
            "epoch: 2, step: 160, loss: 1.224\n",
            "epoch: 2, step: 161, loss: 1.113\n",
            "epoch: 2, step: 162, loss: 1.073\n",
            "epoch: 2, step: 163, loss: 1.071\n",
            "epoch: 2, step: 164, loss: 1.065\n",
            "epoch: 2, step: 165, loss: 1.111\n",
            "epoch: 2, step: 166, loss: 1.102\n",
            "epoch: 2, step: 167, loss: 1.057\n",
            "epoch: 2, step: 168, loss: 1.080\n",
            "epoch: 2, step: 169, loss: 1.113\n",
            "epoch: 2, step: 170, loss: 1.062\n",
            "epoch: 2, step: 171, loss: 1.087\n",
            "epoch: 2, step: 172, loss: 1.080\n",
            "epoch: 2, step: 173, loss: 0.998\n",
            "epoch: 2, step: 174, loss: 0.963\n",
            "epoch: 2, step: 175, loss: 1.047\n",
            "epoch: 2, step: 176, loss: 1.226\n",
            "epoch: 2, step: 177, loss: 1.118\n",
            "epoch: 2, step: 178, loss: 1.062\n",
            "epoch: 2, step: 179, loss: 1.084\n",
            "epoch: 2, step: 180, loss: 1.103\n",
            "epoch: 2, step: 181, loss: 0.985\n",
            "epoch: 2, step: 182, loss: 1.032\n",
            "epoch: 2, step: 183, loss: 1.026\n",
            "epoch: 2, step: 184, loss: 1.090\n",
            "epoch: 2, step: 185, loss: 1.094\n",
            "epoch: 2, step: 186, loss: 1.038\n",
            "epoch: 2, step: 187, loss: 1.139\n",
            "epoch: 2, step: 188, loss: 1.093\n",
            "epoch: 2, step: 189, loss: 1.041\n",
            "epoch: 2, step: 190, loss: 1.055\n",
            "epoch: 2, step: 191, loss: 1.030\n",
            "epoch: 2, step: 192, loss: 1.078\n",
            "epoch: 2, step: 193, loss: 1.146\n",
            "epoch: 2, step: 194, loss: 1.154\n",
            "epoch: 2, step: 195, loss: 1.068\n",
            "epoch: 2, step: 196, loss: 1.044\n",
            "epoch: 2, step: 197, loss: 1.101\n",
            "epoch: 2, step: 198, loss: 1.119\n",
            "epoch: 2, step: 199, loss: 1.024\n",
            "epoch: 2, step: 200, loss: 0.934\n",
            "epoch: 2, step: 201, loss: 1.051\n",
            "epoch: 2, step: 202, loss: 1.005\n",
            "epoch: 2, step: 203, loss: 1.054\n",
            "epoch: 2, step: 204, loss: 0.996\n",
            "epoch: 2, step: 205, loss: 1.070\n",
            "epoch: 2, step: 206, loss: 0.995\n",
            "epoch: 2, step: 207, loss: 1.057\n",
            "epoch: 2, step: 208, loss: 1.052\n",
            "epoch: 2, step: 209, loss: 0.911\n",
            "epoch: 2, step: 210, loss: 1.008\n",
            "epoch: 2, step: 211, loss: 1.079\n",
            "epoch: 2, step: 212, loss: 1.131\n",
            "epoch: 2, step: 213, loss: 1.061\n",
            "epoch: 2, step: 214, loss: 1.087\n",
            "epoch: 2, step: 215, loss: 1.036\n",
            "epoch: 2, step: 216, loss: 0.983\n",
            "epoch: 2, step: 217, loss: 1.063\n",
            "epoch: 2, step: 218, loss: 1.084\n",
            "epoch: 2, step: 219, loss: 1.042\n",
            "epoch: 2, step: 220, loss: 0.962\n",
            "epoch: 2, step: 221, loss: 1.009\n",
            "epoch: 2, step: 222, loss: 1.093\n",
            "epoch: 2, step: 223, loss: 1.074\n",
            "epoch: 2, step: 224, loss: 1.005\n",
            "epoch: 2, step: 225, loss: 0.982\n",
            "epoch: 2, step: 226, loss: 0.967\n",
            "epoch: 2, step: 227, loss: 1.022\n",
            "epoch: 2, step: 228, loss: 1.038\n",
            "epoch: 2, step: 229, loss: 1.011\n",
            "epoch: 2, step: 230, loss: 1.002\n",
            "epoch: 2, step: 231, loss: 0.917\n",
            "epoch: 2, step: 232, loss: 1.097\n",
            "epoch: 2, step: 233, loss: 1.036\n",
            "epoch: 2, step: 234, loss: 0.990\n",
            "epoch: 2, step: 235, loss: 0.979\n",
            "epoch: 2, step: 236, loss: 1.137\n",
            "epoch: 2, step: 237, loss: 1.057\n",
            "epoch: 2, step: 238, loss: 1.121\n",
            "epoch: 2, step: 239, loss: 1.048\n",
            "epoch: 2, step: 240, loss: 1.060\n",
            "epoch: 2, step: 241, loss: 1.051\n",
            "epoch: 2, step: 242, loss: 1.049\n",
            "epoch: 2, step: 243, loss: 1.046\n",
            "epoch: 2, step: 244, loss: 1.113\n",
            "epoch: 2, step: 245, loss: 1.137\n",
            "epoch: 2, step: 246, loss: 1.028\n",
            "epoch: 2, step: 247, loss: 1.108\n",
            "epoch: 2, step: 248, loss: 1.083\n",
            "epoch: 2, step: 249, loss: 1.007\n",
            "epoch: 2, step: 250, loss: 1.096\n",
            "epoch: 2, step: 251, loss: 1.026\n",
            "epoch: 2, step: 252, loss: 1.117\n",
            "epoch: 2, step: 253, loss: 1.003\n",
            "epoch: 2, step: 254, loss: 1.015\n",
            "epoch: 2, step: 255, loss: 1.024\n",
            "epoch: 2, step: 256, loss: 1.044\n",
            "epoch: 2, step: 257, loss: 0.948\n",
            "epoch: 2, step: 258, loss: 1.147\n",
            "epoch: 2, step: 259, loss: 1.119\n",
            "epoch: 2, step: 260, loss: 1.053\n",
            "epoch: 2, step: 261, loss: 0.950\n",
            "epoch: 2, step: 262, loss: 0.955\n",
            "epoch: 2, step: 263, loss: 1.007\n",
            "epoch: 2, step: 264, loss: 1.031\n",
            "epoch: 2, step: 265, loss: 1.050\n",
            "epoch: 2, step: 266, loss: 1.037\n",
            "epoch: 2, step: 267, loss: 1.062\n",
            "epoch: 2, step: 268, loss: 1.016\n",
            "epoch: 2, step: 269, loss: 1.084\n",
            "epoch: 2, step: 270, loss: 0.963\n",
            "epoch: 2, step: 271, loss: 1.016\n",
            "epoch: 2, step: 272, loss: 1.082\n",
            "epoch: 2, step: 273, loss: 1.075\n",
            "epoch: 2, step: 274, loss: 0.976\n",
            "epoch: 2, step: 275, loss: 1.089\n",
            "epoch: 2, step: 276, loss: 1.006\n",
            "epoch: 2, step: 277, loss: 1.051\n",
            "epoch: 2, step: 278, loss: 1.033\n",
            "epoch: 2, step: 279, loss: 1.047\n",
            "epoch: 2, step: 280, loss: 1.038\n",
            "epoch: 2, step: 281, loss: 1.040\n",
            "epoch: 2, step: 282, loss: 0.921\n",
            "epoch: 2, step: 283, loss: 0.998\n",
            "epoch: 2, step: 284, loss: 1.078\n",
            "epoch: 2, step: 285, loss: 1.003\n",
            "epoch: 2, step: 286, loss: 1.023\n",
            "epoch: 2, step: 287, loss: 0.936\n",
            "epoch: 2, step: 288, loss: 1.031\n",
            "epoch: 2, step: 289, loss: 1.060\n",
            "epoch: 2, step: 290, loss: 1.108\n",
            "epoch: 2, step: 291, loss: 0.965\n",
            "epoch: 2, step: 292, loss: 1.008\n",
            "epoch: 2, step: 293, loss: 1.019\n",
            "epoch: 2, step: 294, loss: 1.087\n",
            "epoch: 2, step: 295, loss: 1.145\n",
            "epoch: 2, step: 296, loss: 0.995\n",
            "epoch: 2, step: 297, loss: 1.008\n",
            "epoch: 2, step: 298, loss: 0.996\n",
            "epoch: 2, step: 299, loss: 1.019\n",
            "epoch: 2, step: 300, loss: 0.959\n",
            "epoch: 2, step: 301, loss: 0.962\n",
            "epoch: 2, step: 302, loss: 0.985\n",
            "epoch: 2, step: 303, loss: 1.128\n",
            "epoch: 2, step: 304, loss: 1.105\n",
            "epoch: 2, step: 305, loss: 0.891\n",
            "epoch: 2, step: 306, loss: 1.026\n",
            "epoch: 2, step: 307, loss: 1.078\n",
            "epoch: 2, step: 308, loss: 1.022\n",
            "epoch: 2, step: 309, loss: 1.012\n",
            "epoch: 2, step: 310, loss: 1.039\n",
            "epoch: 2, step: 311, loss: 1.095\n",
            "epoch: 2, step: 312, loss: 1.043\n",
            "epoch: 2, step: 313, loss: 0.934\n",
            "epoch: 2, step: 314, loss: 1.135\n",
            "epoch: 2, step: 315, loss: 1.036\n",
            "epoch: 2, step: 316, loss: 0.994\n",
            "epoch: 2, step: 317, loss: 1.067\n",
            "epoch: 2, step: 318, loss: 1.018\n",
            "epoch: 2, step: 319, loss: 1.200\n",
            "epoch: 2, step: 320, loss: 1.030\n",
            "epoch: 2, step: 321, loss: 1.028\n",
            "epoch: 2, step: 322, loss: 1.023\n",
            "epoch: 2, step: 323, loss: 1.053\n",
            "epoch: 2, step: 324, loss: 1.034\n",
            "epoch: 2, step: 325, loss: 1.138\n",
            "epoch: 2, step: 326, loss: 0.954\n",
            "epoch: 2, step: 327, loss: 0.983\n",
            "epoch: 2, step: 328, loss: 1.033\n",
            "epoch: 2, step: 329, loss: 0.936\n",
            "epoch: 2, step: 330, loss: 1.062\n",
            "epoch: 2, step: 331, loss: 0.974\n",
            "epoch: 2, step: 332, loss: 0.970\n",
            "epoch: 2, step: 333, loss: 1.066\n",
            "epoch: 2, step: 334, loss: 1.052\n",
            "epoch: 2, step: 335, loss: 1.081\n",
            "epoch: 2, step: 336, loss: 1.050\n",
            "epoch: 2, step: 337, loss: 1.072\n",
            "epoch: 2, step: 338, loss: 0.984\n",
            "epoch: 2, step: 339, loss: 1.091\n",
            "epoch: 2, step: 340, loss: 1.047\n",
            "epoch: 2, step: 341, loss: 1.078\n",
            "epoch: 2, step: 342, loss: 0.994\n",
            "epoch: 2, step: 343, loss: 1.092\n",
            "epoch: 2, step: 344, loss: 1.020\n",
            "epoch: 2, step: 345, loss: 1.029\n",
            "epoch: 2, step: 346, loss: 1.005\n",
            "epoch: 2, step: 347, loss: 1.113\n",
            "epoch: 2, step: 348, loss: 0.996\n",
            "epoch: 2, step: 349, loss: 0.979\n",
            "epoch: 2, step: 350, loss: 1.016\n",
            "epoch: 2, step: 351, loss: 0.967\n",
            "epoch: 2, step: 352, loss: 1.024\n",
            "epoch: 2, step: 353, loss: 1.093\n",
            "epoch: 2, step: 354, loss: 1.051\n",
            "epoch: 2, step: 355, loss: 1.035\n",
            "epoch: 2, step: 356, loss: 1.014\n",
            "epoch: 2, step: 357, loss: 1.096\n",
            "epoch: 2, step: 358, loss: 1.048\n",
            "epoch: 2, step: 359, loss: 1.033\n",
            "epoch: 2, step: 360, loss: 1.036\n",
            "epoch: 2, step: 361, loss: 1.027\n",
            "epoch: 2, step: 362, loss: 0.986\n",
            "epoch: 2, step: 363, loss: 0.967\n",
            "epoch: 2, step: 364, loss: 0.980\n",
            "epoch: 2, step: 365, loss: 1.010\n",
            "epoch: 2, step: 366, loss: 1.044\n",
            "epoch: 2, step: 367, loss: 1.045\n",
            "epoch: 2, step: 368, loss: 0.917\n",
            "epoch: 2, step: 369, loss: 0.994\n",
            "epoch: 2, step: 370, loss: 1.024\n",
            "epoch: 2, step: 371, loss: 1.099\n",
            "epoch: 2, step: 372, loss: 1.039\n",
            "epoch: 2, step: 373, loss: 0.985\n",
            "epoch: 2, step: 374, loss: 0.961\n",
            "epoch: 2, step: 375, loss: 1.056\n",
            "epoch: 2, step: 376, loss: 0.946\n",
            "epoch: 2, step: 377, loss: 1.106\n",
            "epoch: 2, step: 378, loss: 1.043\n",
            "epoch: 2, step: 379, loss: 0.944\n",
            "epoch: 2, step: 380, loss: 1.082\n",
            "epoch: 2, step: 381, loss: 1.070\n",
            "epoch: 2, step: 382, loss: 1.010\n",
            "epoch: 2, step: 383, loss: 1.135\n",
            "epoch: 2, step: 384, loss: 1.105\n",
            "epoch: 2, step: 385, loss: 0.969\n",
            "epoch: 2, step: 386, loss: 0.921\n",
            "epoch: 2, step: 387, loss: 0.966\n",
            "epoch: 2, step: 388, loss: 0.948\n",
            "epoch: 2, step: 389, loss: 0.973\n",
            "epoch: 2, step: 390, loss: 1.020\n",
            "epoch: 2, step: 391, loss: 1.012\n",
            "epoch: 2, step: 392, loss: 1.016\n",
            "epoch: 2, step: 393, loss: 0.934\n",
            "epoch: 2, step: 394, loss: 0.935\n",
            "epoch: 2, step: 395, loss: 1.053\n",
            "epoch: 2, step: 396, loss: 0.960\n",
            "epoch: 2, step: 397, loss: 1.031\n",
            "epoch: 2, step: 398, loss: 1.025\n",
            "epoch: 2, step: 399, loss: 1.020\n",
            "epoch: 2, step: 400, loss: 0.933\n",
            "epoch: 2, step: 401, loss: 1.077\n",
            "epoch: 2, step: 402, loss: 1.026\n",
            "epoch: 2, step: 403, loss: 1.024\n",
            "epoch: 2, step: 404, loss: 0.940\n",
            "epoch: 2, step: 405, loss: 1.023\n",
            "epoch: 2, step: 406, loss: 1.121\n",
            "epoch: 2, step: 407, loss: 1.004\n",
            "epoch: 2, step: 408, loss: 1.005\n",
            "epoch: 2, step: 409, loss: 1.079\n",
            "epoch: 2, step: 410, loss: 0.911\n",
            "epoch: 2, step: 411, loss: 0.984\n",
            "epoch: 2, step: 412, loss: 0.995\n",
            "epoch: 2, step: 413, loss: 1.036\n",
            "epoch: 2, step: 414, loss: 0.976\n",
            "epoch: 2, step: 415, loss: 0.952\n",
            "epoch: 2, step: 416, loss: 0.982\n",
            "epoch: 2, step: 417, loss: 1.078\n",
            "epoch: 2, step: 418, loss: 1.066\n",
            "epoch: 2, step: 419, loss: 1.087\n",
            "epoch: 2, step: 420, loss: 1.040\n",
            "epoch: 2, step: 421, loss: 1.000\n",
            "epoch: 2, step: 422, loss: 0.965\n",
            "epoch: 2, step: 423, loss: 0.997\n",
            "epoch: 2, step: 424, loss: 1.031\n",
            "epoch: 2, step: 425, loss: 1.086\n",
            "epoch: 2, step: 426, loss: 0.964\n",
            "epoch: 2, step: 427, loss: 0.935\n",
            "epoch: 2, step: 428, loss: 1.052\n",
            "epoch: 2, step: 429, loss: 0.996\n",
            "epoch: 2, step: 430, loss: 1.044\n",
            "epoch: 2, step: 431, loss: 0.943\n",
            "epoch: 2, step: 432, loss: 1.081\n",
            "epoch: 2, step: 433, loss: 1.021\n",
            "epoch: 2, step: 434, loss: 1.011\n",
            "epoch: 2, step: 435, loss: 1.027\n",
            "epoch: 2, step: 436, loss: 1.001\n",
            "epoch: 2, step: 437, loss: 0.987\n",
            "epoch: 2, step: 438, loss: 0.991\n",
            "epoch: 2, step: 439, loss: 0.982\n",
            "epoch: 2, step: 440, loss: 1.100\n",
            "epoch: 2, step: 441, loss: 1.022\n",
            "epoch: 2, step: 442, loss: 0.975\n",
            "epoch: 2, step: 443, loss: 1.020\n",
            "epoch: 2, step: 444, loss: 0.962\n",
            "epoch: 2, step: 445, loss: 1.043\n",
            "epoch: 2, step: 446, loss: 1.014\n",
            "epoch: 2, step: 447, loss: 0.965\n",
            "epoch: 2, step: 448, loss: 1.125\n",
            "epoch: 2, step: 449, loss: 1.014\n",
            "epoch: 2, step: 450, loss: 0.976\n",
            "epoch: 2, step: 451, loss: 1.065\n",
            "epoch: 2, step: 452, loss: 0.940\n",
            "epoch: 2, step: 453, loss: 1.106\n",
            "epoch: 3, step: 1, loss: 0.980\n",
            "epoch: 3, step: 2, loss: 1.001\n",
            "epoch: 3, step: 3, loss: 0.858\n",
            "epoch: 3, step: 4, loss: 0.945\n",
            "epoch: 3, step: 5, loss: 0.941\n",
            "epoch: 3, step: 6, loss: 0.962\n",
            "epoch: 3, step: 7, loss: 1.008\n",
            "epoch: 3, step: 8, loss: 1.068\n",
            "epoch: 3, step: 9, loss: 0.909\n",
            "epoch: 3, step: 10, loss: 0.892\n",
            "epoch: 3, step: 11, loss: 1.003\n",
            "epoch: 3, step: 12, loss: 0.946\n",
            "epoch: 3, step: 13, loss: 0.987\n",
            "epoch: 3, step: 14, loss: 0.847\n",
            "epoch: 3, step: 15, loss: 1.010\n",
            "epoch: 3, step: 16, loss: 1.017\n",
            "epoch: 3, step: 17, loss: 0.941\n",
            "epoch: 3, step: 18, loss: 0.947\n",
            "epoch: 3, step: 19, loss: 0.952\n",
            "epoch: 3, step: 20, loss: 1.054\n",
            "epoch: 3, step: 21, loss: 0.976\n",
            "epoch: 3, step: 22, loss: 0.931\n",
            "epoch: 3, step: 23, loss: 0.962\n",
            "epoch: 3, step: 24, loss: 0.976\n",
            "epoch: 3, step: 25, loss: 0.895\n",
            "epoch: 3, step: 26, loss: 0.914\n",
            "epoch: 3, step: 27, loss: 0.992\n",
            "epoch: 3, step: 28, loss: 1.023\n",
            "epoch: 3, step: 29, loss: 0.970\n",
            "epoch: 3, step: 30, loss: 0.914\n",
            "epoch: 3, step: 31, loss: 0.951\n",
            "epoch: 3, step: 32, loss: 0.960\n",
            "epoch: 3, step: 33, loss: 1.003\n",
            "epoch: 3, step: 34, loss: 0.944\n",
            "epoch: 3, step: 35, loss: 1.046\n",
            "epoch: 3, step: 36, loss: 0.968\n",
            "epoch: 3, step: 37, loss: 0.995\n",
            "epoch: 3, step: 38, loss: 1.026\n",
            "epoch: 3, step: 39, loss: 0.972\n",
            "epoch: 3, step: 40, loss: 0.987\n",
            "epoch: 3, step: 41, loss: 0.965\n",
            "epoch: 3, step: 42, loss: 0.907\n",
            "epoch: 3, step: 43, loss: 0.891\n",
            "epoch: 3, step: 44, loss: 1.009\n",
            "epoch: 3, step: 45, loss: 0.924\n",
            "epoch: 3, step: 46, loss: 0.862\n",
            "epoch: 3, step: 47, loss: 0.990\n",
            "epoch: 3, step: 48, loss: 1.010\n",
            "epoch: 3, step: 49, loss: 1.028\n",
            "epoch: 3, step: 50, loss: 0.879\n",
            "epoch: 3, step: 51, loss: 1.001\n",
            "epoch: 3, step: 52, loss: 0.988\n",
            "epoch: 3, step: 53, loss: 0.966\n",
            "epoch: 3, step: 54, loss: 0.941\n",
            "epoch: 3, step: 55, loss: 0.965\n",
            "epoch: 3, step: 56, loss: 0.935\n",
            "epoch: 3, step: 57, loss: 1.046\n",
            "epoch: 3, step: 58, loss: 1.004\n",
            "epoch: 3, step: 59, loss: 0.948\n",
            "epoch: 3, step: 60, loss: 0.962\n",
            "epoch: 3, step: 61, loss: 1.021\n",
            "epoch: 3, step: 62, loss: 0.936\n",
            "epoch: 3, step: 63, loss: 0.991\n",
            "epoch: 3, step: 64, loss: 0.916\n",
            "epoch: 3, step: 65, loss: 1.058\n",
            "epoch: 3, step: 66, loss: 0.896\n",
            "epoch: 3, step: 67, loss: 0.946\n",
            "epoch: 3, step: 68, loss: 0.949\n",
            "epoch: 3, step: 69, loss: 0.942\n",
            "epoch: 3, step: 70, loss: 0.950\n",
            "epoch: 3, step: 71, loss: 0.938\n",
            "epoch: 3, step: 72, loss: 0.904\n",
            "epoch: 3, step: 73, loss: 1.064\n",
            "epoch: 3, step: 74, loss: 0.956\n",
            "epoch: 3, step: 75, loss: 0.894\n",
            "epoch: 3, step: 76, loss: 0.916\n",
            "epoch: 3, step: 77, loss: 1.040\n",
            "epoch: 3, step: 78, loss: 0.966\n",
            "epoch: 3, step: 79, loss: 0.886\n",
            "epoch: 3, step: 80, loss: 0.970\n",
            "epoch: 3, step: 81, loss: 0.902\n",
            "epoch: 3, step: 82, loss: 0.950\n",
            "epoch: 3, step: 83, loss: 0.955\n",
            "epoch: 3, step: 84, loss: 1.068\n",
            "epoch: 3, step: 85, loss: 0.922\n",
            "epoch: 3, step: 86, loss: 1.079\n",
            "epoch: 3, step: 87, loss: 0.977\n",
            "epoch: 3, step: 88, loss: 0.932\n",
            "epoch: 3, step: 89, loss: 0.964\n",
            "epoch: 3, step: 90, loss: 0.894\n",
            "epoch: 3, step: 91, loss: 0.878\n",
            "epoch: 3, step: 92, loss: 1.074\n",
            "epoch: 3, step: 93, loss: 1.006\n",
            "epoch: 3, step: 94, loss: 0.941\n",
            "epoch: 3, step: 95, loss: 0.979\n",
            "epoch: 3, step: 96, loss: 0.952\n",
            "epoch: 3, step: 97, loss: 0.883\n",
            "epoch: 3, step: 98, loss: 0.896\n",
            "epoch: 3, step: 99, loss: 0.919\n",
            "epoch: 3, step: 100, loss: 1.014\n",
            "epoch: 3, step: 101, loss: 0.959\n",
            "epoch: 3, step: 102, loss: 0.940\n",
            "epoch: 3, step: 103, loss: 1.014\n",
            "epoch: 3, step: 104, loss: 1.065\n",
            "epoch: 3, step: 105, loss: 1.007\n",
            "epoch: 3, step: 106, loss: 0.892\n",
            "epoch: 3, step: 107, loss: 0.936\n",
            "epoch: 3, step: 108, loss: 0.935\n",
            "epoch: 3, step: 109, loss: 0.909\n",
            "epoch: 3, step: 110, loss: 0.936\n",
            "epoch: 3, step: 111, loss: 0.963\n",
            "epoch: 3, step: 112, loss: 0.918\n",
            "epoch: 3, step: 113, loss: 0.884\n",
            "epoch: 3, step: 114, loss: 0.914\n",
            "epoch: 3, step: 115, loss: 0.861\n",
            "epoch: 3, step: 116, loss: 0.954\n",
            "epoch: 3, step: 117, loss: 0.889\n",
            "epoch: 3, step: 118, loss: 0.976\n",
            "epoch: 3, step: 119, loss: 0.959\n",
            "epoch: 3, step: 120, loss: 0.922\n",
            "epoch: 3, step: 121, loss: 0.930\n",
            "epoch: 3, step: 122, loss: 0.959\n",
            "epoch: 3, step: 123, loss: 0.938\n",
            "epoch: 3, step: 124, loss: 0.918\n",
            "epoch: 3, step: 125, loss: 0.950\n",
            "epoch: 3, step: 126, loss: 0.932\n",
            "epoch: 3, step: 127, loss: 1.031\n",
            "epoch: 3, step: 128, loss: 0.870\n",
            "epoch: 3, step: 129, loss: 0.948\n",
            "epoch: 3, step: 130, loss: 0.969\n",
            "epoch: 3, step: 131, loss: 1.051\n",
            "epoch: 3, step: 132, loss: 1.007\n",
            "epoch: 3, step: 133, loss: 1.001\n",
            "epoch: 3, step: 134, loss: 0.940\n",
            "epoch: 3, step: 135, loss: 0.979\n",
            "epoch: 3, step: 136, loss: 1.003\n",
            "epoch: 3, step: 137, loss: 0.942\n",
            "epoch: 3, step: 138, loss: 0.878\n",
            "epoch: 3, step: 139, loss: 0.839\n",
            "epoch: 3, step: 140, loss: 0.986\n",
            "epoch: 3, step: 141, loss: 1.006\n",
            "epoch: 3, step: 142, loss: 0.960\n",
            "epoch: 3, step: 143, loss: 0.951\n",
            "epoch: 3, step: 144, loss: 0.994\n",
            "epoch: 3, step: 145, loss: 1.030\n",
            "epoch: 3, step: 146, loss: 0.934\n",
            "epoch: 3, step: 147, loss: 0.909\n",
            "epoch: 3, step: 148, loss: 1.016\n",
            "epoch: 3, step: 149, loss: 0.926\n",
            "epoch: 3, step: 150, loss: 0.926\n",
            "epoch: 3, step: 151, loss: 0.919\n",
            "epoch: 3, step: 152, loss: 0.917\n",
            "epoch: 3, step: 153, loss: 0.992\n",
            "epoch: 3, step: 154, loss: 0.966\n",
            "epoch: 3, step: 155, loss: 0.917\n",
            "epoch: 3, step: 156, loss: 0.939\n",
            "epoch: 3, step: 157, loss: 0.925\n",
            "epoch: 3, step: 158, loss: 0.954\n",
            "epoch: 3, step: 159, loss: 0.902\n",
            "epoch: 3, step: 160, loss: 1.015\n",
            "epoch: 3, step: 161, loss: 0.847\n",
            "epoch: 3, step: 162, loss: 0.942\n",
            "epoch: 3, step: 163, loss: 0.935\n",
            "epoch: 3, step: 164, loss: 0.934\n",
            "epoch: 3, step: 165, loss: 0.951\n",
            "epoch: 3, step: 166, loss: 0.898\n",
            "epoch: 3, step: 167, loss: 0.879\n",
            "epoch: 3, step: 168, loss: 0.896\n",
            "epoch: 3, step: 169, loss: 0.948\n",
            "epoch: 3, step: 170, loss: 0.883\n",
            "epoch: 3, step: 171, loss: 0.885\n",
            "epoch: 3, step: 172, loss: 0.867\n",
            "epoch: 3, step: 173, loss: 0.924\n",
            "epoch: 3, step: 174, loss: 0.962\n",
            "epoch: 3, step: 175, loss: 0.891\n",
            "epoch: 3, step: 176, loss: 0.868\n",
            "epoch: 3, step: 177, loss: 0.933\n",
            "epoch: 3, step: 178, loss: 0.975\n",
            "epoch: 3, step: 179, loss: 0.916\n",
            "epoch: 3, step: 180, loss: 0.967\n",
            "epoch: 3, step: 181, loss: 1.029\n",
            "epoch: 3, step: 182, loss: 1.012\n",
            "epoch: 3, step: 183, loss: 0.974\n",
            "epoch: 3, step: 184, loss: 0.911\n",
            "epoch: 3, step: 185, loss: 0.898\n",
            "epoch: 3, step: 186, loss: 0.845\n",
            "epoch: 3, step: 187, loss: 0.909\n",
            "epoch: 3, step: 188, loss: 0.914\n",
            "epoch: 3, step: 189, loss: 0.887\n",
            "epoch: 3, step: 190, loss: 0.963\n",
            "epoch: 3, step: 191, loss: 0.865\n",
            "epoch: 3, step: 192, loss: 0.961\n",
            "epoch: 3, step: 193, loss: 0.929\n",
            "epoch: 3, step: 194, loss: 0.948\n",
            "epoch: 3, step: 195, loss: 0.995\n",
            "epoch: 3, step: 196, loss: 0.902\n",
            "epoch: 3, step: 197, loss: 0.886\n",
            "epoch: 3, step: 198, loss: 0.966\n",
            "epoch: 3, step: 199, loss: 0.929\n",
            "epoch: 3, step: 200, loss: 0.887\n",
            "epoch: 3, step: 201, loss: 0.876\n",
            "epoch: 3, step: 202, loss: 1.007\n",
            "epoch: 3, step: 203, loss: 1.009\n",
            "epoch: 3, step: 204, loss: 0.832\n",
            "epoch: 3, step: 205, loss: 0.946\n",
            "epoch: 3, step: 206, loss: 0.903\n",
            "epoch: 3, step: 207, loss: 0.840\n",
            "epoch: 3, step: 208, loss: 1.017\n",
            "epoch: 3, step: 209, loss: 0.955\n",
            "epoch: 3, step: 210, loss: 0.891\n",
            "epoch: 3, step: 211, loss: 0.933\n",
            "epoch: 3, step: 212, loss: 0.961\n",
            "epoch: 3, step: 213, loss: 0.969\n",
            "epoch: 3, step: 214, loss: 0.957\n",
            "epoch: 3, step: 215, loss: 0.904\n",
            "epoch: 3, step: 216, loss: 0.893\n",
            "epoch: 3, step: 217, loss: 0.885\n",
            "epoch: 3, step: 218, loss: 0.828\n",
            "epoch: 3, step: 219, loss: 0.982\n",
            "epoch: 3, step: 220, loss: 1.008\n",
            "epoch: 3, step: 221, loss: 0.987\n",
            "epoch: 3, step: 222, loss: 0.967\n",
            "epoch: 3, step: 223, loss: 0.950\n",
            "epoch: 3, step: 224, loss: 0.892\n",
            "epoch: 3, step: 225, loss: 0.925\n",
            "epoch: 3, step: 226, loss: 1.005\n",
            "epoch: 3, step: 227, loss: 0.870\n",
            "epoch: 3, step: 228, loss: 0.856\n",
            "epoch: 3, step: 229, loss: 0.946\n",
            "epoch: 3, step: 230, loss: 0.896\n",
            "epoch: 3, step: 231, loss: 0.907\n",
            "epoch: 3, step: 232, loss: 0.883\n",
            "epoch: 3, step: 233, loss: 0.949\n",
            "epoch: 3, step: 234, loss: 0.912\n",
            "epoch: 3, step: 235, loss: 0.856\n",
            "epoch: 3, step: 236, loss: 0.991\n",
            "epoch: 3, step: 237, loss: 0.951\n",
            "epoch: 3, step: 238, loss: 0.887\n",
            "epoch: 3, step: 239, loss: 0.988\n",
            "epoch: 3, step: 240, loss: 0.881\n",
            "epoch: 3, step: 241, loss: 0.910\n",
            "epoch: 3, step: 242, loss: 0.987\n",
            "epoch: 3, step: 243, loss: 0.928\n",
            "epoch: 3, step: 244, loss: 0.834\n",
            "epoch: 3, step: 245, loss: 0.935\n",
            "epoch: 3, step: 246, loss: 0.923\n",
            "epoch: 3, step: 247, loss: 0.931\n",
            "epoch: 3, step: 248, loss: 0.959\n",
            "epoch: 3, step: 249, loss: 1.054\n",
            "epoch: 3, step: 250, loss: 0.926\n",
            "epoch: 3, step: 251, loss: 0.853\n",
            "epoch: 3, step: 252, loss: 1.086\n",
            "epoch: 3, step: 253, loss: 0.866\n",
            "epoch: 3, step: 254, loss: 0.874\n",
            "epoch: 3, step: 255, loss: 0.972\n",
            "epoch: 3, step: 256, loss: 0.876\n",
            "epoch: 3, step: 257, loss: 0.897\n",
            "epoch: 3, step: 258, loss: 1.026\n",
            "epoch: 3, step: 259, loss: 0.833\n",
            "epoch: 3, step: 260, loss: 0.907\n",
            "epoch: 3, step: 261, loss: 0.838\n",
            "epoch: 3, step: 262, loss: 0.786\n",
            "epoch: 3, step: 263, loss: 0.900\n",
            "epoch: 3, step: 264, loss: 0.900\n",
            "epoch: 3, step: 265, loss: 0.924\n",
            "epoch: 3, step: 266, loss: 0.893\n",
            "epoch: 3, step: 267, loss: 0.863\n",
            "epoch: 3, step: 268, loss: 0.924\n",
            "epoch: 3, step: 269, loss: 0.936\n",
            "epoch: 3, step: 270, loss: 0.774\n",
            "epoch: 3, step: 271, loss: 0.875\n",
            "epoch: 3, step: 272, loss: 0.848\n",
            "epoch: 3, step: 273, loss: 1.005\n",
            "epoch: 3, step: 274, loss: 0.961\n",
            "epoch: 3, step: 275, loss: 0.889\n",
            "epoch: 3, step: 276, loss: 0.907\n",
            "epoch: 3, step: 277, loss: 0.940\n",
            "epoch: 3, step: 278, loss: 0.856\n",
            "epoch: 3, step: 279, loss: 0.821\n",
            "epoch: 3, step: 280, loss: 0.938\n",
            "epoch: 3, step: 281, loss: 0.866\n",
            "epoch: 3, step: 282, loss: 0.989\n",
            "epoch: 3, step: 283, loss: 0.858\n",
            "epoch: 3, step: 284, loss: 0.924\n",
            "epoch: 3, step: 285, loss: 0.840\n",
            "epoch: 3, step: 286, loss: 0.999\n",
            "epoch: 3, step: 287, loss: 0.860\n",
            "epoch: 3, step: 288, loss: 0.917\n",
            "epoch: 3, step: 289, loss: 0.872\n",
            "epoch: 3, step: 290, loss: 0.977\n",
            "epoch: 3, step: 291, loss: 0.915\n",
            "epoch: 3, step: 292, loss: 0.932\n",
            "epoch: 3, step: 293, loss: 0.883\n",
            "epoch: 3, step: 294, loss: 1.023\n",
            "epoch: 3, step: 295, loss: 0.914\n",
            "epoch: 3, step: 296, loss: 0.947\n",
            "epoch: 3, step: 297, loss: 1.006\n",
            "epoch: 3, step: 298, loss: 0.908\n",
            "epoch: 3, step: 299, loss: 0.937\n",
            "epoch: 3, step: 300, loss: 0.909\n",
            "epoch: 3, step: 301, loss: 0.921\n",
            "epoch: 3, step: 302, loss: 0.964\n",
            "epoch: 3, step: 303, loss: 0.968\n",
            "epoch: 3, step: 304, loss: 0.872\n",
            "epoch: 3, step: 305, loss: 0.827\n",
            "epoch: 3, step: 306, loss: 0.889\n",
            "epoch: 3, step: 307, loss: 0.985\n",
            "epoch: 3, step: 308, loss: 0.935\n",
            "epoch: 3, step: 309, loss: 0.995\n",
            "epoch: 3, step: 310, loss: 0.861\n",
            "epoch: 3, step: 311, loss: 0.914\n",
            "epoch: 3, step: 312, loss: 0.844\n",
            "epoch: 3, step: 313, loss: 0.855\n",
            "epoch: 3, step: 314, loss: 0.925\n",
            "epoch: 3, step: 315, loss: 0.831\n",
            "epoch: 3, step: 316, loss: 0.954\n",
            "epoch: 3, step: 317, loss: 0.969\n",
            "epoch: 3, step: 318, loss: 0.827\n",
            "epoch: 3, step: 319, loss: 0.934\n",
            "epoch: 3, step: 320, loss: 1.022\n",
            "epoch: 3, step: 321, loss: 0.954\n",
            "epoch: 3, step: 322, loss: 0.821\n",
            "epoch: 3, step: 323, loss: 0.875\n",
            "epoch: 3, step: 324, loss: 1.009\n",
            "epoch: 3, step: 325, loss: 0.848\n",
            "epoch: 3, step: 326, loss: 0.916\n",
            "epoch: 3, step: 327, loss: 0.957\n",
            "epoch: 3, step: 328, loss: 0.820\n",
            "epoch: 3, step: 329, loss: 0.863\n",
            "epoch: 3, step: 330, loss: 0.825\n",
            "epoch: 3, step: 331, loss: 0.816\n",
            "epoch: 3, step: 332, loss: 0.891\n",
            "epoch: 3, step: 333, loss: 0.893\n",
            "epoch: 3, step: 334, loss: 0.875\n",
            "epoch: 3, step: 335, loss: 0.808\n",
            "epoch: 3, step: 336, loss: 0.943\n",
            "epoch: 3, step: 337, loss: 0.871\n",
            "epoch: 3, step: 338, loss: 0.891\n",
            "epoch: 3, step: 339, loss: 0.881\n",
            "epoch: 3, step: 340, loss: 0.939\n",
            "epoch: 3, step: 341, loss: 0.927\n",
            "epoch: 3, step: 342, loss: 0.869\n",
            "epoch: 3, step: 343, loss: 0.822\n",
            "epoch: 3, step: 344, loss: 0.900\n",
            "epoch: 3, step: 345, loss: 0.966\n",
            "epoch: 3, step: 346, loss: 0.972\n",
            "epoch: 3, step: 347, loss: 0.880\n",
            "epoch: 3, step: 348, loss: 0.904\n",
            "epoch: 3, step: 349, loss: 0.949\n",
            "epoch: 3, step: 350, loss: 0.846\n",
            "epoch: 3, step: 351, loss: 1.027\n",
            "epoch: 3, step: 352, loss: 0.910\n",
            "epoch: 3, step: 353, loss: 0.840\n",
            "epoch: 3, step: 354, loss: 0.868\n",
            "epoch: 3, step: 355, loss: 0.958\n",
            "epoch: 3, step: 356, loss: 0.837\n",
            "epoch: 3, step: 357, loss: 0.891\n",
            "epoch: 3, step: 358, loss: 0.783\n",
            "epoch: 3, step: 359, loss: 0.863\n",
            "epoch: 3, step: 360, loss: 0.989\n",
            "epoch: 3, step: 361, loss: 0.848\n",
            "epoch: 3, step: 362, loss: 0.943\n",
            "epoch: 3, step: 363, loss: 0.850\n",
            "epoch: 3, step: 364, loss: 0.942\n",
            "epoch: 3, step: 365, loss: 0.918\n",
            "epoch: 3, step: 366, loss: 0.738\n",
            "epoch: 3, step: 367, loss: 0.869\n",
            "epoch: 3, step: 368, loss: 0.853\n",
            "epoch: 3, step: 369, loss: 0.889\n",
            "epoch: 3, step: 370, loss: 0.985\n",
            "epoch: 3, step: 371, loss: 0.956\n",
            "epoch: 3, step: 372, loss: 1.029\n",
            "epoch: 3, step: 373, loss: 0.847\n",
            "epoch: 3, step: 374, loss: 0.913\n",
            "epoch: 3, step: 375, loss: 0.928\n",
            "epoch: 3, step: 376, loss: 0.951\n",
            "epoch: 3, step: 377, loss: 0.810\n",
            "epoch: 3, step: 378, loss: 0.823\n",
            "epoch: 3, step: 379, loss: 0.926\n",
            "epoch: 3, step: 380, loss: 0.868\n",
            "epoch: 3, step: 381, loss: 0.899\n",
            "epoch: 3, step: 382, loss: 0.959\n",
            "epoch: 3, step: 383, loss: 0.977\n",
            "epoch: 3, step: 384, loss: 0.902\n",
            "epoch: 3, step: 385, loss: 0.987\n",
            "epoch: 3, step: 386, loss: 0.859\n",
            "epoch: 3, step: 387, loss: 0.834\n",
            "epoch: 3, step: 388, loss: 0.796\n",
            "epoch: 3, step: 389, loss: 0.966\n",
            "epoch: 3, step: 390, loss: 0.962\n",
            "epoch: 3, step: 391, loss: 0.879\n",
            "epoch: 3, step: 392, loss: 0.901\n",
            "epoch: 3, step: 393, loss: 0.879\n",
            "epoch: 3, step: 394, loss: 0.921\n",
            "epoch: 3, step: 395, loss: 0.880\n",
            "epoch: 3, step: 396, loss: 0.888\n",
            "epoch: 3, step: 397, loss: 0.945\n",
            "epoch: 3, step: 398, loss: 0.830\n",
            "epoch: 3, step: 399, loss: 0.824\n",
            "epoch: 3, step: 400, loss: 0.933\n",
            "epoch: 3, step: 401, loss: 0.980\n",
            "epoch: 3, step: 402, loss: 0.879\n",
            "epoch: 3, step: 403, loss: 0.857\n",
            "epoch: 3, step: 404, loss: 1.010\n",
            "epoch: 3, step: 405, loss: 0.862\n",
            "epoch: 3, step: 406, loss: 0.879\n",
            "epoch: 3, step: 407, loss: 1.068\n",
            "epoch: 3, step: 408, loss: 0.895\n",
            "epoch: 3, step: 409, loss: 0.860\n",
            "epoch: 3, step: 410, loss: 0.894\n",
            "epoch: 3, step: 411, loss: 0.776\n",
            "epoch: 3, step: 412, loss: 0.881\n",
            "epoch: 3, step: 413, loss: 0.888\n",
            "epoch: 3, step: 414, loss: 0.898\n",
            "epoch: 3, step: 415, loss: 0.873\n",
            "epoch: 3, step: 416, loss: 0.842\n",
            "epoch: 3, step: 417, loss: 0.928\n",
            "epoch: 3, step: 418, loss: 0.871\n",
            "epoch: 3, step: 419, loss: 0.954\n",
            "epoch: 3, step: 420, loss: 0.875\n",
            "epoch: 3, step: 421, loss: 0.841\n",
            "epoch: 3, step: 422, loss: 0.814\n",
            "epoch: 3, step: 423, loss: 0.847\n",
            "epoch: 3, step: 424, loss: 0.865\n",
            "epoch: 3, step: 425, loss: 0.878\n",
            "epoch: 3, step: 426, loss: 0.836\n",
            "epoch: 3, step: 427, loss: 0.868\n",
            "epoch: 3, step: 428, loss: 0.921\n",
            "epoch: 3, step: 429, loss: 0.870\n",
            "epoch: 3, step: 430, loss: 0.884\n",
            "epoch: 3, step: 431, loss: 0.862\n",
            "epoch: 3, step: 432, loss: 0.857\n",
            "epoch: 3, step: 433, loss: 0.961\n",
            "epoch: 3, step: 434, loss: 0.881\n",
            "epoch: 3, step: 435, loss: 0.919\n",
            "epoch: 3, step: 436, loss: 0.922\n",
            "epoch: 3, step: 437, loss: 0.908\n",
            "epoch: 3, step: 438, loss: 0.901\n",
            "epoch: 3, step: 439, loss: 0.832\n",
            "epoch: 3, step: 440, loss: 0.882\n",
            "epoch: 3, step: 441, loss: 0.750\n",
            "epoch: 3, step: 442, loss: 0.906\n",
            "epoch: 3, step: 443, loss: 0.792\n",
            "epoch: 3, step: 444, loss: 0.789\n",
            "epoch: 3, step: 445, loss: 0.863\n",
            "epoch: 3, step: 446, loss: 0.849\n",
            "epoch: 3, step: 447, loss: 0.846\n",
            "epoch: 3, step: 448, loss: 0.820\n",
            "epoch: 3, step: 449, loss: 0.869\n",
            "epoch: 3, step: 450, loss: 0.874\n",
            "epoch: 3, step: 451, loss: 0.894\n",
            "epoch: 3, step: 452, loss: 0.833\n",
            "epoch: 3, step: 453, loss: 0.851\n",
            "epoch: 4, step: 1, loss: 0.833\n",
            "epoch: 4, step: 2, loss: 0.770\n",
            "epoch: 4, step: 3, loss: 0.906\n",
            "epoch: 4, step: 4, loss: 0.816\n",
            "epoch: 4, step: 5, loss: 0.865\n",
            "epoch: 4, step: 6, loss: 0.904\n",
            "epoch: 4, step: 7, loss: 0.796\n",
            "epoch: 4, step: 8, loss: 0.813\n",
            "epoch: 4, step: 9, loss: 0.858\n",
            "epoch: 4, step: 10, loss: 0.928\n",
            "epoch: 4, step: 11, loss: 0.908\n",
            "epoch: 4, step: 12, loss: 0.850\n",
            "epoch: 4, step: 13, loss: 0.881\n",
            "epoch: 4, step: 14, loss: 0.864\n",
            "epoch: 4, step: 15, loss: 0.861\n",
            "epoch: 4, step: 16, loss: 0.846\n",
            "epoch: 4, step: 17, loss: 0.844\n",
            "epoch: 4, step: 18, loss: 0.805\n",
            "epoch: 4, step: 19, loss: 0.833\n",
            "epoch: 4, step: 20, loss: 0.826\n",
            "epoch: 4, step: 21, loss: 0.913\n",
            "epoch: 4, step: 22, loss: 0.827\n",
            "epoch: 4, step: 23, loss: 0.829\n",
            "epoch: 4, step: 24, loss: 0.866\n",
            "epoch: 4, step: 25, loss: 0.855\n",
            "epoch: 4, step: 26, loss: 0.851\n",
            "epoch: 4, step: 27, loss: 0.822\n",
            "epoch: 4, step: 28, loss: 0.928\n",
            "epoch: 4, step: 29, loss: 0.918\n",
            "epoch: 4, step: 30, loss: 0.970\n",
            "epoch: 4, step: 31, loss: 0.777\n",
            "epoch: 4, step: 32, loss: 0.848\n",
            "epoch: 4, step: 33, loss: 0.770\n",
            "epoch: 4, step: 34, loss: 0.918\n",
            "epoch: 4, step: 35, loss: 0.804\n",
            "epoch: 4, step: 36, loss: 0.810\n",
            "epoch: 4, step: 37, loss: 0.887\n",
            "epoch: 4, step: 38, loss: 0.840\n",
            "epoch: 4, step: 39, loss: 0.803\n",
            "epoch: 4, step: 40, loss: 0.822\n",
            "epoch: 4, step: 41, loss: 0.935\n",
            "epoch: 4, step: 42, loss: 0.848\n",
            "epoch: 4, step: 43, loss: 0.820\n",
            "epoch: 4, step: 44, loss: 0.813\n",
            "epoch: 4, step: 45, loss: 0.878\n",
            "epoch: 4, step: 46, loss: 0.816\n",
            "epoch: 4, step: 47, loss: 0.858\n",
            "epoch: 4, step: 48, loss: 0.792\n",
            "epoch: 4, step: 49, loss: 0.867\n",
            "epoch: 4, step: 50, loss: 0.743\n",
            "epoch: 4, step: 51, loss: 0.898\n",
            "epoch: 4, step: 52, loss: 0.803\n",
            "epoch: 4, step: 53, loss: 0.823\n",
            "epoch: 4, step: 54, loss: 0.861\n",
            "epoch: 4, step: 55, loss: 0.832\n",
            "epoch: 4, step: 56, loss: 0.810\n",
            "epoch: 4, step: 57, loss: 0.756\n",
            "epoch: 4, step: 58, loss: 0.727\n",
            "epoch: 4, step: 59, loss: 0.814\n",
            "epoch: 4, step: 60, loss: 0.817\n",
            "epoch: 4, step: 61, loss: 0.863\n",
            "epoch: 4, step: 62, loss: 0.818\n",
            "epoch: 4, step: 63, loss: 0.898\n",
            "epoch: 4, step: 64, loss: 0.764\n",
            "epoch: 4, step: 65, loss: 0.834\n",
            "epoch: 4, step: 66, loss: 0.759\n",
            "epoch: 4, step: 67, loss: 0.961\n",
            "epoch: 4, step: 68, loss: 0.858\n",
            "epoch: 4, step: 69, loss: 0.878\n",
            "epoch: 4, step: 70, loss: 0.849\n",
            "epoch: 4, step: 71, loss: 0.851\n",
            "epoch: 4, step: 72, loss: 0.822\n",
            "epoch: 4, step: 73, loss: 0.820\n",
            "epoch: 4, step: 74, loss: 0.767\n",
            "epoch: 4, step: 75, loss: 0.869\n",
            "epoch: 4, step: 76, loss: 0.807\n",
            "epoch: 4, step: 77, loss: 0.792\n",
            "epoch: 4, step: 78, loss: 0.871\n",
            "epoch: 4, step: 79, loss: 0.822\n",
            "epoch: 4, step: 80, loss: 0.848\n",
            "epoch: 4, step: 81, loss: 0.798\n",
            "epoch: 4, step: 82, loss: 0.936\n",
            "epoch: 4, step: 83, loss: 0.863\n",
            "epoch: 4, step: 84, loss: 0.845\n",
            "epoch: 4, step: 85, loss: 0.797\n",
            "epoch: 4, step: 86, loss: 0.800\n",
            "epoch: 4, step: 87, loss: 0.844\n",
            "epoch: 4, step: 88, loss: 0.778\n",
            "epoch: 4, step: 89, loss: 0.868\n",
            "epoch: 4, step: 90, loss: 0.839\n",
            "epoch: 4, step: 91, loss: 0.861\n",
            "epoch: 4, step: 92, loss: 0.818\n",
            "epoch: 4, step: 93, loss: 0.795\n",
            "epoch: 4, step: 94, loss: 0.837\n",
            "epoch: 4, step: 95, loss: 0.932\n",
            "epoch: 4, step: 96, loss: 0.826\n",
            "epoch: 4, step: 97, loss: 0.818\n",
            "epoch: 4, step: 98, loss: 0.867\n",
            "epoch: 4, step: 99, loss: 0.810\n",
            "epoch: 4, step: 100, loss: 0.786\n",
            "epoch: 4, step: 101, loss: 0.846\n",
            "epoch: 4, step: 102, loss: 0.916\n",
            "epoch: 4, step: 103, loss: 0.834\n",
            "epoch: 4, step: 104, loss: 0.949\n",
            "epoch: 4, step: 105, loss: 0.867\n",
            "epoch: 4, step: 106, loss: 0.890\n",
            "epoch: 4, step: 107, loss: 0.908\n",
            "epoch: 4, step: 108, loss: 0.865\n",
            "epoch: 4, step: 109, loss: 0.843\n",
            "epoch: 4, step: 110, loss: 0.810\n",
            "epoch: 4, step: 111, loss: 0.956\n",
            "epoch: 4, step: 112, loss: 0.840\n",
            "epoch: 4, step: 113, loss: 0.922\n",
            "epoch: 4, step: 114, loss: 0.876\n",
            "epoch: 4, step: 115, loss: 0.793\n",
            "epoch: 4, step: 116, loss: 0.888\n",
            "epoch: 4, step: 117, loss: 0.786\n",
            "epoch: 4, step: 118, loss: 0.865\n",
            "epoch: 4, step: 119, loss: 0.794\n",
            "epoch: 4, step: 120, loss: 0.835\n",
            "epoch: 4, step: 121, loss: 0.890\n",
            "epoch: 4, step: 122, loss: 0.854\n",
            "epoch: 4, step: 123, loss: 0.790\n",
            "epoch: 4, step: 124, loss: 0.854\n",
            "epoch: 4, step: 125, loss: 0.804\n",
            "epoch: 4, step: 126, loss: 0.856\n",
            "epoch: 4, step: 127, loss: 0.803\n",
            "epoch: 4, step: 128, loss: 0.874\n",
            "epoch: 4, step: 129, loss: 0.776\n",
            "epoch: 4, step: 130, loss: 0.911\n",
            "epoch: 4, step: 131, loss: 0.788\n",
            "epoch: 4, step: 132, loss: 0.911\n",
            "epoch: 4, step: 133, loss: 0.883\n",
            "epoch: 4, step: 134, loss: 0.835\n",
            "epoch: 4, step: 135, loss: 0.796\n",
            "epoch: 4, step: 136, loss: 0.844\n",
            "epoch: 4, step: 137, loss: 0.787\n",
            "epoch: 4, step: 138, loss: 0.888\n",
            "epoch: 4, step: 139, loss: 0.949\n",
            "epoch: 4, step: 140, loss: 0.858\n",
            "epoch: 4, step: 141, loss: 0.844\n",
            "epoch: 4, step: 142, loss: 0.845\n",
            "epoch: 4, step: 143, loss: 0.748\n",
            "epoch: 4, step: 144, loss: 0.802\n",
            "epoch: 4, step: 145, loss: 0.922\n",
            "epoch: 4, step: 146, loss: 0.742\n",
            "epoch: 4, step: 147, loss: 0.764\n",
            "epoch: 4, step: 148, loss: 0.837\n",
            "epoch: 4, step: 149, loss: 0.875\n",
            "epoch: 4, step: 150, loss: 0.886\n",
            "epoch: 4, step: 151, loss: 0.832\n",
            "epoch: 4, step: 152, loss: 0.786\n",
            "epoch: 4, step: 153, loss: 0.789\n",
            "epoch: 4, step: 154, loss: 0.803\n",
            "epoch: 4, step: 155, loss: 0.758\n",
            "epoch: 4, step: 156, loss: 0.821\n",
            "epoch: 4, step: 157, loss: 0.806\n",
            "epoch: 4, step: 158, loss: 0.870\n",
            "epoch: 4, step: 159, loss: 0.736\n",
            "epoch: 4, step: 160, loss: 0.874\n",
            "epoch: 4, step: 161, loss: 0.835\n",
            "epoch: 4, step: 162, loss: 0.810\n",
            "epoch: 4, step: 163, loss: 0.803\n",
            "epoch: 4, step: 164, loss: 0.784\n",
            "epoch: 4, step: 165, loss: 0.746\n",
            "epoch: 4, step: 166, loss: 0.845\n",
            "epoch: 4, step: 167, loss: 0.871\n",
            "epoch: 4, step: 168, loss: 0.843\n",
            "epoch: 4, step: 169, loss: 0.833\n",
            "epoch: 4, step: 170, loss: 0.829\n",
            "epoch: 4, step: 171, loss: 0.937\n",
            "epoch: 4, step: 172, loss: 0.861\n",
            "epoch: 4, step: 173, loss: 0.766\n",
            "epoch: 4, step: 174, loss: 0.834\n",
            "epoch: 4, step: 175, loss: 0.813\n",
            "epoch: 4, step: 176, loss: 0.827\n",
            "epoch: 4, step: 177, loss: 0.841\n",
            "epoch: 4, step: 178, loss: 0.875\n",
            "epoch: 4, step: 179, loss: 0.871\n",
            "epoch: 4, step: 180, loss: 0.797\n",
            "epoch: 4, step: 181, loss: 0.810\n",
            "epoch: 4, step: 182, loss: 0.827\n",
            "epoch: 4, step: 183, loss: 0.767\n",
            "epoch: 4, step: 184, loss: 0.853\n",
            "epoch: 4, step: 185, loss: 0.842\n",
            "epoch: 4, step: 186, loss: 0.799\n",
            "epoch: 4, step: 187, loss: 0.884\n",
            "epoch: 4, step: 188, loss: 0.856\n",
            "epoch: 4, step: 189, loss: 0.859\n",
            "epoch: 4, step: 190, loss: 0.831\n",
            "epoch: 4, step: 191, loss: 0.821\n",
            "epoch: 4, step: 192, loss: 0.808\n",
            "epoch: 4, step: 193, loss: 0.776\n",
            "epoch: 4, step: 194, loss: 0.796\n",
            "epoch: 4, step: 195, loss: 0.861\n",
            "epoch: 4, step: 196, loss: 0.767\n",
            "epoch: 4, step: 197, loss: 0.866\n",
            "epoch: 4, step: 198, loss: 0.826\n",
            "epoch: 4, step: 199, loss: 0.887\n",
            "epoch: 4, step: 200, loss: 0.869\n",
            "epoch: 4, step: 201, loss: 0.763\n",
            "epoch: 4, step: 202, loss: 0.793\n",
            "epoch: 4, step: 203, loss: 0.732\n",
            "epoch: 4, step: 204, loss: 0.808\n",
            "epoch: 4, step: 205, loss: 0.833\n",
            "epoch: 4, step: 206, loss: 0.778\n",
            "epoch: 4, step: 207, loss: 0.862\n",
            "epoch: 4, step: 208, loss: 0.764\n",
            "epoch: 4, step: 209, loss: 0.732\n",
            "epoch: 4, step: 210, loss: 0.743\n",
            "epoch: 4, step: 211, loss: 0.847\n",
            "epoch: 4, step: 212, loss: 0.779\n",
            "epoch: 4, step: 213, loss: 0.813\n",
            "epoch: 4, step: 214, loss: 0.862\n",
            "epoch: 4, step: 215, loss: 0.783\n",
            "epoch: 4, step: 216, loss: 0.839\n",
            "epoch: 4, step: 217, loss: 0.816\n",
            "epoch: 4, step: 218, loss: 0.776\n",
            "epoch: 4, step: 219, loss: 0.859\n",
            "epoch: 4, step: 220, loss: 0.927\n",
            "epoch: 4, step: 221, loss: 0.846\n",
            "epoch: 4, step: 222, loss: 0.904\n",
            "epoch: 4, step: 223, loss: 0.799\n",
            "epoch: 4, step: 224, loss: 0.935\n",
            "epoch: 4, step: 225, loss: 0.788\n",
            "epoch: 4, step: 226, loss: 0.872\n",
            "epoch: 4, step: 227, loss: 0.797\n",
            "epoch: 4, step: 228, loss: 0.774\n",
            "epoch: 4, step: 229, loss: 0.874\n",
            "epoch: 4, step: 230, loss: 0.896\n",
            "epoch: 4, step: 231, loss: 0.760\n",
            "epoch: 4, step: 232, loss: 0.753\n",
            "epoch: 4, step: 233, loss: 0.790\n",
            "epoch: 4, step: 234, loss: 0.862\n",
            "epoch: 4, step: 235, loss: 0.873\n",
            "epoch: 4, step: 236, loss: 0.852\n",
            "epoch: 4, step: 237, loss: 0.747\n",
            "epoch: 4, step: 238, loss: 0.794\n",
            "epoch: 4, step: 239, loss: 0.812\n",
            "epoch: 4, step: 240, loss: 0.825\n",
            "epoch: 4, step: 241, loss: 0.744\n",
            "epoch: 4, step: 242, loss: 0.844\n",
            "epoch: 4, step: 243, loss: 0.750\n",
            "epoch: 4, step: 244, loss: 0.740\n",
            "epoch: 4, step: 245, loss: 0.839\n",
            "epoch: 4, step: 246, loss: 0.822\n",
            "epoch: 4, step: 247, loss: 0.815\n",
            "epoch: 4, step: 248, loss: 0.866\n",
            "epoch: 4, step: 249, loss: 0.851\n",
            "epoch: 4, step: 250, loss: 0.803\n",
            "epoch: 4, step: 251, loss: 0.855\n",
            "epoch: 4, step: 252, loss: 0.884\n",
            "epoch: 4, step: 253, loss: 0.741\n",
            "epoch: 4, step: 254, loss: 0.791\n",
            "epoch: 4, step: 255, loss: 0.908\n",
            "epoch: 4, step: 256, loss: 0.777\n",
            "epoch: 4, step: 257, loss: 0.841\n",
            "epoch: 4, step: 258, loss: 0.831\n",
            "epoch: 4, step: 259, loss: 0.781\n",
            "epoch: 4, step: 260, loss: 0.812\n",
            "epoch: 4, step: 261, loss: 0.787\n",
            "epoch: 4, step: 262, loss: 0.810\n",
            "epoch: 4, step: 263, loss: 0.790\n",
            "epoch: 4, step: 264, loss: 0.855\n",
            "epoch: 4, step: 265, loss: 0.796\n",
            "epoch: 4, step: 266, loss: 0.888\n",
            "epoch: 4, step: 267, loss: 0.858\n",
            "epoch: 4, step: 268, loss: 0.788\n",
            "epoch: 4, step: 269, loss: 0.859\n",
            "epoch: 4, step: 270, loss: 0.824\n",
            "epoch: 4, step: 271, loss: 0.908\n",
            "epoch: 4, step: 272, loss: 0.807\n",
            "epoch: 4, step: 273, loss: 0.805\n",
            "epoch: 4, step: 274, loss: 0.819\n",
            "epoch: 4, step: 275, loss: 0.808\n",
            "epoch: 4, step: 276, loss: 0.701\n",
            "epoch: 4, step: 277, loss: 0.861\n",
            "epoch: 4, step: 278, loss: 0.755\n",
            "epoch: 4, step: 279, loss: 0.767\n",
            "epoch: 4, step: 280, loss: 0.868\n",
            "epoch: 4, step: 281, loss: 0.807\n",
            "epoch: 4, step: 282, loss: 0.727\n",
            "epoch: 4, step: 283, loss: 0.931\n",
            "epoch: 4, step: 284, loss: 0.823\n",
            "epoch: 4, step: 285, loss: 0.842\n",
            "epoch: 4, step: 286, loss: 0.740\n",
            "epoch: 4, step: 287, loss: 0.798\n",
            "epoch: 4, step: 288, loss: 0.782\n",
            "epoch: 4, step: 289, loss: 0.802\n",
            "epoch: 4, step: 290, loss: 0.773\n",
            "epoch: 4, step: 291, loss: 0.732\n",
            "epoch: 4, step: 292, loss: 0.785\n",
            "epoch: 4, step: 293, loss: 0.864\n",
            "epoch: 4, step: 294, loss: 0.778\n",
            "epoch: 4, step: 295, loss: 0.781\n",
            "epoch: 4, step: 296, loss: 0.821\n",
            "epoch: 4, step: 297, loss: 0.913\n",
            "epoch: 4, step: 298, loss: 0.787\n",
            "epoch: 4, step: 299, loss: 0.766\n",
            "epoch: 4, step: 300, loss: 0.864\n",
            "epoch: 4, step: 301, loss: 0.841\n",
            "epoch: 4, step: 302, loss: 0.793\n",
            "epoch: 4, step: 303, loss: 0.805\n",
            "epoch: 4, step: 304, loss: 0.782\n",
            "epoch: 4, step: 305, loss: 0.839\n",
            "epoch: 4, step: 306, loss: 0.845\n",
            "epoch: 4, step: 307, loss: 0.813\n",
            "epoch: 4, step: 308, loss: 0.757\n",
            "epoch: 4, step: 309, loss: 0.913\n",
            "epoch: 4, step: 310, loss: 0.730\n",
            "epoch: 4, step: 311, loss: 0.848\n",
            "epoch: 4, step: 312, loss: 0.736\n",
            "epoch: 4, step: 313, loss: 0.761\n",
            "epoch: 4, step: 314, loss: 0.718\n",
            "epoch: 4, step: 315, loss: 0.825\n",
            "epoch: 4, step: 316, loss: 0.792\n",
            "epoch: 4, step: 317, loss: 0.819\n",
            "epoch: 4, step: 318, loss: 0.722\n",
            "epoch: 4, step: 319, loss: 0.801\n",
            "epoch: 4, step: 320, loss: 0.871\n",
            "epoch: 4, step: 321, loss: 0.868\n",
            "epoch: 4, step: 322, loss: 0.814\n",
            "epoch: 4, step: 323, loss: 0.768\n",
            "epoch: 4, step: 324, loss: 0.822\n",
            "epoch: 4, step: 325, loss: 0.839\n",
            "epoch: 4, step: 326, loss: 0.767\n",
            "epoch: 4, step: 327, loss: 0.740\n",
            "epoch: 4, step: 328, loss: 0.733\n",
            "epoch: 4, step: 329, loss: 0.738\n",
            "epoch: 4, step: 330, loss: 0.756\n",
            "epoch: 4, step: 331, loss: 0.806\n",
            "epoch: 4, step: 332, loss: 0.841\n",
            "epoch: 4, step: 333, loss: 0.837\n",
            "epoch: 4, step: 334, loss: 0.817\n",
            "epoch: 4, step: 335, loss: 0.780\n",
            "epoch: 4, step: 336, loss: 0.791\n",
            "epoch: 4, step: 337, loss: 0.892\n",
            "epoch: 4, step: 338, loss: 0.801\n",
            "epoch: 4, step: 339, loss: 0.716\n",
            "epoch: 4, step: 340, loss: 0.779\n",
            "epoch: 4, step: 341, loss: 0.731\n",
            "epoch: 4, step: 342, loss: 0.780\n",
            "epoch: 4, step: 343, loss: 0.810\n",
            "epoch: 4, step: 344, loss: 0.820\n",
            "epoch: 4, step: 345, loss: 0.690\n",
            "epoch: 4, step: 346, loss: 0.816\n",
            "epoch: 4, step: 347, loss: 0.744\n",
            "epoch: 4, step: 348, loss: 0.759\n",
            "epoch: 4, step: 349, loss: 0.862\n",
            "epoch: 4, step: 350, loss: 0.912\n",
            "epoch: 4, step: 351, loss: 0.756\n",
            "epoch: 4, step: 352, loss: 0.787\n",
            "epoch: 4, step: 353, loss: 0.796\n",
            "epoch: 4, step: 354, loss: 0.759\n",
            "epoch: 4, step: 355, loss: 0.805\n",
            "epoch: 4, step: 356, loss: 0.828\n",
            "epoch: 4, step: 357, loss: 0.894\n",
            "epoch: 4, step: 358, loss: 0.760\n",
            "epoch: 4, step: 359, loss: 0.745\n",
            "epoch: 4, step: 360, loss: 0.714\n",
            "epoch: 4, step: 361, loss: 0.792\n",
            "epoch: 4, step: 362, loss: 0.808\n",
            "epoch: 4, step: 363, loss: 0.760\n",
            "epoch: 4, step: 364, loss: 0.816\n",
            "epoch: 4, step: 365, loss: 0.885\n",
            "epoch: 4, step: 366, loss: 0.789\n",
            "epoch: 4, step: 367, loss: 0.745\n",
            "epoch: 4, step: 368, loss: 0.764\n",
            "epoch: 4, step: 369, loss: 0.775\n",
            "epoch: 4, step: 370, loss: 0.851\n",
            "epoch: 4, step: 371, loss: 0.747\n",
            "epoch: 4, step: 372, loss: 0.825\n",
            "epoch: 4, step: 373, loss: 0.782\n",
            "epoch: 4, step: 374, loss: 0.829\n",
            "epoch: 4, step: 375, loss: 0.834\n",
            "epoch: 4, step: 376, loss: 0.851\n",
            "epoch: 4, step: 377, loss: 0.762\n",
            "epoch: 4, step: 378, loss: 0.834\n",
            "epoch: 4, step: 379, loss: 0.855\n",
            "epoch: 4, step: 380, loss: 0.797\n",
            "epoch: 4, step: 381, loss: 0.787\n",
            "epoch: 4, step: 382, loss: 0.876\n",
            "epoch: 4, step: 383, loss: 0.827\n",
            "epoch: 4, step: 384, loss: 0.820\n",
            "epoch: 4, step: 385, loss: 0.759\n",
            "epoch: 4, step: 386, loss: 0.817\n",
            "epoch: 4, step: 387, loss: 0.773\n",
            "epoch: 4, step: 388, loss: 0.765\n",
            "epoch: 4, step: 389, loss: 0.760\n",
            "epoch: 4, step: 390, loss: 0.836\n",
            "epoch: 4, step: 391, loss: 0.814\n",
            "epoch: 4, step: 392, loss: 0.844\n",
            "epoch: 4, step: 393, loss: 0.789\n",
            "epoch: 4, step: 394, loss: 0.749\n",
            "epoch: 4, step: 395, loss: 0.819\n",
            "epoch: 4, step: 396, loss: 0.759\n",
            "epoch: 4, step: 397, loss: 0.820\n",
            "epoch: 4, step: 398, loss: 0.897\n",
            "epoch: 4, step: 399, loss: 0.735\n",
            "epoch: 4, step: 400, loss: 0.785\n",
            "epoch: 4, step: 401, loss: 0.867\n",
            "epoch: 4, step: 402, loss: 0.792\n",
            "epoch: 4, step: 403, loss: 0.911\n",
            "epoch: 4, step: 404, loss: 0.772\n",
            "epoch: 4, step: 405, loss: 0.732\n",
            "epoch: 4, step: 406, loss: 0.833\n",
            "epoch: 4, step: 407, loss: 0.883\n",
            "epoch: 4, step: 408, loss: 0.749\n",
            "epoch: 4, step: 409, loss: 0.837\n",
            "epoch: 4, step: 410, loss: 0.786\n",
            "epoch: 4, step: 411, loss: 0.867\n",
            "epoch: 4, step: 412, loss: 0.858\n",
            "epoch: 4, step: 413, loss: 0.806\n",
            "epoch: 4, step: 414, loss: 0.833\n",
            "epoch: 4, step: 415, loss: 0.808\n",
            "epoch: 4, step: 416, loss: 0.754\n",
            "epoch: 4, step: 417, loss: 0.792\n",
            "epoch: 4, step: 418, loss: 0.816\n",
            "epoch: 4, step: 419, loss: 0.836\n",
            "epoch: 4, step: 420, loss: 0.728\n",
            "epoch: 4, step: 421, loss: 0.826\n",
            "epoch: 4, step: 422, loss: 0.742\n",
            "epoch: 4, step: 423, loss: 0.748\n",
            "epoch: 4, step: 424, loss: 0.815\n",
            "epoch: 4, step: 425, loss: 0.780\n",
            "epoch: 4, step: 426, loss: 0.727\n",
            "epoch: 4, step: 427, loss: 0.862\n",
            "epoch: 4, step: 428, loss: 0.805\n",
            "epoch: 4, step: 429, loss: 0.832\n",
            "epoch: 4, step: 430, loss: 0.829\n",
            "epoch: 4, step: 431, loss: 0.778\n",
            "epoch: 4, step: 432, loss: 0.793\n",
            "epoch: 4, step: 433, loss: 0.817\n",
            "epoch: 4, step: 434, loss: 0.834\n",
            "epoch: 4, step: 435, loss: 0.811\n",
            "epoch: 4, step: 436, loss: 0.777\n",
            "epoch: 4, step: 437, loss: 0.821\n",
            "epoch: 4, step: 438, loss: 0.880\n",
            "epoch: 4, step: 439, loss: 0.837\n",
            "epoch: 4, step: 440, loss: 0.907\n",
            "epoch: 4, step: 441, loss: 0.802\n",
            "epoch: 4, step: 442, loss: 0.725\n",
            "epoch: 4, step: 443, loss: 0.928\n",
            "epoch: 4, step: 444, loss: 0.859\n",
            "epoch: 4, step: 445, loss: 0.832\n",
            "epoch: 4, step: 446, loss: 0.736\n",
            "epoch: 4, step: 447, loss: 0.832\n",
            "epoch: 4, step: 448, loss: 0.736\n",
            "epoch: 4, step: 449, loss: 0.906\n",
            "epoch: 4, step: 450, loss: 0.774\n",
            "epoch: 4, step: 451, loss: 0.708\n",
            "epoch: 4, step: 452, loss: 0.875\n",
            "epoch: 4, step: 453, loss: 0.794\n",
            "epoch: 5, step: 1, loss: 0.667\n",
            "epoch: 5, step: 2, loss: 0.802\n",
            "epoch: 5, step: 3, loss: 0.862\n",
            "epoch: 5, step: 4, loss: 0.748\n",
            "epoch: 5, step: 5, loss: 0.774\n",
            "epoch: 5, step: 6, loss: 0.698\n",
            "epoch: 5, step: 7, loss: 0.822\n",
            "epoch: 5, step: 8, loss: 0.853\n",
            "epoch: 5, step: 9, loss: 0.797\n",
            "epoch: 5, step: 10, loss: 0.696\n",
            "epoch: 5, step: 11, loss: 0.722\n",
            "epoch: 5, step: 12, loss: 0.806\n",
            "epoch: 5, step: 13, loss: 0.690\n",
            "epoch: 5, step: 14, loss: 0.721\n",
            "epoch: 5, step: 15, loss: 0.760\n",
            "epoch: 5, step: 16, loss: 0.770\n",
            "epoch: 5, step: 17, loss: 0.719\n",
            "epoch: 5, step: 18, loss: 0.700\n",
            "epoch: 5, step: 19, loss: 0.725\n",
            "epoch: 5, step: 20, loss: 0.739\n",
            "epoch: 5, step: 21, loss: 0.774\n",
            "epoch: 5, step: 22, loss: 0.718\n",
            "epoch: 5, step: 23, loss: 0.827\n",
            "epoch: 5, step: 24, loss: 0.808\n",
            "epoch: 5, step: 25, loss: 0.767\n",
            "epoch: 5, step: 26, loss: 0.732\n",
            "epoch: 5, step: 27, loss: 0.727\n",
            "epoch: 5, step: 28, loss: 0.826\n",
            "epoch: 5, step: 29, loss: 0.765\n",
            "epoch: 5, step: 30, loss: 0.802\n",
            "epoch: 5, step: 31, loss: 0.722\n",
            "epoch: 5, step: 32, loss: 0.824\n",
            "epoch: 5, step: 33, loss: 0.822\n",
            "epoch: 5, step: 34, loss: 0.702\n",
            "epoch: 5, step: 35, loss: 0.747\n",
            "epoch: 5, step: 36, loss: 0.764\n",
            "epoch: 5, step: 37, loss: 0.692\n",
            "epoch: 5, step: 38, loss: 0.792\n",
            "epoch: 5, step: 39, loss: 0.719\n",
            "epoch: 5, step: 40, loss: 0.756\n",
            "epoch: 5, step: 41, loss: 0.823\n",
            "epoch: 5, step: 42, loss: 0.743\n",
            "epoch: 5, step: 43, loss: 0.841\n",
            "epoch: 5, step: 44, loss: 0.722\n",
            "epoch: 5, step: 45, loss: 0.760\n",
            "epoch: 5, step: 46, loss: 0.701\n",
            "epoch: 5, step: 47, loss: 0.739\n",
            "epoch: 5, step: 48, loss: 0.856\n",
            "epoch: 5, step: 49, loss: 0.769\n",
            "epoch: 5, step: 50, loss: 0.720\n",
            "epoch: 5, step: 51, loss: 0.706\n",
            "epoch: 5, step: 52, loss: 0.747\n",
            "epoch: 5, step: 53, loss: 0.721\n",
            "epoch: 5, step: 54, loss: 0.754\n",
            "epoch: 5, step: 55, loss: 0.760\n",
            "epoch: 5, step: 56, loss: 0.666\n",
            "epoch: 5, step: 57, loss: 0.855\n",
            "epoch: 5, step: 58, loss: 0.715\n",
            "epoch: 5, step: 59, loss: 0.741\n",
            "epoch: 5, step: 60, loss: 0.765\n",
            "epoch: 5, step: 61, loss: 0.858\n",
            "epoch: 5, step: 62, loss: 0.765\n",
            "epoch: 5, step: 63, loss: 0.774\n",
            "epoch: 5, step: 64, loss: 0.775\n",
            "epoch: 5, step: 65, loss: 0.884\n",
            "epoch: 5, step: 66, loss: 0.753\n",
            "epoch: 5, step: 67, loss: 0.815\n",
            "epoch: 5, step: 68, loss: 0.797\n",
            "epoch: 5, step: 69, loss: 0.712\n",
            "epoch: 5, step: 70, loss: 0.673\n",
            "epoch: 5, step: 71, loss: 0.739\n",
            "epoch: 5, step: 72, loss: 0.794\n",
            "epoch: 5, step: 73, loss: 0.767\n",
            "epoch: 5, step: 74, loss: 0.887\n",
            "epoch: 5, step: 75, loss: 0.765\n",
            "epoch: 5, step: 76, loss: 0.672\n",
            "epoch: 5, step: 77, loss: 0.777\n",
            "epoch: 5, step: 78, loss: 0.736\n",
            "epoch: 5, step: 79, loss: 0.703\n",
            "epoch: 5, step: 80, loss: 0.740\n",
            "epoch: 5, step: 81, loss: 0.845\n",
            "epoch: 5, step: 82, loss: 0.881\n",
            "epoch: 5, step: 83, loss: 0.699\n",
            "epoch: 5, step: 84, loss: 0.732\n",
            "epoch: 5, step: 85, loss: 0.742\n",
            "epoch: 5, step: 86, loss: 0.759\n",
            "epoch: 5, step: 87, loss: 0.699\n",
            "epoch: 5, step: 88, loss: 0.766\n",
            "epoch: 5, step: 89, loss: 0.700\n",
            "epoch: 5, step: 90, loss: 0.810\n",
            "epoch: 5, step: 91, loss: 0.725\n",
            "epoch: 5, step: 92, loss: 0.827\n",
            "epoch: 5, step: 93, loss: 0.773\n",
            "epoch: 5, step: 94, loss: 0.768\n",
            "epoch: 5, step: 95, loss: 0.780\n",
            "epoch: 5, step: 96, loss: 0.724\n",
            "epoch: 5, step: 97, loss: 0.802\n",
            "epoch: 5, step: 98, loss: 0.722\n",
            "epoch: 5, step: 99, loss: 0.740\n",
            "epoch: 5, step: 100, loss: 0.719\n",
            "epoch: 5, step: 101, loss: 0.761\n",
            "epoch: 5, step: 102, loss: 0.775\n",
            "epoch: 5, step: 103, loss: 0.705\n",
            "epoch: 5, step: 104, loss: 0.755\n",
            "epoch: 5, step: 105, loss: 0.773\n",
            "epoch: 5, step: 106, loss: 0.765\n",
            "epoch: 5, step: 107, loss: 0.712\n",
            "epoch: 5, step: 108, loss: 0.736\n",
            "epoch: 5, step: 109, loss: 0.785\n",
            "epoch: 5, step: 110, loss: 0.716\n",
            "epoch: 5, step: 111, loss: 0.712\n",
            "epoch: 5, step: 112, loss: 0.765\n",
            "epoch: 5, step: 113, loss: 0.761\n",
            "epoch: 5, step: 114, loss: 0.712\n",
            "epoch: 5, step: 115, loss: 0.788\n",
            "epoch: 5, step: 116, loss: 0.690\n",
            "epoch: 5, step: 117, loss: 0.725\n",
            "epoch: 5, step: 118, loss: 0.752\n",
            "epoch: 5, step: 119, loss: 0.771\n",
            "epoch: 5, step: 120, loss: 0.764\n",
            "epoch: 5, step: 121, loss: 0.689\n",
            "epoch: 5, step: 122, loss: 0.756\n",
            "epoch: 5, step: 123, loss: 0.729\n",
            "epoch: 5, step: 124, loss: 0.759\n",
            "epoch: 5, step: 125, loss: 0.673\n",
            "epoch: 5, step: 126, loss: 0.826\n",
            "epoch: 5, step: 127, loss: 0.648\n",
            "epoch: 5, step: 128, loss: 0.664\n",
            "epoch: 5, step: 129, loss: 0.711\n",
            "epoch: 5, step: 130, loss: 0.720\n",
            "epoch: 5, step: 131, loss: 0.735\n",
            "epoch: 5, step: 132, loss: 0.741\n",
            "epoch: 5, step: 133, loss: 0.772\n",
            "epoch: 5, step: 134, loss: 0.814\n",
            "epoch: 5, step: 135, loss: 0.787\n",
            "epoch: 5, step: 136, loss: 0.737\n",
            "epoch: 5, step: 137, loss: 0.747\n",
            "epoch: 5, step: 138, loss: 0.767\n",
            "epoch: 5, step: 139, loss: 0.708\n",
            "epoch: 5, step: 140, loss: 0.798\n",
            "epoch: 5, step: 141, loss: 0.745\n",
            "epoch: 5, step: 142, loss: 0.743\n",
            "epoch: 5, step: 143, loss: 0.791\n",
            "epoch: 5, step: 144, loss: 0.837\n",
            "epoch: 5, step: 145, loss: 0.675\n",
            "epoch: 5, step: 146, loss: 0.697\n",
            "epoch: 5, step: 147, loss: 0.724\n",
            "epoch: 5, step: 148, loss: 0.703\n",
            "epoch: 5, step: 149, loss: 0.770\n",
            "epoch: 5, step: 150, loss: 0.752\n",
            "epoch: 5, step: 151, loss: 0.780\n",
            "epoch: 5, step: 152, loss: 0.703\n",
            "epoch: 5, step: 153, loss: 0.830\n",
            "epoch: 5, step: 154, loss: 0.671\n",
            "epoch: 5, step: 155, loss: 0.772\n",
            "epoch: 5, step: 156, loss: 0.745\n",
            "epoch: 5, step: 157, loss: 0.749\n",
            "epoch: 5, step: 158, loss: 0.817\n",
            "epoch: 5, step: 159, loss: 0.773\n",
            "epoch: 5, step: 160, loss: 0.772\n",
            "epoch: 5, step: 161, loss: 0.715\n",
            "epoch: 5, step: 162, loss: 0.698\n",
            "epoch: 5, step: 163, loss: 0.688\n",
            "epoch: 5, step: 164, loss: 0.710\n",
            "epoch: 5, step: 165, loss: 0.741\n",
            "epoch: 5, step: 166, loss: 0.784\n",
            "epoch: 5, step: 167, loss: 0.753\n",
            "epoch: 5, step: 168, loss: 0.619\n",
            "epoch: 5, step: 169, loss: 0.811\n",
            "epoch: 5, step: 170, loss: 0.674\n",
            "epoch: 5, step: 171, loss: 0.809\n",
            "epoch: 5, step: 172, loss: 0.665\n",
            "epoch: 5, step: 173, loss: 0.857\n",
            "epoch: 5, step: 174, loss: 0.760\n",
            "epoch: 5, step: 175, loss: 0.716\n",
            "epoch: 5, step: 176, loss: 0.754\n",
            "epoch: 5, step: 177, loss: 0.676\n",
            "epoch: 5, step: 178, loss: 0.760\n",
            "epoch: 5, step: 179, loss: 0.679\n",
            "epoch: 5, step: 180, loss: 0.713\n",
            "epoch: 5, step: 181, loss: 0.694\n",
            "epoch: 5, step: 182, loss: 0.707\n",
            "epoch: 5, step: 183, loss: 0.687\n",
            "epoch: 5, step: 184, loss: 0.842\n",
            "epoch: 5, step: 185, loss: 0.862\n",
            "epoch: 5, step: 186, loss: 0.777\n",
            "epoch: 5, step: 187, loss: 0.782\n",
            "epoch: 5, step: 188, loss: 0.722\n",
            "epoch: 5, step: 189, loss: 0.665\n",
            "epoch: 5, step: 190, loss: 0.793\n",
            "epoch: 5, step: 191, loss: 0.766\n",
            "epoch: 5, step: 192, loss: 0.752\n",
            "epoch: 5, step: 193, loss: 0.689\n",
            "epoch: 5, step: 194, loss: 0.809\n",
            "epoch: 5, step: 195, loss: 0.781\n",
            "epoch: 5, step: 196, loss: 0.797\n",
            "epoch: 5, step: 197, loss: 0.733\n",
            "epoch: 5, step: 198, loss: 0.709\n",
            "epoch: 5, step: 199, loss: 0.714\n",
            "epoch: 5, step: 200, loss: 0.703\n",
            "epoch: 5, step: 201, loss: 0.701\n",
            "epoch: 5, step: 202, loss: 0.756\n",
            "epoch: 5, step: 203, loss: 0.764\n",
            "epoch: 5, step: 204, loss: 0.750\n",
            "epoch: 5, step: 205, loss: 0.735\n",
            "epoch: 5, step: 206, loss: 0.809\n",
            "epoch: 5, step: 207, loss: 0.815\n",
            "epoch: 5, step: 208, loss: 0.737\n",
            "epoch: 5, step: 209, loss: 0.635\n",
            "epoch: 5, step: 210, loss: 0.726\n",
            "epoch: 5, step: 211, loss: 0.777\n",
            "epoch: 5, step: 212, loss: 0.650\n",
            "epoch: 5, step: 213, loss: 0.772\n",
            "epoch: 5, step: 214, loss: 0.774\n",
            "epoch: 5, step: 215, loss: 0.795\n",
            "epoch: 5, step: 216, loss: 0.703\n",
            "epoch: 5, step: 217, loss: 0.757\n",
            "epoch: 5, step: 218, loss: 0.735\n",
            "epoch: 5, step: 219, loss: 0.738\n",
            "epoch: 5, step: 220, loss: 0.709\n",
            "epoch: 5, step: 221, loss: 0.772\n",
            "epoch: 5, step: 222, loss: 0.820\n",
            "epoch: 5, step: 223, loss: 0.731\n",
            "epoch: 5, step: 224, loss: 0.774\n",
            "epoch: 5, step: 225, loss: 0.751\n",
            "epoch: 5, step: 226, loss: 0.717\n",
            "epoch: 5, step: 227, loss: 0.806\n",
            "epoch: 5, step: 228, loss: 0.752\n",
            "epoch: 5, step: 229, loss: 0.683\n",
            "epoch: 5, step: 230, loss: 0.687\n",
            "epoch: 5, step: 231, loss: 0.716\n",
            "epoch: 5, step: 232, loss: 0.751\n",
            "epoch: 5, step: 233, loss: 0.826\n",
            "epoch: 5, step: 234, loss: 0.812\n",
            "epoch: 5, step: 235, loss: 0.776\n",
            "epoch: 5, step: 236, loss: 0.697\n",
            "epoch: 5, step: 237, loss: 0.919\n",
            "epoch: 5, step: 238, loss: 0.738\n",
            "epoch: 5, step: 239, loss: 0.682\n",
            "epoch: 5, step: 240, loss: 0.696\n",
            "epoch: 5, step: 241, loss: 0.816\n",
            "epoch: 5, step: 242, loss: 0.697\n",
            "epoch: 5, step: 243, loss: 0.818\n",
            "epoch: 5, step: 244, loss: 0.704\n",
            "epoch: 5, step: 245, loss: 0.758\n",
            "epoch: 5, step: 246, loss: 0.792\n",
            "epoch: 5, step: 247, loss: 0.682\n",
            "epoch: 5, step: 248, loss: 0.781\n",
            "epoch: 5, step: 249, loss: 0.662\n",
            "epoch: 5, step: 250, loss: 0.829\n",
            "epoch: 5, step: 251, loss: 0.785\n",
            "epoch: 5, step: 252, loss: 0.729\n",
            "epoch: 5, step: 253, loss: 0.811\n",
            "epoch: 5, step: 254, loss: 0.678\n",
            "epoch: 5, step: 255, loss: 0.737\n",
            "epoch: 5, step: 256, loss: 0.752\n",
            "epoch: 5, step: 257, loss: 0.726\n",
            "epoch: 5, step: 258, loss: 0.728\n",
            "epoch: 5, step: 259, loss: 0.684\n",
            "epoch: 5, step: 260, loss: 0.763\n",
            "epoch: 5, step: 261, loss: 0.672\n",
            "epoch: 5, step: 262, loss: 0.718\n",
            "epoch: 5, step: 263, loss: 0.781\n",
            "epoch: 5, step: 264, loss: 0.711\n",
            "epoch: 5, step: 265, loss: 0.762\n",
            "epoch: 5, step: 266, loss: 0.707\n",
            "epoch: 5, step: 267, loss: 0.650\n",
            "epoch: 5, step: 268, loss: 0.826\n",
            "epoch: 5, step: 269, loss: 0.782\n",
            "epoch: 5, step: 270, loss: 0.702\n",
            "epoch: 5, step: 271, loss: 0.682\n",
            "epoch: 5, step: 272, loss: 0.688\n",
            "epoch: 5, step: 273, loss: 0.776\n",
            "epoch: 5, step: 274, loss: 0.824\n",
            "epoch: 5, step: 275, loss: 0.698\n",
            "epoch: 5, step: 276, loss: 0.699\n",
            "epoch: 5, step: 277, loss: 0.686\n",
            "epoch: 5, step: 278, loss: 0.818\n",
            "epoch: 5, step: 279, loss: 0.791\n",
            "epoch: 5, step: 280, loss: 0.851\n",
            "epoch: 5, step: 281, loss: 0.728\n",
            "epoch: 5, step: 282, loss: 0.665\n",
            "epoch: 5, step: 283, loss: 0.772\n",
            "epoch: 5, step: 284, loss: 0.689\n",
            "epoch: 5, step: 285, loss: 0.682\n",
            "epoch: 5, step: 286, loss: 0.648\n",
            "epoch: 5, step: 287, loss: 0.784\n",
            "epoch: 5, step: 288, loss: 0.741\n",
            "epoch: 5, step: 289, loss: 0.807\n",
            "epoch: 5, step: 290, loss: 0.823\n",
            "epoch: 5, step: 291, loss: 0.690\n",
            "epoch: 5, step: 292, loss: 0.755\n",
            "epoch: 5, step: 293, loss: 0.845\n",
            "epoch: 5, step: 294, loss: 0.811\n",
            "epoch: 5, step: 295, loss: 0.745\n",
            "epoch: 5, step: 296, loss: 0.779\n",
            "epoch: 5, step: 297, loss: 0.718\n",
            "epoch: 5, step: 298, loss: 0.790\n",
            "epoch: 5, step: 299, loss: 0.734\n",
            "epoch: 5, step: 300, loss: 0.778\n",
            "epoch: 5, step: 301, loss: 0.714\n",
            "epoch: 5, step: 302, loss: 0.683\n",
            "epoch: 5, step: 303, loss: 0.739\n",
            "epoch: 5, step: 304, loss: 0.628\n",
            "epoch: 5, step: 305, loss: 0.743\n",
            "epoch: 5, step: 306, loss: 0.684\n",
            "epoch: 5, step: 307, loss: 0.799\n",
            "epoch: 5, step: 308, loss: 0.789\n",
            "epoch: 5, step: 309, loss: 0.680\n",
            "epoch: 5, step: 310, loss: 0.664\n",
            "epoch: 5, step: 311, loss: 0.780\n",
            "epoch: 5, step: 312, loss: 0.730\n",
            "epoch: 5, step: 313, loss: 0.766\n",
            "epoch: 5, step: 314, loss: 0.694\n",
            "epoch: 5, step: 315, loss: 0.750\n",
            "epoch: 5, step: 316, loss: 0.701\n",
            "epoch: 5, step: 317, loss: 0.745\n",
            "epoch: 5, step: 318, loss: 0.802\n",
            "epoch: 5, step: 319, loss: 0.767\n",
            "epoch: 5, step: 320, loss: 0.824\n",
            "epoch: 5, step: 321, loss: 0.717\n",
            "epoch: 5, step: 322, loss: 0.702\n",
            "epoch: 5, step: 323, loss: 0.689\n",
            "epoch: 5, step: 324, loss: 0.676\n",
            "epoch: 5, step: 325, loss: 0.769\n",
            "epoch: 5, step: 326, loss: 0.699\n",
            "epoch: 5, step: 327, loss: 0.665\n",
            "epoch: 5, step: 328, loss: 0.739\n",
            "epoch: 5, step: 329, loss: 0.736\n",
            "epoch: 5, step: 330, loss: 0.710\n",
            "epoch: 5, step: 331, loss: 0.725\n",
            "epoch: 5, step: 332, loss: 0.736\n",
            "epoch: 5, step: 333, loss: 0.804\n",
            "epoch: 5, step: 334, loss: 0.708\n",
            "epoch: 5, step: 335, loss: 0.710\n",
            "epoch: 5, step: 336, loss: 0.662\n",
            "epoch: 5, step: 337, loss: 0.763\n",
            "epoch: 5, step: 338, loss: 0.736\n",
            "epoch: 5, step: 339, loss: 0.666\n",
            "epoch: 5, step: 340, loss: 0.813\n",
            "epoch: 5, step: 341, loss: 0.690\n",
            "epoch: 5, step: 342, loss: 0.710\n",
            "epoch: 5, step: 343, loss: 0.708\n",
            "epoch: 5, step: 344, loss: 0.624\n",
            "epoch: 5, step: 345, loss: 0.746\n",
            "epoch: 5, step: 346, loss: 0.611\n",
            "epoch: 5, step: 347, loss: 0.642\n",
            "epoch: 5, step: 348, loss: 0.786\n",
            "epoch: 5, step: 349, loss: 0.676\n",
            "epoch: 5, step: 350, loss: 0.773\n",
            "epoch: 5, step: 351, loss: 0.663\n",
            "epoch: 5, step: 352, loss: 0.727\n",
            "epoch: 5, step: 353, loss: 0.759\n",
            "epoch: 5, step: 354, loss: 0.671\n",
            "epoch: 5, step: 355, loss: 0.737\n",
            "epoch: 5, step: 356, loss: 0.673\n",
            "epoch: 5, step: 357, loss: 0.674\n",
            "epoch: 5, step: 358, loss: 0.653\n",
            "epoch: 5, step: 359, loss: 0.748\n",
            "epoch: 5, step: 360, loss: 0.721\n",
            "epoch: 5, step: 361, loss: 0.729\n",
            "epoch: 5, step: 362, loss: 0.654\n",
            "epoch: 5, step: 363, loss: 0.773\n",
            "epoch: 5, step: 364, loss: 0.678\n",
            "epoch: 5, step: 365, loss: 0.716\n",
            "epoch: 5, step: 366, loss: 0.724\n",
            "epoch: 5, step: 367, loss: 0.708\n",
            "epoch: 5, step: 368, loss: 0.665\n",
            "epoch: 5, step: 369, loss: 0.698\n",
            "epoch: 5, step: 370, loss: 0.750\n",
            "epoch: 5, step: 371, loss: 0.698\n",
            "epoch: 5, step: 372, loss: 0.760\n",
            "epoch: 5, step: 373, loss: 0.673\n",
            "epoch: 5, step: 374, loss: 0.599\n",
            "epoch: 5, step: 375, loss: 0.700\n",
            "epoch: 5, step: 376, loss: 0.773\n",
            "epoch: 5, step: 377, loss: 0.723\n",
            "epoch: 5, step: 378, loss: 0.833\n",
            "epoch: 5, step: 379, loss: 0.698\n",
            "epoch: 5, step: 380, loss: 0.675\n",
            "epoch: 5, step: 381, loss: 0.662\n",
            "epoch: 5, step: 382, loss: 0.783\n",
            "epoch: 5, step: 383, loss: 0.687\n",
            "epoch: 5, step: 384, loss: 0.639\n",
            "epoch: 5, step: 385, loss: 0.844\n",
            "epoch: 5, step: 386, loss: 0.695\n",
            "epoch: 5, step: 387, loss: 0.727\n",
            "epoch: 5, step: 388, loss: 0.694\n",
            "epoch: 5, step: 389, loss: 0.712\n",
            "epoch: 5, step: 390, loss: 0.663\n",
            "epoch: 5, step: 391, loss: 0.704\n",
            "epoch: 5, step: 392, loss: 0.741\n",
            "epoch: 5, step: 393, loss: 0.689\n",
            "epoch: 5, step: 394, loss: 0.833\n",
            "epoch: 5, step: 395, loss: 0.692\n",
            "epoch: 5, step: 396, loss: 0.719\n",
            "epoch: 5, step: 397, loss: 0.718\n",
            "epoch: 5, step: 398, loss: 0.785\n",
            "epoch: 5, step: 399, loss: 0.666\n",
            "epoch: 5, step: 400, loss: 0.745\n",
            "epoch: 5, step: 401, loss: 0.727\n",
            "epoch: 5, step: 402, loss: 0.736\n",
            "epoch: 5, step: 403, loss: 0.671\n",
            "epoch: 5, step: 404, loss: 0.666\n",
            "epoch: 5, step: 405, loss: 0.885\n",
            "epoch: 5, step: 406, loss: 0.767\n",
            "epoch: 5, step: 407, loss: 0.724\n",
            "epoch: 5, step: 408, loss: 0.907\n",
            "epoch: 5, step: 409, loss: 0.716\n",
            "epoch: 5, step: 410, loss: 0.727\n",
            "epoch: 5, step: 411, loss: 0.727\n",
            "epoch: 5, step: 412, loss: 0.826\n",
            "epoch: 5, step: 413, loss: 0.734\n",
            "epoch: 5, step: 414, loss: 0.642\n",
            "epoch: 5, step: 415, loss: 0.723\n",
            "epoch: 5, step: 416, loss: 0.669\n",
            "epoch: 5, step: 417, loss: 0.724\n",
            "epoch: 5, step: 418, loss: 0.722\n",
            "epoch: 5, step: 419, loss: 0.831\n",
            "epoch: 5, step: 420, loss: 0.678\n",
            "epoch: 5, step: 421, loss: 0.736\n",
            "epoch: 5, step: 422, loss: 0.705\n",
            "epoch: 5, step: 423, loss: 0.798\n",
            "epoch: 5, step: 424, loss: 0.705\n",
            "epoch: 5, step: 425, loss: 0.711\n",
            "epoch: 5, step: 426, loss: 0.740\n",
            "epoch: 5, step: 427, loss: 0.818\n",
            "epoch: 5, step: 428, loss: 0.703\n",
            "epoch: 5, step: 429, loss: 0.744\n",
            "epoch: 5, step: 430, loss: 0.781\n",
            "epoch: 5, step: 431, loss: 0.820\n",
            "epoch: 5, step: 432, loss: 0.763\n",
            "epoch: 5, step: 433, loss: 0.802\n",
            "epoch: 5, step: 434, loss: 0.706\n",
            "epoch: 5, step: 435, loss: 0.692\n",
            "epoch: 5, step: 436, loss: 0.770\n",
            "epoch: 5, step: 437, loss: 0.834\n",
            "epoch: 5, step: 438, loss: 0.782\n",
            "epoch: 5, step: 439, loss: 0.769\n",
            "epoch: 5, step: 440, loss: 0.758\n",
            "epoch: 5, step: 441, loss: 0.675\n",
            "epoch: 5, step: 442, loss: 0.759\n",
            "epoch: 5, step: 443, loss: 0.708\n",
            "epoch: 5, step: 444, loss: 0.737\n",
            "epoch: 5, step: 445, loss: 0.660\n",
            "epoch: 5, step: 446, loss: 0.758\n",
            "epoch: 5, step: 447, loss: 0.821\n",
            "epoch: 5, step: 448, loss: 0.680\n",
            "epoch: 5, step: 449, loss: 0.763\n",
            "epoch: 5, step: 450, loss: 0.696\n",
            "epoch: 5, step: 451, loss: 0.669\n",
            "epoch: 5, step: 452, loss: 0.627\n",
            "epoch: 5, step: 453, loss: 0.713\n",
            "epoch: 6, step: 1, loss: 0.677\n",
            "epoch: 6, step: 2, loss: 0.637\n",
            "epoch: 6, step: 3, loss: 0.824\n",
            "epoch: 6, step: 4, loss: 0.686\n",
            "epoch: 6, step: 5, loss: 0.769\n",
            "epoch: 6, step: 6, loss: 0.674\n",
            "epoch: 6, step: 7, loss: 0.732\n",
            "epoch: 6, step: 8, loss: 0.654\n",
            "epoch: 6, step: 9, loss: 0.657\n",
            "epoch: 6, step: 10, loss: 0.714\n",
            "epoch: 6, step: 11, loss: 0.715\n",
            "epoch: 6, step: 12, loss: 0.628\n",
            "epoch: 6, step: 13, loss: 0.625\n",
            "epoch: 6, step: 14, loss: 0.635\n",
            "epoch: 6, step: 15, loss: 0.713\n",
            "epoch: 6, step: 16, loss: 0.696\n",
            "epoch: 6, step: 17, loss: 0.710\n",
            "epoch: 6, step: 18, loss: 0.659\n",
            "epoch: 6, step: 19, loss: 0.723\n",
            "epoch: 6, step: 20, loss: 0.641\n",
            "epoch: 6, step: 21, loss: 0.639\n",
            "epoch: 6, step: 22, loss: 0.771\n",
            "epoch: 6, step: 23, loss: 0.616\n",
            "epoch: 6, step: 24, loss: 0.732\n",
            "epoch: 6, step: 25, loss: 0.731\n",
            "epoch: 6, step: 26, loss: 0.684\n",
            "epoch: 6, step: 27, loss: 0.632\n",
            "epoch: 6, step: 28, loss: 0.722\n",
            "epoch: 6, step: 29, loss: 0.646\n",
            "epoch: 6, step: 30, loss: 0.725\n",
            "epoch: 6, step: 31, loss: 0.638\n",
            "epoch: 6, step: 32, loss: 0.673\n",
            "epoch: 6, step: 33, loss: 0.690\n",
            "epoch: 6, step: 34, loss: 0.631\n",
            "epoch: 6, step: 35, loss: 0.634\n",
            "epoch: 6, step: 36, loss: 0.677\n",
            "epoch: 6, step: 37, loss: 0.738\n",
            "epoch: 6, step: 38, loss: 0.657\n",
            "epoch: 6, step: 39, loss: 0.623\n",
            "epoch: 6, step: 40, loss: 0.630\n",
            "epoch: 6, step: 41, loss: 0.635\n",
            "epoch: 6, step: 42, loss: 0.684\n",
            "epoch: 6, step: 43, loss: 0.797\n",
            "epoch: 6, step: 44, loss: 0.717\n",
            "epoch: 6, step: 45, loss: 0.630\n",
            "epoch: 6, step: 46, loss: 0.620\n",
            "epoch: 6, step: 47, loss: 0.665\n",
            "epoch: 6, step: 48, loss: 0.684\n",
            "epoch: 6, step: 49, loss: 0.723\n",
            "epoch: 6, step: 50, loss: 0.655\n",
            "epoch: 6, step: 51, loss: 0.616\n",
            "epoch: 6, step: 52, loss: 0.570\n",
            "epoch: 6, step: 53, loss: 0.723\n",
            "epoch: 6, step: 54, loss: 0.692\n",
            "epoch: 6, step: 55, loss: 0.748\n",
            "epoch: 6, step: 56, loss: 0.643\n",
            "epoch: 6, step: 57, loss: 0.573\n",
            "epoch: 6, step: 58, loss: 0.662\n",
            "epoch: 6, step: 59, loss: 0.701\n",
            "epoch: 6, step: 60, loss: 0.744\n",
            "epoch: 6, step: 61, loss: 0.658\n",
            "epoch: 6, step: 62, loss: 0.563\n",
            "epoch: 6, step: 63, loss: 0.709\n",
            "epoch: 6, step: 64, loss: 0.678\n",
            "epoch: 6, step: 65, loss: 0.690\n",
            "epoch: 6, step: 66, loss: 0.669\n",
            "epoch: 6, step: 67, loss: 0.670\n",
            "epoch: 6, step: 68, loss: 0.675\n",
            "epoch: 6, step: 69, loss: 0.682\n",
            "epoch: 6, step: 70, loss: 0.706\n",
            "epoch: 6, step: 71, loss: 0.756\n",
            "epoch: 6, step: 72, loss: 0.776\n",
            "epoch: 6, step: 73, loss: 0.726\n",
            "epoch: 6, step: 74, loss: 0.722\n",
            "epoch: 6, step: 75, loss: 0.715\n",
            "epoch: 6, step: 76, loss: 0.746\n",
            "epoch: 6, step: 77, loss: 0.747\n",
            "epoch: 6, step: 78, loss: 0.641\n",
            "epoch: 6, step: 79, loss: 0.619\n",
            "epoch: 6, step: 80, loss: 0.662\n",
            "epoch: 6, step: 81, loss: 0.697\n",
            "epoch: 6, step: 82, loss: 0.660\n",
            "epoch: 6, step: 83, loss: 0.662\n",
            "epoch: 6, step: 84, loss: 0.656\n",
            "epoch: 6, step: 85, loss: 0.626\n",
            "epoch: 6, step: 86, loss: 0.685\n",
            "epoch: 6, step: 87, loss: 0.693\n",
            "epoch: 6, step: 88, loss: 0.712\n",
            "epoch: 6, step: 89, loss: 0.806\n",
            "epoch: 6, step: 90, loss: 0.677\n",
            "epoch: 6, step: 91, loss: 0.614\n",
            "epoch: 6, step: 92, loss: 0.623\n",
            "epoch: 6, step: 93, loss: 0.619\n",
            "epoch: 6, step: 94, loss: 0.674\n",
            "epoch: 6, step: 95, loss: 0.803\n",
            "epoch: 6, step: 96, loss: 0.664\n",
            "epoch: 6, step: 97, loss: 0.666\n",
            "epoch: 6, step: 98, loss: 0.634\n",
            "epoch: 6, step: 99, loss: 0.723\n",
            "epoch: 6, step: 100, loss: 0.709\n",
            "epoch: 6, step: 101, loss: 0.648\n",
            "epoch: 6, step: 102, loss: 0.738\n",
            "epoch: 6, step: 103, loss: 0.623\n",
            "epoch: 6, step: 104, loss: 0.646\n",
            "epoch: 6, step: 105, loss: 0.706\n",
            "epoch: 6, step: 106, loss: 0.669\n",
            "epoch: 6, step: 107, loss: 0.742\n",
            "epoch: 6, step: 108, loss: 0.608\n",
            "epoch: 6, step: 109, loss: 0.740\n",
            "epoch: 6, step: 110, loss: 0.632\n",
            "epoch: 6, step: 111, loss: 0.725\n",
            "epoch: 6, step: 112, loss: 0.708\n",
            "epoch: 6, step: 113, loss: 0.652\n",
            "epoch: 6, step: 114, loss: 0.706\n",
            "epoch: 6, step: 115, loss: 0.628\n",
            "epoch: 6, step: 116, loss: 0.608\n",
            "epoch: 6, step: 117, loss: 0.661\n",
            "epoch: 6, step: 118, loss: 0.709\n",
            "epoch: 6, step: 119, loss: 0.641\n",
            "epoch: 6, step: 120, loss: 0.605\n",
            "epoch: 6, step: 121, loss: 0.752\n",
            "epoch: 6, step: 122, loss: 0.655\n",
            "epoch: 6, step: 123, loss: 0.783\n",
            "epoch: 6, step: 124, loss: 0.652\n",
            "epoch: 6, step: 125, loss: 0.643\n",
            "epoch: 6, step: 126, loss: 0.777\n",
            "epoch: 6, step: 127, loss: 0.736\n",
            "epoch: 6, step: 128, loss: 0.605\n",
            "epoch: 6, step: 129, loss: 0.750\n",
            "epoch: 6, step: 130, loss: 0.704\n",
            "epoch: 6, step: 131, loss: 0.639\n",
            "epoch: 6, step: 132, loss: 0.649\n",
            "epoch: 6, step: 133, loss: 0.728\n",
            "epoch: 6, step: 134, loss: 0.703\n",
            "epoch: 6, step: 135, loss: 0.736\n",
            "epoch: 6, step: 136, loss: 0.669\n",
            "epoch: 6, step: 137, loss: 0.581\n",
            "epoch: 6, step: 138, loss: 0.627\n",
            "epoch: 6, step: 139, loss: 0.706\n",
            "epoch: 6, step: 140, loss: 0.662\n",
            "epoch: 6, step: 141, loss: 0.678\n",
            "epoch: 6, step: 142, loss: 0.690\n",
            "epoch: 6, step: 143, loss: 0.683\n",
            "epoch: 6, step: 144, loss: 0.793\n",
            "epoch: 6, step: 145, loss: 0.721\n",
            "epoch: 6, step: 146, loss: 0.702\n",
            "epoch: 6, step: 147, loss: 0.664\n",
            "epoch: 6, step: 148, loss: 0.661\n",
            "epoch: 6, step: 149, loss: 0.631\n",
            "epoch: 6, step: 150, loss: 0.731\n",
            "epoch: 6, step: 151, loss: 0.677\n",
            "epoch: 6, step: 152, loss: 0.671\n",
            "epoch: 6, step: 153, loss: 0.622\n",
            "epoch: 6, step: 154, loss: 0.640\n",
            "epoch: 6, step: 155, loss: 0.706\n",
            "epoch: 6, step: 156, loss: 0.594\n",
            "epoch: 6, step: 157, loss: 0.674\n",
            "epoch: 6, step: 158, loss: 0.680\n",
            "epoch: 6, step: 159, loss: 0.680\n",
            "epoch: 6, step: 160, loss: 0.613\n",
            "epoch: 6, step: 161, loss: 0.675\n",
            "epoch: 6, step: 162, loss: 0.677\n",
            "epoch: 6, step: 163, loss: 0.626\n",
            "epoch: 6, step: 164, loss: 0.692\n",
            "epoch: 6, step: 165, loss: 0.661\n",
            "epoch: 6, step: 166, loss: 0.694\n",
            "epoch: 6, step: 167, loss: 0.653\n",
            "epoch: 6, step: 168, loss: 0.708\n",
            "epoch: 6, step: 169, loss: 0.794\n",
            "epoch: 6, step: 170, loss: 0.652\n",
            "epoch: 6, step: 171, loss: 0.661\n",
            "epoch: 6, step: 172, loss: 0.751\n",
            "epoch: 6, step: 173, loss: 0.655\n",
            "epoch: 6, step: 174, loss: 0.719\n",
            "epoch: 6, step: 175, loss: 0.655\n",
            "epoch: 6, step: 176, loss: 0.598\n",
            "epoch: 6, step: 177, loss: 0.641\n",
            "epoch: 6, step: 178, loss: 0.657\n",
            "epoch: 6, step: 179, loss: 0.705\n",
            "epoch: 6, step: 180, loss: 0.663\n",
            "epoch: 6, step: 181, loss: 0.671\n",
            "epoch: 6, step: 182, loss: 0.691\n",
            "epoch: 6, step: 183, loss: 0.744\n",
            "epoch: 6, step: 184, loss: 0.695\n",
            "epoch: 6, step: 185, loss: 0.640\n",
            "epoch: 6, step: 186, loss: 0.715\n",
            "epoch: 6, step: 187, loss: 0.659\n",
            "epoch: 6, step: 188, loss: 0.659\n",
            "epoch: 6, step: 189, loss: 0.699\n",
            "epoch: 6, step: 190, loss: 0.665\n",
            "epoch: 6, step: 191, loss: 0.667\n",
            "epoch: 6, step: 192, loss: 0.671\n",
            "epoch: 6, step: 193, loss: 0.639\n",
            "epoch: 6, step: 194, loss: 0.717\n",
            "epoch: 6, step: 195, loss: 0.634\n",
            "epoch: 6, step: 196, loss: 0.649\n",
            "epoch: 6, step: 197, loss: 0.647\n",
            "epoch: 6, step: 198, loss: 0.731\n",
            "epoch: 6, step: 199, loss: 0.639\n",
            "epoch: 6, step: 200, loss: 0.710\n",
            "epoch: 6, step: 201, loss: 0.651\n",
            "epoch: 6, step: 202, loss: 0.589\n",
            "epoch: 6, step: 203, loss: 0.582\n",
            "epoch: 6, step: 204, loss: 0.653\n",
            "epoch: 6, step: 205, loss: 0.654\n",
            "epoch: 6, step: 206, loss: 0.733\n",
            "epoch: 6, step: 207, loss: 0.684\n",
            "epoch: 6, step: 208, loss: 0.676\n",
            "epoch: 6, step: 209, loss: 0.704\n",
            "epoch: 6, step: 210, loss: 0.697\n",
            "epoch: 6, step: 211, loss: 0.629\n",
            "epoch: 6, step: 212, loss: 0.728\n",
            "epoch: 6, step: 213, loss: 0.708\n",
            "epoch: 6, step: 214, loss: 0.731\n",
            "epoch: 6, step: 215, loss: 0.635\n",
            "epoch: 6, step: 216, loss: 0.655\n",
            "epoch: 6, step: 217, loss: 0.732\n",
            "epoch: 6, step: 218, loss: 0.674\n",
            "epoch: 6, step: 219, loss: 0.703\n",
            "epoch: 6, step: 220, loss: 0.767\n",
            "epoch: 6, step: 221, loss: 0.704\n",
            "epoch: 6, step: 222, loss: 0.635\n",
            "epoch: 6, step: 223, loss: 0.636\n",
            "epoch: 6, step: 224, loss: 0.662\n",
            "epoch: 6, step: 225, loss: 0.715\n",
            "epoch: 6, step: 226, loss: 0.605\n",
            "epoch: 6, step: 227, loss: 0.614\n",
            "epoch: 6, step: 228, loss: 0.661\n",
            "epoch: 6, step: 229, loss: 0.673\n",
            "epoch: 6, step: 230, loss: 0.662\n",
            "epoch: 6, step: 231, loss: 0.675\n",
            "epoch: 6, step: 232, loss: 0.644\n",
            "epoch: 6, step: 233, loss: 0.726\n",
            "epoch: 6, step: 234, loss: 0.662\n",
            "epoch: 6, step: 235, loss: 0.666\n",
            "epoch: 6, step: 236, loss: 0.671\n",
            "epoch: 6, step: 237, loss: 0.624\n",
            "epoch: 6, step: 238, loss: 0.642\n",
            "epoch: 6, step: 239, loss: 0.767\n",
            "epoch: 6, step: 240, loss: 0.764\n",
            "epoch: 6, step: 241, loss: 0.747\n",
            "epoch: 6, step: 242, loss: 0.671\n",
            "epoch: 6, step: 243, loss: 0.611\n",
            "epoch: 6, step: 244, loss: 0.650\n",
            "epoch: 6, step: 245, loss: 0.640\n",
            "epoch: 6, step: 246, loss: 0.685\n",
            "epoch: 6, step: 247, loss: 0.688\n",
            "epoch: 6, step: 248, loss: 0.679\n",
            "epoch: 6, step: 249, loss: 0.741\n",
            "epoch: 6, step: 250, loss: 0.658\n",
            "epoch: 6, step: 251, loss: 0.789\n",
            "epoch: 6, step: 252, loss: 0.670\n",
            "epoch: 6, step: 253, loss: 0.702\n",
            "epoch: 6, step: 254, loss: 0.619\n",
            "epoch: 6, step: 255, loss: 0.689\n",
            "epoch: 6, step: 256, loss: 0.655\n",
            "epoch: 6, step: 257, loss: 0.629\n",
            "epoch: 6, step: 258, loss: 0.682\n",
            "epoch: 6, step: 259, loss: 0.630\n",
            "epoch: 6, step: 260, loss: 0.705\n",
            "epoch: 6, step: 261, loss: 0.670\n",
            "epoch: 6, step: 262, loss: 0.665\n",
            "epoch: 6, step: 263, loss: 0.638\n",
            "epoch: 6, step: 264, loss: 0.586\n",
            "epoch: 6, step: 265, loss: 0.703\n",
            "epoch: 6, step: 266, loss: 0.680\n",
            "epoch: 6, step: 267, loss: 0.707\n",
            "epoch: 6, step: 268, loss: 0.611\n",
            "epoch: 6, step: 269, loss: 0.684\n",
            "epoch: 6, step: 270, loss: 0.683\n",
            "epoch: 6, step: 271, loss: 0.674\n",
            "epoch: 6, step: 272, loss: 0.674\n",
            "epoch: 6, step: 273, loss: 0.622\n",
            "epoch: 6, step: 274, loss: 0.626\n",
            "epoch: 6, step: 275, loss: 0.624\n",
            "epoch: 6, step: 276, loss: 0.644\n",
            "epoch: 6, step: 277, loss: 0.659\n",
            "epoch: 6, step: 278, loss: 0.667\n",
            "epoch: 6, step: 279, loss: 0.722\n",
            "epoch: 6, step: 280, loss: 0.593\n",
            "epoch: 6, step: 281, loss: 0.651\n",
            "epoch: 6, step: 282, loss: 0.603\n",
            "epoch: 6, step: 283, loss: 0.686\n",
            "epoch: 6, step: 284, loss: 0.695\n",
            "epoch: 6, step: 285, loss: 0.693\n",
            "epoch: 6, step: 286, loss: 0.743\n",
            "epoch: 6, step: 287, loss: 0.750\n",
            "epoch: 6, step: 288, loss: 0.742\n",
            "epoch: 6, step: 289, loss: 0.617\n",
            "epoch: 6, step: 290, loss: 0.672\n",
            "epoch: 6, step: 291, loss: 0.711\n",
            "epoch: 6, step: 292, loss: 0.679\n",
            "epoch: 6, step: 293, loss: 0.698\n",
            "epoch: 6, step: 294, loss: 0.736\n",
            "epoch: 6, step: 295, loss: 0.741\n",
            "epoch: 6, step: 296, loss: 0.538\n",
            "epoch: 6, step: 297, loss: 0.660\n",
            "epoch: 6, step: 298, loss: 0.681\n",
            "epoch: 6, step: 299, loss: 0.716\n",
            "epoch: 6, step: 300, loss: 0.623\n",
            "epoch: 6, step: 301, loss: 0.643\n",
            "epoch: 6, step: 302, loss: 0.719\n",
            "epoch: 6, step: 303, loss: 0.606\n",
            "epoch: 6, step: 304, loss: 0.548\n",
            "epoch: 6, step: 305, loss: 0.694\n",
            "epoch: 6, step: 306, loss: 0.626\n",
            "epoch: 6, step: 307, loss: 0.741\n",
            "epoch: 6, step: 308, loss: 0.663\n",
            "epoch: 6, step: 309, loss: 0.667\n",
            "epoch: 6, step: 310, loss: 0.661\n",
            "epoch: 6, step: 311, loss: 0.657\n",
            "epoch: 6, step: 312, loss: 0.747\n",
            "epoch: 6, step: 313, loss: 0.730\n",
            "epoch: 6, step: 314, loss: 0.642\n",
            "epoch: 6, step: 315, loss: 0.707\n",
            "epoch: 6, step: 316, loss: 0.595\n",
            "epoch: 6, step: 317, loss: 0.707\n",
            "epoch: 6, step: 318, loss: 0.662\n",
            "epoch: 6, step: 319, loss: 0.702\n",
            "epoch: 6, step: 320, loss: 0.647\n",
            "epoch: 6, step: 321, loss: 0.721\n",
            "epoch: 6, step: 322, loss: 0.646\n",
            "epoch: 6, step: 323, loss: 0.674\n",
            "epoch: 6, step: 324, loss: 0.606\n",
            "epoch: 6, step: 325, loss: 0.669\n",
            "epoch: 6, step: 326, loss: 0.701\n",
            "epoch: 6, step: 327, loss: 0.711\n",
            "epoch: 6, step: 328, loss: 0.607\n",
            "epoch: 6, step: 329, loss: 0.680\n",
            "epoch: 6, step: 330, loss: 0.709\n",
            "epoch: 6, step: 331, loss: 0.643\n",
            "epoch: 6, step: 332, loss: 0.727\n",
            "epoch: 6, step: 333, loss: 0.660\n",
            "epoch: 6, step: 334, loss: 0.668\n",
            "epoch: 6, step: 335, loss: 0.740\n",
            "epoch: 6, step: 336, loss: 0.722\n",
            "epoch: 6, step: 337, loss: 0.716\n",
            "epoch: 6, step: 338, loss: 0.587\n",
            "epoch: 6, step: 339, loss: 0.609\n",
            "epoch: 6, step: 340, loss: 0.729\n",
            "epoch: 6, step: 341, loss: 0.679\n",
            "epoch: 6, step: 342, loss: 0.673\n",
            "epoch: 6, step: 343, loss: 0.654\n",
            "epoch: 6, step: 344, loss: 0.628\n",
            "epoch: 6, step: 345, loss: 0.741\n",
            "epoch: 6, step: 346, loss: 0.649\n",
            "epoch: 6, step: 347, loss: 0.719\n",
            "epoch: 6, step: 348, loss: 0.692\n",
            "epoch: 6, step: 349, loss: 0.709\n",
            "epoch: 6, step: 350, loss: 0.606\n",
            "epoch: 6, step: 351, loss: 0.658\n",
            "epoch: 6, step: 352, loss: 0.664\n",
            "epoch: 6, step: 353, loss: 0.675\n",
            "epoch: 6, step: 354, loss: 0.608\n",
            "epoch: 6, step: 355, loss: 0.702\n",
            "epoch: 6, step: 356, loss: 0.707\n",
            "epoch: 6, step: 357, loss: 0.591\n",
            "epoch: 6, step: 358, loss: 0.706\n",
            "epoch: 6, step: 359, loss: 0.667\n",
            "epoch: 6, step: 360, loss: 0.601\n",
            "epoch: 6, step: 361, loss: 0.715\n",
            "epoch: 6, step: 362, loss: 0.698\n",
            "epoch: 6, step: 363, loss: 0.735\n",
            "epoch: 6, step: 364, loss: 0.678\n",
            "epoch: 6, step: 365, loss: 0.633\n",
            "epoch: 6, step: 366, loss: 0.670\n",
            "epoch: 6, step: 367, loss: 0.643\n",
            "epoch: 6, step: 368, loss: 0.696\n",
            "epoch: 6, step: 369, loss: 0.705\n",
            "epoch: 6, step: 370, loss: 0.698\n",
            "epoch: 6, step: 371, loss: 0.666\n",
            "epoch: 6, step: 372, loss: 0.629\n",
            "epoch: 6, step: 373, loss: 0.671\n",
            "epoch: 6, step: 374, loss: 0.623\n",
            "epoch: 6, step: 375, loss: 0.651\n",
            "epoch: 6, step: 376, loss: 0.610\n",
            "epoch: 6, step: 377, loss: 0.610\n",
            "epoch: 6, step: 378, loss: 0.687\n",
            "epoch: 6, step: 379, loss: 0.693\n",
            "epoch: 6, step: 380, loss: 0.638\n",
            "epoch: 6, step: 381, loss: 0.655\n",
            "epoch: 6, step: 382, loss: 0.570\n",
            "epoch: 6, step: 383, loss: 0.631\n",
            "epoch: 6, step: 384, loss: 0.635\n",
            "epoch: 6, step: 385, loss: 0.773\n",
            "epoch: 6, step: 386, loss: 0.654\n",
            "epoch: 6, step: 387, loss: 0.628\n",
            "epoch: 6, step: 388, loss: 0.635\n",
            "epoch: 6, step: 389, loss: 0.626\n",
            "epoch: 6, step: 390, loss: 0.570\n",
            "epoch: 6, step: 391, loss: 0.713\n",
            "epoch: 6, step: 392, loss: 0.655\n",
            "epoch: 6, step: 393, loss: 0.658\n",
            "epoch: 6, step: 394, loss: 0.628\n",
            "epoch: 6, step: 395, loss: 0.661\n",
            "epoch: 6, step: 396, loss: 0.702\n",
            "epoch: 6, step: 397, loss: 0.710\n",
            "epoch: 6, step: 398, loss: 0.673\n",
            "epoch: 6, step: 399, loss: 0.663\n",
            "epoch: 6, step: 400, loss: 0.606\n",
            "epoch: 6, step: 401, loss: 0.699\n",
            "epoch: 6, step: 402, loss: 0.696\n",
            "epoch: 6, step: 403, loss: 0.648\n",
            "epoch: 6, step: 404, loss: 0.659\n",
            "epoch: 6, step: 405, loss: 0.592\n",
            "epoch: 6, step: 406, loss: 0.651\n",
            "epoch: 6, step: 407, loss: 0.713\n",
            "epoch: 6, step: 408, loss: 0.714\n",
            "epoch: 6, step: 409, loss: 0.682\n",
            "epoch: 6, step: 410, loss: 0.674\n",
            "epoch: 6, step: 411, loss: 0.653\n",
            "epoch: 6, step: 412, loss: 0.643\n",
            "epoch: 6, step: 413, loss: 0.661\n",
            "epoch: 6, step: 414, loss: 0.620\n",
            "epoch: 6, step: 415, loss: 0.684\n",
            "epoch: 6, step: 416, loss: 0.654\n",
            "epoch: 6, step: 417, loss: 0.700\n",
            "epoch: 6, step: 418, loss: 0.696\n",
            "epoch: 6, step: 419, loss: 0.667\n",
            "epoch: 6, step: 420, loss: 0.723\n",
            "epoch: 6, step: 421, loss: 0.533\n",
            "epoch: 6, step: 422, loss: 0.695\n",
            "epoch: 6, step: 423, loss: 0.695\n",
            "epoch: 6, step: 424, loss: 0.652\n",
            "epoch: 6, step: 425, loss: 0.561\n",
            "epoch: 6, step: 426, loss: 0.732\n",
            "epoch: 6, step: 427, loss: 0.697\n",
            "epoch: 6, step: 428, loss: 0.631\n",
            "epoch: 6, step: 429, loss: 0.746\n",
            "epoch: 6, step: 430, loss: 0.676\n",
            "epoch: 6, step: 431, loss: 0.612\n",
            "epoch: 6, step: 432, loss: 0.711\n",
            "epoch: 6, step: 433, loss: 0.770\n",
            "epoch: 6, step: 434, loss: 0.686\n",
            "epoch: 6, step: 435, loss: 0.677\n",
            "epoch: 6, step: 436, loss: 0.685\n",
            "epoch: 6, step: 437, loss: 0.675\n",
            "epoch: 6, step: 438, loss: 0.601\n",
            "epoch: 6, step: 439, loss: 0.706\n",
            "epoch: 6, step: 440, loss: 0.697\n",
            "epoch: 6, step: 441, loss: 0.680\n",
            "epoch: 6, step: 442, loss: 0.577\n",
            "epoch: 6, step: 443, loss: 0.683\n",
            "epoch: 6, step: 444, loss: 0.601\n",
            "epoch: 6, step: 445, loss: 0.606\n",
            "epoch: 6, step: 446, loss: 0.671\n",
            "epoch: 6, step: 447, loss: 0.708\n",
            "epoch: 6, step: 448, loss: 0.653\n",
            "epoch: 6, step: 449, loss: 0.704\n",
            "epoch: 6, step: 450, loss: 0.673\n",
            "epoch: 6, step: 451, loss: 0.744\n",
            "epoch: 6, step: 452, loss: 0.638\n",
            "epoch: 6, step: 453, loss: 0.667\n",
            "epoch: 7, step: 1, loss: 0.616\n",
            "epoch: 7, step: 2, loss: 0.586\n",
            "epoch: 7, step: 3, loss: 0.605\n",
            "epoch: 7, step: 4, loss: 0.665\n",
            "epoch: 7, step: 5, loss: 0.620\n",
            "epoch: 7, step: 6, loss: 0.691\n",
            "epoch: 7, step: 7, loss: 0.658\n",
            "epoch: 7, step: 8, loss: 0.613\n",
            "epoch: 7, step: 9, loss: 0.642\n",
            "epoch: 7, step: 10, loss: 0.645\n",
            "epoch: 7, step: 11, loss: 0.625\n",
            "epoch: 7, step: 12, loss: 0.620\n",
            "epoch: 7, step: 13, loss: 0.578\n",
            "epoch: 7, step: 14, loss: 0.573\n",
            "epoch: 7, step: 15, loss: 0.644\n",
            "epoch: 7, step: 16, loss: 0.554\n",
            "epoch: 7, step: 17, loss: 0.592\n",
            "epoch: 7, step: 18, loss: 0.542\n",
            "epoch: 7, step: 19, loss: 0.658\n",
            "epoch: 7, step: 20, loss: 0.590\n",
            "epoch: 7, step: 21, loss: 0.563\n",
            "epoch: 7, step: 22, loss: 0.543\n",
            "epoch: 7, step: 23, loss: 0.609\n",
            "epoch: 7, step: 24, loss: 0.609\n",
            "epoch: 7, step: 25, loss: 0.639\n",
            "epoch: 7, step: 26, loss: 0.593\n",
            "epoch: 7, step: 27, loss: 0.645\n",
            "epoch: 7, step: 28, loss: 0.637\n",
            "epoch: 7, step: 29, loss: 0.616\n",
            "epoch: 7, step: 30, loss: 0.588\n",
            "epoch: 7, step: 31, loss: 0.657\n",
            "epoch: 7, step: 32, loss: 0.615\n",
            "epoch: 7, step: 33, loss: 0.613\n",
            "epoch: 7, step: 34, loss: 0.614\n",
            "epoch: 7, step: 35, loss: 0.578\n",
            "epoch: 7, step: 36, loss: 0.626\n",
            "epoch: 7, step: 37, loss: 0.616\n",
            "epoch: 7, step: 38, loss: 0.638\n",
            "epoch: 7, step: 39, loss: 0.626\n",
            "epoch: 7, step: 40, loss: 0.561\n",
            "epoch: 7, step: 41, loss: 0.651\n",
            "epoch: 7, step: 42, loss: 0.589\n",
            "epoch: 7, step: 43, loss: 0.577\n",
            "epoch: 7, step: 44, loss: 0.561\n",
            "epoch: 7, step: 45, loss: 0.600\n",
            "epoch: 7, step: 46, loss: 0.566\n",
            "epoch: 7, step: 47, loss: 0.595\n",
            "epoch: 7, step: 48, loss: 0.616\n",
            "epoch: 7, step: 49, loss: 0.536\n",
            "epoch: 7, step: 50, loss: 0.623\n",
            "epoch: 7, step: 51, loss: 0.659\n",
            "epoch: 7, step: 52, loss: 0.563\n",
            "epoch: 7, step: 53, loss: 0.702\n",
            "epoch: 7, step: 54, loss: 0.601\n",
            "epoch: 7, step: 55, loss: 0.630\n",
            "epoch: 7, step: 56, loss: 0.564\n",
            "epoch: 7, step: 57, loss: 0.613\n",
            "epoch: 7, step: 58, loss: 0.565\n",
            "epoch: 7, step: 59, loss: 0.579\n",
            "epoch: 7, step: 60, loss: 0.665\n",
            "epoch: 7, step: 61, loss: 0.525\n",
            "epoch: 7, step: 62, loss: 0.630\n",
            "epoch: 7, step: 63, loss: 0.605\n",
            "epoch: 7, step: 64, loss: 0.642\n",
            "epoch: 7, step: 65, loss: 0.548\n",
            "epoch: 7, step: 66, loss: 0.672\n",
            "epoch: 7, step: 67, loss: 0.614\n",
            "epoch: 7, step: 68, loss: 0.669\n",
            "epoch: 7, step: 69, loss: 0.600\n",
            "epoch: 7, step: 70, loss: 0.545\n",
            "epoch: 7, step: 71, loss: 0.685\n",
            "epoch: 7, step: 72, loss: 0.637\n",
            "epoch: 7, step: 73, loss: 0.677\n",
            "epoch: 7, step: 74, loss: 0.692\n",
            "epoch: 7, step: 75, loss: 0.576\n",
            "epoch: 7, step: 76, loss: 0.554\n",
            "epoch: 7, step: 77, loss: 0.596\n",
            "epoch: 7, step: 78, loss: 0.663\n",
            "epoch: 7, step: 79, loss: 0.632\n",
            "epoch: 7, step: 80, loss: 0.645\n",
            "epoch: 7, step: 81, loss: 0.588\n",
            "epoch: 7, step: 82, loss: 0.608\n",
            "epoch: 7, step: 83, loss: 0.585\n",
            "epoch: 7, step: 84, loss: 0.595\n",
            "epoch: 7, step: 85, loss: 0.635\n",
            "epoch: 7, step: 86, loss: 0.673\n",
            "epoch: 7, step: 87, loss: 0.591\n",
            "epoch: 7, step: 88, loss: 0.652\n",
            "epoch: 7, step: 89, loss: 0.615\n",
            "epoch: 7, step: 90, loss: 0.644\n",
            "epoch: 7, step: 91, loss: 0.539\n",
            "epoch: 7, step: 92, loss: 0.599\n",
            "epoch: 7, step: 93, loss: 0.584\n",
            "epoch: 7, step: 94, loss: 0.685\n",
            "epoch: 7, step: 95, loss: 0.616\n",
            "epoch: 7, step: 96, loss: 0.620\n",
            "epoch: 7, step: 97, loss: 0.618\n",
            "epoch: 7, step: 98, loss: 0.551\n",
            "epoch: 7, step: 99, loss: 0.634\n",
            "epoch: 7, step: 100, loss: 0.639\n",
            "epoch: 7, step: 101, loss: 0.606\n",
            "epoch: 7, step: 102, loss: 0.608\n",
            "epoch: 7, step: 103, loss: 0.599\n",
            "epoch: 7, step: 104, loss: 0.674\n",
            "epoch: 7, step: 105, loss: 0.630\n",
            "epoch: 7, step: 106, loss: 0.657\n",
            "epoch: 7, step: 107, loss: 0.589\n",
            "epoch: 7, step: 108, loss: 0.569\n",
            "epoch: 7, step: 109, loss: 0.528\n",
            "epoch: 7, step: 110, loss: 0.640\n",
            "epoch: 7, step: 111, loss: 0.552\n",
            "epoch: 7, step: 112, loss: 0.601\n",
            "epoch: 7, step: 113, loss: 0.567\n",
            "epoch: 7, step: 114, loss: 0.696\n",
            "epoch: 7, step: 115, loss: 0.604\n",
            "epoch: 7, step: 116, loss: 0.589\n",
            "epoch: 7, step: 117, loss: 0.640\n",
            "epoch: 7, step: 118, loss: 0.625\n",
            "epoch: 7, step: 119, loss: 0.617\n",
            "epoch: 7, step: 120, loss: 0.607\n",
            "epoch: 7, step: 121, loss: 0.626\n",
            "epoch: 7, step: 122, loss: 0.677\n",
            "epoch: 7, step: 123, loss: 0.655\n",
            "epoch: 7, step: 124, loss: 0.579\n",
            "epoch: 7, step: 125, loss: 0.595\n",
            "epoch: 7, step: 126, loss: 0.605\n",
            "epoch: 7, step: 127, loss: 0.596\n",
            "epoch: 7, step: 128, loss: 0.685\n",
            "epoch: 7, step: 129, loss: 0.625\n",
            "epoch: 7, step: 130, loss: 0.631\n",
            "epoch: 7, step: 131, loss: 0.705\n",
            "epoch: 7, step: 132, loss: 0.650\n",
            "epoch: 7, step: 133, loss: 0.572\n",
            "epoch: 7, step: 134, loss: 0.582\n",
            "epoch: 7, step: 135, loss: 0.669\n",
            "epoch: 7, step: 136, loss: 0.546\n",
            "epoch: 7, step: 137, loss: 0.692\n",
            "epoch: 7, step: 138, loss: 0.543\n",
            "epoch: 7, step: 139, loss: 0.599\n",
            "epoch: 7, step: 140, loss: 0.622\n",
            "epoch: 7, step: 141, loss: 0.604\n",
            "epoch: 7, step: 142, loss: 0.620\n",
            "epoch: 7, step: 143, loss: 0.646\n",
            "epoch: 7, step: 144, loss: 0.565\n",
            "epoch: 7, step: 145, loss: 0.601\n",
            "epoch: 7, step: 146, loss: 0.621\n",
            "epoch: 7, step: 147, loss: 0.649\n",
            "epoch: 7, step: 148, loss: 0.564\n",
            "epoch: 7, step: 149, loss: 0.604\n",
            "epoch: 7, step: 150, loss: 0.584\n",
            "epoch: 7, step: 151, loss: 0.632\n",
            "epoch: 7, step: 152, loss: 0.663\n",
            "epoch: 7, step: 153, loss: 0.602\n",
            "epoch: 7, step: 154, loss: 0.672\n",
            "epoch: 7, step: 155, loss: 0.615\n",
            "epoch: 7, step: 156, loss: 0.530\n",
            "epoch: 7, step: 157, loss: 0.665\n",
            "epoch: 7, step: 158, loss: 0.664\n",
            "epoch: 7, step: 159, loss: 0.648\n",
            "epoch: 7, step: 160, loss: 0.626\n",
            "epoch: 7, step: 161, loss: 0.573\n",
            "epoch: 7, step: 162, loss: 0.633\n",
            "epoch: 7, step: 163, loss: 0.625\n",
            "epoch: 7, step: 164, loss: 0.655\n",
            "epoch: 7, step: 165, loss: 0.710\n",
            "epoch: 7, step: 166, loss: 0.592\n",
            "epoch: 7, step: 167, loss: 0.567\n",
            "epoch: 7, step: 168, loss: 0.629\n",
            "epoch: 7, step: 169, loss: 0.597\n",
            "epoch: 7, step: 170, loss: 0.647\n",
            "epoch: 7, step: 171, loss: 0.571\n",
            "epoch: 7, step: 172, loss: 0.581\n",
            "epoch: 7, step: 173, loss: 0.647\n",
            "epoch: 7, step: 174, loss: 0.674\n",
            "epoch: 7, step: 175, loss: 0.683\n",
            "epoch: 7, step: 176, loss: 0.641\n",
            "epoch: 7, step: 177, loss: 0.633\n",
            "epoch: 7, step: 178, loss: 0.571\n",
            "epoch: 7, step: 179, loss: 0.648\n",
            "epoch: 7, step: 180, loss: 0.643\n",
            "epoch: 7, step: 181, loss: 0.596\n",
            "epoch: 7, step: 182, loss: 0.636\n",
            "epoch: 7, step: 183, loss: 0.656\n",
            "epoch: 7, step: 184, loss: 0.557\n",
            "epoch: 7, step: 185, loss: 0.714\n",
            "epoch: 7, step: 186, loss: 0.588\n",
            "epoch: 7, step: 187, loss: 0.643\n",
            "epoch: 7, step: 188, loss: 0.632\n",
            "epoch: 7, step: 189, loss: 0.577\n",
            "epoch: 7, step: 190, loss: 0.634\n",
            "epoch: 7, step: 191, loss: 0.705\n",
            "epoch: 7, step: 192, loss: 0.607\n",
            "epoch: 7, step: 193, loss: 0.632\n",
            "epoch: 7, step: 194, loss: 0.547\n",
            "epoch: 7, step: 195, loss: 0.641\n",
            "epoch: 7, step: 196, loss: 0.653\n",
            "epoch: 7, step: 197, loss: 0.623\n",
            "epoch: 7, step: 198, loss: 0.645\n",
            "epoch: 7, step: 199, loss: 0.626\n",
            "epoch: 7, step: 200, loss: 0.624\n",
            "epoch: 7, step: 201, loss: 0.627\n",
            "epoch: 7, step: 202, loss: 0.701\n",
            "epoch: 7, step: 203, loss: 0.680\n",
            "epoch: 7, step: 204, loss: 0.572\n",
            "epoch: 7, step: 205, loss: 0.636\n",
            "epoch: 7, step: 206, loss: 0.593\n",
            "epoch: 7, step: 207, loss: 0.650\n",
            "epoch: 7, step: 208, loss: 0.626\n",
            "epoch: 7, step: 209, loss: 0.537\n",
            "epoch: 7, step: 210, loss: 0.585\n",
            "epoch: 7, step: 211, loss: 0.583\n",
            "epoch: 7, step: 212, loss: 0.610\n",
            "epoch: 7, step: 213, loss: 0.600\n",
            "epoch: 7, step: 214, loss: 0.631\n",
            "epoch: 7, step: 215, loss: 0.619\n",
            "epoch: 7, step: 216, loss: 0.604\n",
            "epoch: 7, step: 217, loss: 0.643\n",
            "epoch: 7, step: 218, loss: 0.564\n",
            "epoch: 7, step: 219, loss: 0.597\n",
            "epoch: 7, step: 220, loss: 0.673\n",
            "epoch: 7, step: 221, loss: 0.642\n",
            "epoch: 7, step: 222, loss: 0.613\n",
            "epoch: 7, step: 223, loss: 0.582\n",
            "epoch: 7, step: 224, loss: 0.622\n",
            "epoch: 7, step: 225, loss: 0.628\n",
            "epoch: 7, step: 226, loss: 0.516\n",
            "epoch: 7, step: 227, loss: 0.581\n",
            "epoch: 7, step: 228, loss: 0.669\n",
            "epoch: 7, step: 229, loss: 0.635\n",
            "epoch: 7, step: 230, loss: 0.621\n",
            "epoch: 7, step: 231, loss: 0.634\n",
            "epoch: 7, step: 232, loss: 0.540\n",
            "epoch: 7, step: 233, loss: 0.704\n",
            "epoch: 7, step: 234, loss: 0.625\n",
            "epoch: 7, step: 235, loss: 0.663\n",
            "epoch: 7, step: 236, loss: 0.625\n",
            "epoch: 7, step: 237, loss: 0.604\n",
            "epoch: 7, step: 238, loss: 0.615\n",
            "epoch: 7, step: 239, loss: 0.652\n",
            "epoch: 7, step: 240, loss: 0.604\n",
            "epoch: 7, step: 241, loss: 0.656\n",
            "epoch: 7, step: 242, loss: 0.598\n",
            "epoch: 7, step: 243, loss: 0.655\n",
            "epoch: 7, step: 244, loss: 0.584\n",
            "epoch: 7, step: 245, loss: 0.615\n",
            "epoch: 7, step: 246, loss: 0.584\n",
            "epoch: 7, step: 247, loss: 0.490\n",
            "epoch: 7, step: 248, loss: 0.549\n",
            "epoch: 7, step: 249, loss: 0.625\n",
            "epoch: 7, step: 250, loss: 0.575\n",
            "epoch: 7, step: 251, loss: 0.593\n",
            "epoch: 7, step: 252, loss: 0.592\n",
            "epoch: 7, step: 253, loss: 0.614\n",
            "epoch: 7, step: 254, loss: 0.589\n",
            "epoch: 7, step: 255, loss: 0.652\n",
            "epoch: 7, step: 256, loss: 0.626\n",
            "epoch: 7, step: 257, loss: 0.600\n",
            "epoch: 7, step: 258, loss: 0.547\n",
            "epoch: 7, step: 259, loss: 0.586\n",
            "epoch: 7, step: 260, loss: 0.693\n",
            "epoch: 7, step: 261, loss: 0.562\n",
            "epoch: 7, step: 262, loss: 0.607\n",
            "epoch: 7, step: 263, loss: 0.574\n",
            "epoch: 7, step: 264, loss: 0.662\n",
            "epoch: 7, step: 265, loss: 0.572\n",
            "epoch: 7, step: 266, loss: 0.610\n",
            "epoch: 7, step: 267, loss: 0.563\n",
            "epoch: 7, step: 268, loss: 0.635\n",
            "epoch: 7, step: 269, loss: 0.656\n",
            "epoch: 7, step: 270, loss: 0.634\n",
            "epoch: 7, step: 271, loss: 0.597\n",
            "epoch: 7, step: 272, loss: 0.599\n",
            "epoch: 7, step: 273, loss: 0.643\n",
            "epoch: 7, step: 274, loss: 0.640\n",
            "epoch: 7, step: 275, loss: 0.588\n",
            "epoch: 7, step: 276, loss: 0.641\n",
            "epoch: 7, step: 277, loss: 0.682\n",
            "epoch: 7, step: 278, loss: 0.601\n",
            "epoch: 7, step: 279, loss: 0.583\n",
            "epoch: 7, step: 280, loss: 0.613\n",
            "epoch: 7, step: 281, loss: 0.653\n",
            "epoch: 7, step: 282, loss: 0.719\n",
            "epoch: 7, step: 283, loss: 0.583\n",
            "epoch: 7, step: 284, loss: 0.603\n",
            "epoch: 7, step: 285, loss: 0.626\n",
            "epoch: 7, step: 286, loss: 0.675\n",
            "epoch: 7, step: 287, loss: 0.621\n",
            "epoch: 7, step: 288, loss: 0.658\n",
            "epoch: 7, step: 289, loss: 0.661\n",
            "epoch: 7, step: 290, loss: 0.577\n",
            "epoch: 7, step: 291, loss: 0.593\n",
            "epoch: 7, step: 292, loss: 0.679\n",
            "epoch: 7, step: 293, loss: 0.556\n",
            "epoch: 7, step: 294, loss: 0.603\n",
            "epoch: 7, step: 295, loss: 0.611\n",
            "epoch: 7, step: 296, loss: 0.607\n",
            "epoch: 7, step: 297, loss: 0.584\n",
            "epoch: 7, step: 298, loss: 0.693\n",
            "epoch: 7, step: 299, loss: 0.621\n",
            "epoch: 7, step: 300, loss: 0.626\n",
            "epoch: 7, step: 301, loss: 0.606\n",
            "epoch: 7, step: 302, loss: 0.591\n",
            "epoch: 7, step: 303, loss: 0.688\n",
            "epoch: 7, step: 304, loss: 0.578\n",
            "epoch: 7, step: 305, loss: 0.592\n",
            "epoch: 7, step: 306, loss: 0.592\n",
            "epoch: 7, step: 307, loss: 0.666\n",
            "epoch: 7, step: 308, loss: 0.602\n",
            "epoch: 7, step: 309, loss: 0.629\n",
            "epoch: 7, step: 310, loss: 0.612\n",
            "epoch: 7, step: 311, loss: 0.665\n",
            "epoch: 7, step: 312, loss: 0.599\n",
            "epoch: 7, step: 313, loss: 0.652\n",
            "epoch: 7, step: 314, loss: 0.615\n",
            "epoch: 7, step: 315, loss: 0.616\n",
            "epoch: 7, step: 316, loss: 0.554\n",
            "epoch: 7, step: 317, loss: 0.565\n",
            "epoch: 7, step: 318, loss: 0.705\n",
            "epoch: 7, step: 319, loss: 0.713\n",
            "epoch: 7, step: 320, loss: 0.620\n",
            "epoch: 7, step: 321, loss: 0.621\n",
            "epoch: 7, step: 322, loss: 0.703\n",
            "epoch: 7, step: 323, loss: 0.636\n",
            "epoch: 7, step: 324, loss: 0.524\n",
            "epoch: 7, step: 325, loss: 0.613\n",
            "epoch: 7, step: 326, loss: 0.638\n",
            "epoch: 7, step: 327, loss: 0.585\n",
            "epoch: 7, step: 328, loss: 0.488\n",
            "epoch: 7, step: 329, loss: 0.645\n",
            "epoch: 7, step: 330, loss: 0.710\n",
            "epoch: 7, step: 331, loss: 0.595\n",
            "epoch: 7, step: 332, loss: 0.587\n",
            "epoch: 7, step: 333, loss: 0.593\n",
            "epoch: 7, step: 334, loss: 0.551\n",
            "epoch: 7, step: 335, loss: 0.547\n",
            "epoch: 7, step: 336, loss: 0.628\n",
            "epoch: 7, step: 337, loss: 0.595\n",
            "epoch: 7, step: 338, loss: 0.564\n",
            "epoch: 7, step: 339, loss: 0.571\n",
            "epoch: 7, step: 340, loss: 0.603\n",
            "epoch: 7, step: 341, loss: 0.591\n",
            "epoch: 7, step: 342, loss: 0.622\n",
            "epoch: 7, step: 343, loss: 0.573\n",
            "epoch: 7, step: 344, loss: 0.624\n",
            "epoch: 7, step: 345, loss: 0.553\n",
            "epoch: 7, step: 346, loss: 0.622\n",
            "epoch: 7, step: 347, loss: 0.631\n",
            "epoch: 7, step: 348, loss: 0.588\n",
            "epoch: 7, step: 349, loss: 0.588\n",
            "epoch: 7, step: 350, loss: 0.611\n",
            "epoch: 7, step: 351, loss: 0.606\n",
            "epoch: 7, step: 352, loss: 0.549\n",
            "epoch: 7, step: 353, loss: 0.628\n",
            "epoch: 7, step: 354, loss: 0.656\n",
            "epoch: 7, step: 355, loss: 0.675\n",
            "epoch: 7, step: 356, loss: 0.553\n",
            "epoch: 7, step: 357, loss: 0.570\n",
            "epoch: 7, step: 358, loss: 0.588\n",
            "epoch: 7, step: 359, loss: 0.684\n",
            "epoch: 7, step: 360, loss: 0.688\n",
            "epoch: 7, step: 361, loss: 0.662\n",
            "epoch: 7, step: 362, loss: 0.594\n",
            "epoch: 7, step: 363, loss: 0.578\n",
            "epoch: 7, step: 364, loss: 0.598\n",
            "epoch: 7, step: 365, loss: 0.643\n",
            "epoch: 7, step: 366, loss: 0.595\n",
            "epoch: 7, step: 367, loss: 0.651\n",
            "epoch: 7, step: 368, loss: 0.621\n",
            "epoch: 7, step: 369, loss: 0.639\n",
            "epoch: 7, step: 370, loss: 0.610\n",
            "epoch: 7, step: 371, loss: 0.525\n",
            "epoch: 7, step: 372, loss: 0.608\n",
            "epoch: 7, step: 373, loss: 0.601\n",
            "epoch: 7, step: 374, loss: 0.642\n",
            "epoch: 7, step: 375, loss: 0.597\n",
            "epoch: 7, step: 376, loss: 0.650\n",
            "epoch: 7, step: 377, loss: 0.604\n",
            "epoch: 7, step: 378, loss: 0.702\n",
            "epoch: 7, step: 379, loss: 0.572\n",
            "epoch: 7, step: 380, loss: 0.626\n",
            "epoch: 7, step: 381, loss: 0.574\n",
            "epoch: 7, step: 382, loss: 0.579\n",
            "epoch: 7, step: 383, loss: 0.512\n",
            "epoch: 7, step: 384, loss: 0.614\n",
            "epoch: 7, step: 385, loss: 0.612\n",
            "epoch: 7, step: 386, loss: 0.626\n",
            "epoch: 7, step: 387, loss: 0.650\n",
            "epoch: 7, step: 388, loss: 0.544\n",
            "epoch: 7, step: 389, loss: 0.586\n",
            "epoch: 7, step: 390, loss: 0.703\n",
            "epoch: 7, step: 391, loss: 0.566\n",
            "epoch: 7, step: 392, loss: 0.611\n",
            "epoch: 7, step: 393, loss: 0.589\n",
            "epoch: 7, step: 394, loss: 0.622\n",
            "epoch: 7, step: 395, loss: 0.554\n",
            "epoch: 7, step: 396, loss: 0.634\n",
            "epoch: 7, step: 397, loss: 0.583\n",
            "epoch: 7, step: 398, loss: 0.672\n",
            "epoch: 7, step: 399, loss: 0.664\n",
            "epoch: 7, step: 400, loss: 0.650\n",
            "epoch: 7, step: 401, loss: 0.609\n",
            "epoch: 7, step: 402, loss: 0.578\n",
            "epoch: 7, step: 403, loss: 0.645\n",
            "epoch: 7, step: 404, loss: 0.632\n",
            "epoch: 7, step: 405, loss: 0.600\n",
            "epoch: 7, step: 406, loss: 0.602\n",
            "epoch: 7, step: 407, loss: 0.579\n",
            "epoch: 7, step: 408, loss: 0.637\n",
            "epoch: 7, step: 409, loss: 0.651\n",
            "epoch: 7, step: 410, loss: 0.598\n",
            "epoch: 7, step: 411, loss: 0.593\n",
            "epoch: 7, step: 412, loss: 0.599\n",
            "epoch: 7, step: 413, loss: 0.570\n",
            "epoch: 7, step: 414, loss: 0.696\n",
            "epoch: 7, step: 415, loss: 0.590\n",
            "epoch: 7, step: 416, loss: 0.608\n",
            "epoch: 7, step: 417, loss: 0.630\n",
            "epoch: 7, step: 418, loss: 0.585\n",
            "epoch: 7, step: 419, loss: 0.589\n",
            "epoch: 7, step: 420, loss: 0.646\n",
            "epoch: 7, step: 421, loss: 0.565\n",
            "epoch: 7, step: 422, loss: 0.600\n",
            "epoch: 7, step: 423, loss: 0.565\n",
            "epoch: 7, step: 424, loss: 0.634\n",
            "epoch: 7, step: 425, loss: 0.655\n",
            "epoch: 7, step: 426, loss: 0.599\n",
            "epoch: 7, step: 427, loss: 0.623\n",
            "epoch: 7, step: 428, loss: 0.562\n",
            "epoch: 7, step: 429, loss: 0.634\n",
            "epoch: 7, step: 430, loss: 0.619\n",
            "epoch: 7, step: 431, loss: 0.606\n",
            "epoch: 7, step: 432, loss: 0.659\n",
            "epoch: 7, step: 433, loss: 0.632\n",
            "epoch: 7, step: 434, loss: 0.602\n",
            "epoch: 7, step: 435, loss: 0.658\n",
            "epoch: 7, step: 436, loss: 0.669\n",
            "epoch: 7, step: 437, loss: 0.614\n",
            "epoch: 7, step: 438, loss: 0.637\n",
            "epoch: 7, step: 439, loss: 0.647\n",
            "epoch: 7, step: 440, loss: 0.678\n",
            "epoch: 7, step: 441, loss: 0.600\n",
            "epoch: 7, step: 442, loss: 0.572\n",
            "epoch: 7, step: 443, loss: 0.571\n",
            "epoch: 7, step: 444, loss: 0.576\n",
            "epoch: 7, step: 445, loss: 0.582\n",
            "epoch: 7, step: 446, loss: 0.560\n",
            "epoch: 7, step: 447, loss: 0.585\n",
            "epoch: 7, step: 448, loss: 0.580\n",
            "epoch: 7, step: 449, loss: 0.575\n",
            "epoch: 7, step: 450, loss: 0.636\n",
            "epoch: 7, step: 451, loss: 0.569\n",
            "epoch: 7, step: 452, loss: 0.657\n",
            "epoch: 7, step: 453, loss: 0.568\n",
            "epoch: 8, step: 1, loss: 0.562\n",
            "epoch: 8, step: 2, loss: 0.565\n",
            "epoch: 8, step: 3, loss: 0.528\n",
            "epoch: 8, step: 4, loss: 0.540\n",
            "epoch: 8, step: 5, loss: 0.533\n",
            "epoch: 8, step: 6, loss: 0.572\n",
            "epoch: 8, step: 7, loss: 0.544\n",
            "epoch: 8, step: 8, loss: 0.623\n",
            "epoch: 8, step: 9, loss: 0.574\n",
            "epoch: 8, step: 10, loss: 0.561\n",
            "epoch: 8, step: 11, loss: 0.559\n",
            "epoch: 8, step: 12, loss: 0.583\n",
            "epoch: 8, step: 13, loss: 0.615\n",
            "epoch: 8, step: 14, loss: 0.639\n",
            "epoch: 8, step: 15, loss: 0.528\n",
            "epoch: 8, step: 16, loss: 0.605\n",
            "epoch: 8, step: 17, loss: 0.554\n",
            "epoch: 8, step: 18, loss: 0.549\n",
            "epoch: 8, step: 19, loss: 0.618\n",
            "epoch: 8, step: 20, loss: 0.574\n",
            "epoch: 8, step: 21, loss: 0.532\n",
            "epoch: 8, step: 22, loss: 0.550\n",
            "epoch: 8, step: 23, loss: 0.573\n",
            "epoch: 8, step: 24, loss: 0.592\n",
            "epoch: 8, step: 25, loss: 0.612\n",
            "epoch: 8, step: 26, loss: 0.494\n",
            "epoch: 8, step: 27, loss: 0.567\n",
            "epoch: 8, step: 28, loss: 0.505\n",
            "epoch: 8, step: 29, loss: 0.586\n",
            "epoch: 8, step: 30, loss: 0.547\n",
            "epoch: 8, step: 31, loss: 0.542\n",
            "epoch: 8, step: 32, loss: 0.604\n",
            "epoch: 8, step: 33, loss: 0.545\n",
            "epoch: 8, step: 34, loss: 0.563\n",
            "epoch: 8, step: 35, loss: 0.539\n",
            "epoch: 8, step: 36, loss: 0.536\n",
            "epoch: 8, step: 37, loss: 0.561\n",
            "epoch: 8, step: 38, loss: 0.550\n",
            "epoch: 8, step: 39, loss: 0.478\n",
            "epoch: 8, step: 40, loss: 0.495\n",
            "epoch: 8, step: 41, loss: 0.536\n",
            "epoch: 8, step: 42, loss: 0.551\n",
            "epoch: 8, step: 43, loss: 0.562\n",
            "epoch: 8, step: 44, loss: 0.608\n",
            "epoch: 8, step: 45, loss: 0.517\n",
            "epoch: 8, step: 46, loss: 0.548\n",
            "epoch: 8, step: 47, loss: 0.578\n",
            "epoch: 8, step: 48, loss: 0.542\n",
            "epoch: 8, step: 49, loss: 0.529\n",
            "epoch: 8, step: 50, loss: 0.597\n",
            "epoch: 8, step: 51, loss: 0.514\n",
            "epoch: 8, step: 52, loss: 0.589\n",
            "epoch: 8, step: 53, loss: 0.573\n",
            "epoch: 8, step: 54, loss: 0.553\n",
            "epoch: 8, step: 55, loss: 0.534\n",
            "epoch: 8, step: 56, loss: 0.520\n",
            "epoch: 8, step: 57, loss: 0.552\n",
            "epoch: 8, step: 58, loss: 0.571\n",
            "epoch: 8, step: 59, loss: 0.613\n",
            "epoch: 8, step: 60, loss: 0.531\n",
            "epoch: 8, step: 61, loss: 0.684\n",
            "epoch: 8, step: 62, loss: 0.474\n",
            "epoch: 8, step: 63, loss: 0.536\n",
            "epoch: 8, step: 64, loss: 0.653\n",
            "epoch: 8, step: 65, loss: 0.567\n",
            "epoch: 8, step: 66, loss: 0.595\n",
            "epoch: 8, step: 67, loss: 0.537\n",
            "epoch: 8, step: 68, loss: 0.637\n",
            "epoch: 8, step: 69, loss: 0.554\n",
            "epoch: 8, step: 70, loss: 0.557\n",
            "epoch: 8, step: 71, loss: 0.575\n",
            "epoch: 8, step: 72, loss: 0.530\n",
            "epoch: 8, step: 73, loss: 0.538\n",
            "epoch: 8, step: 74, loss: 0.615\n",
            "epoch: 8, step: 75, loss: 0.578\n",
            "epoch: 8, step: 76, loss: 0.496\n",
            "epoch: 8, step: 77, loss: 0.508\n",
            "epoch: 8, step: 78, loss: 0.558\n",
            "epoch: 8, step: 79, loss: 0.598\n",
            "epoch: 8, step: 80, loss: 0.540\n",
            "epoch: 8, step: 81, loss: 0.573\n",
            "epoch: 8, step: 82, loss: 0.575\n",
            "epoch: 8, step: 83, loss: 0.568\n",
            "epoch: 8, step: 84, loss: 0.586\n",
            "epoch: 8, step: 85, loss: 0.572\n",
            "epoch: 8, step: 86, loss: 0.571\n",
            "epoch: 8, step: 87, loss: 0.582\n",
            "epoch: 8, step: 88, loss: 0.663\n",
            "epoch: 8, step: 89, loss: 0.520\n",
            "epoch: 8, step: 90, loss: 0.494\n",
            "epoch: 8, step: 91, loss: 0.590\n",
            "epoch: 8, step: 92, loss: 0.552\n",
            "epoch: 8, step: 93, loss: 0.558\n",
            "epoch: 8, step: 94, loss: 0.642\n",
            "epoch: 8, step: 95, loss: 0.617\n",
            "epoch: 8, step: 96, loss: 0.542\n",
            "epoch: 8, step: 97, loss: 0.647\n",
            "epoch: 8, step: 98, loss: 0.506\n",
            "epoch: 8, step: 99, loss: 0.572\n",
            "epoch: 8, step: 100, loss: 0.552\n",
            "epoch: 8, step: 101, loss: 0.469\n",
            "epoch: 8, step: 102, loss: 0.608\n",
            "epoch: 8, step: 103, loss: 0.543\n",
            "epoch: 8, step: 104, loss: 0.552\n",
            "epoch: 8, step: 105, loss: 0.568\n",
            "epoch: 8, step: 106, loss: 0.589\n",
            "epoch: 8, step: 107, loss: 0.582\n",
            "epoch: 8, step: 108, loss: 0.569\n",
            "epoch: 8, step: 109, loss: 0.636\n",
            "epoch: 8, step: 110, loss: 0.574\n",
            "epoch: 8, step: 111, loss: 0.530\n",
            "epoch: 8, step: 112, loss: 0.568\n",
            "epoch: 8, step: 113, loss: 0.632\n",
            "epoch: 8, step: 114, loss: 0.538\n",
            "epoch: 8, step: 115, loss: 0.614\n",
            "epoch: 8, step: 116, loss: 0.639\n",
            "epoch: 8, step: 117, loss: 0.605\n",
            "epoch: 8, step: 118, loss: 0.581\n",
            "epoch: 8, step: 119, loss: 0.500\n",
            "epoch: 8, step: 120, loss: 0.591\n",
            "epoch: 8, step: 121, loss: 0.616\n",
            "epoch: 8, step: 122, loss: 0.566\n",
            "epoch: 8, step: 123, loss: 0.580\n",
            "epoch: 8, step: 124, loss: 0.528\n",
            "epoch: 8, step: 125, loss: 0.559\n",
            "epoch: 8, step: 126, loss: 0.617\n",
            "epoch: 8, step: 127, loss: 0.489\n",
            "epoch: 8, step: 128, loss: 0.561\n",
            "epoch: 8, step: 129, loss: 0.553\n",
            "epoch: 8, step: 130, loss: 0.564\n",
            "epoch: 8, step: 131, loss: 0.536\n",
            "epoch: 8, step: 132, loss: 0.565\n",
            "epoch: 8, step: 133, loss: 0.563\n",
            "epoch: 8, step: 134, loss: 0.519\n",
            "epoch: 8, step: 135, loss: 0.589\n",
            "epoch: 8, step: 136, loss: 0.583\n",
            "epoch: 8, step: 137, loss: 0.612\n",
            "epoch: 8, step: 138, loss: 0.547\n",
            "epoch: 8, step: 139, loss: 0.584\n",
            "epoch: 8, step: 140, loss: 0.543\n",
            "epoch: 8, step: 141, loss: 0.583\n",
            "epoch: 8, step: 142, loss: 0.547\n",
            "epoch: 8, step: 143, loss: 0.607\n",
            "epoch: 8, step: 144, loss: 0.513\n",
            "epoch: 8, step: 145, loss: 0.581\n",
            "epoch: 8, step: 146, loss: 0.557\n",
            "epoch: 8, step: 147, loss: 0.570\n",
            "epoch: 8, step: 148, loss: 0.579\n",
            "epoch: 8, step: 149, loss: 0.596\n",
            "epoch: 8, step: 150, loss: 0.536\n",
            "epoch: 8, step: 151, loss: 0.537\n",
            "epoch: 8, step: 152, loss: 0.461\n",
            "epoch: 8, step: 153, loss: 0.530\n",
            "epoch: 8, step: 154, loss: 0.556\n",
            "epoch: 8, step: 155, loss: 0.555\n",
            "epoch: 8, step: 156, loss: 0.568\n",
            "epoch: 8, step: 157, loss: 0.635\n",
            "epoch: 8, step: 158, loss: 0.593\n",
            "epoch: 8, step: 159, loss: 0.505\n",
            "epoch: 8, step: 160, loss: 0.536\n",
            "epoch: 8, step: 161, loss: 0.588\n",
            "epoch: 8, step: 162, loss: 0.590\n",
            "epoch: 8, step: 163, loss: 0.541\n",
            "epoch: 8, step: 164, loss: 0.523\n",
            "epoch: 8, step: 165, loss: 0.527\n",
            "epoch: 8, step: 166, loss: 0.580\n",
            "epoch: 8, step: 167, loss: 0.509\n",
            "epoch: 8, step: 168, loss: 0.599\n",
            "epoch: 8, step: 169, loss: 0.552\n",
            "epoch: 8, step: 170, loss: 0.494\n",
            "epoch: 8, step: 171, loss: 0.589\n",
            "epoch: 8, step: 172, loss: 0.567\n",
            "epoch: 8, step: 173, loss: 0.547\n",
            "epoch: 8, step: 174, loss: 0.600\n",
            "epoch: 8, step: 175, loss: 0.587\n",
            "epoch: 8, step: 176, loss: 0.595\n",
            "epoch: 8, step: 177, loss: 0.596\n",
            "epoch: 8, step: 178, loss: 0.590\n",
            "epoch: 8, step: 179, loss: 0.483\n",
            "epoch: 8, step: 180, loss: 0.499\n",
            "epoch: 8, step: 181, loss: 0.560\n",
            "epoch: 8, step: 182, loss: 0.593\n",
            "epoch: 8, step: 183, loss: 0.602\n",
            "epoch: 8, step: 184, loss: 0.549\n",
            "epoch: 8, step: 185, loss: 0.534\n",
            "epoch: 8, step: 186, loss: 0.561\n",
            "epoch: 8, step: 187, loss: 0.488\n",
            "epoch: 8, step: 188, loss: 0.528\n",
            "epoch: 8, step: 189, loss: 0.540\n",
            "epoch: 8, step: 190, loss: 0.591\n",
            "epoch: 8, step: 191, loss: 0.539\n",
            "epoch: 8, step: 192, loss: 0.584\n",
            "epoch: 8, step: 193, loss: 0.653\n",
            "epoch: 8, step: 194, loss: 0.527\n",
            "epoch: 8, step: 195, loss: 0.581\n",
            "epoch: 8, step: 196, loss: 0.574\n",
            "epoch: 8, step: 197, loss: 0.524\n",
            "epoch: 8, step: 198, loss: 0.608\n",
            "epoch: 8, step: 199, loss: 0.468\n",
            "epoch: 8, step: 200, loss: 0.633\n",
            "epoch: 8, step: 201, loss: 0.539\n",
            "epoch: 8, step: 202, loss: 0.540\n",
            "epoch: 8, step: 203, loss: 0.566\n",
            "epoch: 8, step: 204, loss: 0.468\n",
            "epoch: 8, step: 205, loss: 0.577\n",
            "epoch: 8, step: 206, loss: 0.553\n",
            "epoch: 8, step: 207, loss: 0.645\n",
            "epoch: 8, step: 208, loss: 0.611\n",
            "epoch: 8, step: 209, loss: 0.568\n",
            "epoch: 8, step: 210, loss: 0.472\n",
            "epoch: 8, step: 211, loss: 0.579\n",
            "epoch: 8, step: 212, loss: 0.600\n",
            "epoch: 8, step: 213, loss: 0.494\n",
            "epoch: 8, step: 214, loss: 0.509\n",
            "epoch: 8, step: 215, loss: 0.627\n",
            "epoch: 8, step: 216, loss: 0.596\n",
            "epoch: 8, step: 217, loss: 0.544\n",
            "epoch: 8, step: 218, loss: 0.597\n",
            "epoch: 8, step: 219, loss: 0.560\n",
            "epoch: 8, step: 220, loss: 0.562\n",
            "epoch: 8, step: 221, loss: 0.621\n",
            "epoch: 8, step: 222, loss: 0.639\n",
            "epoch: 8, step: 223, loss: 0.571\n",
            "epoch: 8, step: 224, loss: 0.598\n",
            "epoch: 8, step: 225, loss: 0.536\n",
            "epoch: 8, step: 226, loss: 0.550\n",
            "epoch: 8, step: 227, loss: 0.541\n",
            "epoch: 8, step: 228, loss: 0.528\n",
            "epoch: 8, step: 229, loss: 0.572\n",
            "epoch: 8, step: 230, loss: 0.520\n",
            "epoch: 8, step: 231, loss: 0.665\n",
            "epoch: 8, step: 232, loss: 0.555\n",
            "epoch: 8, step: 233, loss: 0.589\n",
            "epoch: 8, step: 234, loss: 0.531\n",
            "epoch: 8, step: 235, loss: 0.578\n",
            "epoch: 8, step: 236, loss: 0.565\n",
            "epoch: 8, step: 237, loss: 0.525\n",
            "epoch: 8, step: 238, loss: 0.511\n",
            "epoch: 8, step: 239, loss: 0.567\n",
            "epoch: 8, step: 240, loss: 0.588\n",
            "epoch: 8, step: 241, loss: 0.567\n",
            "epoch: 8, step: 242, loss: 0.560\n",
            "epoch: 8, step: 243, loss: 0.524\n",
            "epoch: 8, step: 244, loss: 0.623\n",
            "epoch: 8, step: 245, loss: 0.564\n",
            "epoch: 8, step: 246, loss: 0.567\n",
            "epoch: 8, step: 247, loss: 0.684\n",
            "epoch: 8, step: 248, loss: 0.538\n",
            "epoch: 8, step: 249, loss: 0.552\n",
            "epoch: 8, step: 250, loss: 0.621\n",
            "epoch: 8, step: 251, loss: 0.554\n",
            "epoch: 8, step: 252, loss: 0.598\n",
            "epoch: 8, step: 253, loss: 0.630\n",
            "epoch: 8, step: 254, loss: 0.646\n",
            "epoch: 8, step: 255, loss: 0.552\n",
            "epoch: 8, step: 256, loss: 0.477\n",
            "epoch: 8, step: 257, loss: 0.601\n",
            "epoch: 8, step: 258, loss: 0.550\n",
            "epoch: 8, step: 259, loss: 0.519\n",
            "epoch: 8, step: 260, loss: 0.526\n",
            "epoch: 8, step: 261, loss: 0.611\n",
            "epoch: 8, step: 262, loss: 0.510\n",
            "epoch: 8, step: 263, loss: 0.599\n",
            "epoch: 8, step: 264, loss: 0.506\n",
            "epoch: 8, step: 265, loss: 0.584\n",
            "epoch: 8, step: 266, loss: 0.622\n",
            "epoch: 8, step: 267, loss: 0.531\n",
            "epoch: 8, step: 268, loss: 0.538\n",
            "epoch: 8, step: 269, loss: 0.568\n",
            "epoch: 8, step: 270, loss: 0.519\n",
            "epoch: 8, step: 271, loss: 0.614\n",
            "epoch: 8, step: 272, loss: 0.499\n",
            "epoch: 8, step: 273, loss: 0.609\n",
            "epoch: 8, step: 274, loss: 0.549\n",
            "epoch: 8, step: 275, loss: 0.599\n",
            "epoch: 8, step: 276, loss: 0.595\n",
            "epoch: 8, step: 277, loss: 0.496\n",
            "epoch: 8, step: 278, loss: 0.525\n",
            "epoch: 8, step: 279, loss: 0.492\n",
            "epoch: 8, step: 280, loss: 0.475\n",
            "epoch: 8, step: 281, loss: 0.583\n",
            "epoch: 8, step: 282, loss: 0.474\n",
            "epoch: 8, step: 283, loss: 0.551\n",
            "epoch: 8, step: 284, loss: 0.500\n",
            "epoch: 8, step: 285, loss: 0.553\n",
            "epoch: 8, step: 286, loss: 0.537\n",
            "epoch: 8, step: 287, loss: 0.518\n",
            "epoch: 8, step: 288, loss: 0.523\n",
            "epoch: 8, step: 289, loss: 0.463\n",
            "epoch: 8, step: 290, loss: 0.491\n",
            "epoch: 8, step: 291, loss: 0.559\n",
            "epoch: 8, step: 292, loss: 0.574\n",
            "epoch: 8, step: 293, loss: 0.559\n",
            "epoch: 8, step: 294, loss: 0.600\n",
            "epoch: 8, step: 295, loss: 0.550\n",
            "epoch: 8, step: 296, loss: 0.688\n",
            "epoch: 8, step: 297, loss: 0.511\n",
            "epoch: 8, step: 298, loss: 0.521\n",
            "epoch: 8, step: 299, loss: 0.617\n",
            "epoch: 8, step: 300, loss: 0.557\n",
            "epoch: 8, step: 301, loss: 0.609\n",
            "epoch: 8, step: 302, loss: 0.546\n",
            "epoch: 8, step: 303, loss: 0.474\n",
            "epoch: 8, step: 304, loss: 0.494\n",
            "epoch: 8, step: 305, loss: 0.513\n",
            "epoch: 8, step: 306, loss: 0.496\n",
            "epoch: 8, step: 307, loss: 0.547\n",
            "epoch: 8, step: 308, loss: 0.507\n",
            "epoch: 8, step: 309, loss: 0.587\n",
            "epoch: 8, step: 310, loss: 0.612\n",
            "epoch: 8, step: 311, loss: 0.595\n",
            "epoch: 8, step: 312, loss: 0.539\n",
            "epoch: 8, step: 313, loss: 0.573\n",
            "epoch: 8, step: 314, loss: 0.524\n",
            "epoch: 8, step: 315, loss: 0.619\n",
            "epoch: 8, step: 316, loss: 0.572\n",
            "epoch: 8, step: 317, loss: 0.517\n",
            "epoch: 8, step: 318, loss: 0.602\n",
            "epoch: 8, step: 319, loss: 0.669\n",
            "epoch: 8, step: 320, loss: 0.616\n",
            "epoch: 8, step: 321, loss: 0.567\n",
            "epoch: 8, step: 322, loss: 0.627\n",
            "epoch: 8, step: 323, loss: 0.567\n",
            "epoch: 8, step: 324, loss: 0.492\n",
            "epoch: 8, step: 325, loss: 0.611\n",
            "epoch: 8, step: 326, loss: 0.512\n",
            "epoch: 8, step: 327, loss: 0.543\n",
            "epoch: 8, step: 328, loss: 0.573\n",
            "epoch: 8, step: 329, loss: 0.518\n",
            "epoch: 8, step: 330, loss: 0.576\n",
            "epoch: 8, step: 331, loss: 0.523\n",
            "epoch: 8, step: 332, loss: 0.541\n",
            "epoch: 8, step: 333, loss: 0.519\n",
            "epoch: 8, step: 334, loss: 0.573\n",
            "epoch: 8, step: 335, loss: 0.611\n",
            "epoch: 8, step: 336, loss: 0.591\n",
            "epoch: 8, step: 337, loss: 0.554\n",
            "epoch: 8, step: 338, loss: 0.605\n",
            "epoch: 8, step: 339, loss: 0.584\n",
            "epoch: 8, step: 340, loss: 0.559\n",
            "epoch: 8, step: 341, loss: 0.537\n",
            "epoch: 8, step: 342, loss: 0.530\n",
            "epoch: 8, step: 343, loss: 0.501\n",
            "epoch: 8, step: 344, loss: 0.510\n",
            "epoch: 8, step: 345, loss: 0.544\n",
            "epoch: 8, step: 346, loss: 0.608\n",
            "epoch: 8, step: 347, loss: 0.528\n",
            "epoch: 8, step: 348, loss: 0.574\n",
            "epoch: 8, step: 349, loss: 0.624\n",
            "epoch: 8, step: 350, loss: 0.551\n",
            "epoch: 8, step: 351, loss: 0.598\n",
            "epoch: 8, step: 352, loss: 0.574\n",
            "epoch: 8, step: 353, loss: 0.551\n",
            "epoch: 8, step: 354, loss: 0.518\n",
            "epoch: 8, step: 355, loss: 0.567\n",
            "epoch: 8, step: 356, loss: 0.458\n",
            "epoch: 8, step: 357, loss: 0.584\n",
            "epoch: 8, step: 358, loss: 0.687\n",
            "epoch: 8, step: 359, loss: 0.560\n",
            "epoch: 8, step: 360, loss: 0.606\n",
            "epoch: 8, step: 361, loss: 0.548\n",
            "epoch: 8, step: 362, loss: 0.554\n",
            "epoch: 8, step: 363, loss: 0.569\n",
            "epoch: 8, step: 364, loss: 0.497\n",
            "epoch: 8, step: 365, loss: 0.582\n",
            "epoch: 8, step: 366, loss: 0.633\n",
            "epoch: 8, step: 367, loss: 0.511\n",
            "epoch: 8, step: 368, loss: 0.579\n",
            "epoch: 8, step: 369, loss: 0.555\n",
            "epoch: 8, step: 370, loss: 0.612\n",
            "epoch: 8, step: 371, loss: 0.563\n",
            "epoch: 8, step: 372, loss: 0.550\n",
            "epoch: 8, step: 373, loss: 0.507\n",
            "epoch: 8, step: 374, loss: 0.519\n",
            "epoch: 8, step: 375, loss: 0.581\n",
            "epoch: 8, step: 376, loss: 0.547\n",
            "epoch: 8, step: 377, loss: 0.542\n",
            "epoch: 8, step: 378, loss: 0.523\n",
            "epoch: 8, step: 379, loss: 0.546\n",
            "epoch: 8, step: 380, loss: 0.558\n",
            "epoch: 8, step: 381, loss: 0.577\n",
            "epoch: 8, step: 382, loss: 0.582\n",
            "epoch: 8, step: 383, loss: 0.583\n",
            "epoch: 8, step: 384, loss: 0.497\n",
            "epoch: 8, step: 385, loss: 0.551\n",
            "epoch: 8, step: 386, loss: 0.556\n",
            "epoch: 8, step: 387, loss: 0.511\n",
            "epoch: 8, step: 388, loss: 0.567\n",
            "epoch: 8, step: 389, loss: 0.635\n",
            "epoch: 8, step: 390, loss: 0.590\n",
            "epoch: 8, step: 391, loss: 0.549\n",
            "epoch: 8, step: 392, loss: 0.494\n",
            "epoch: 8, step: 393, loss: 0.496\n",
            "epoch: 8, step: 394, loss: 0.602\n",
            "epoch: 8, step: 395, loss: 0.564\n",
            "epoch: 8, step: 396, loss: 0.622\n",
            "epoch: 8, step: 397, loss: 0.663\n",
            "epoch: 8, step: 398, loss: 0.474\n",
            "epoch: 8, step: 399, loss: 0.556\n",
            "epoch: 8, step: 400, loss: 0.530\n",
            "epoch: 8, step: 401, loss: 0.511\n",
            "epoch: 8, step: 402, loss: 0.573\n",
            "epoch: 8, step: 403, loss: 0.522\n",
            "epoch: 8, step: 404, loss: 0.604\n",
            "epoch: 8, step: 405, loss: 0.536\n",
            "epoch: 8, step: 406, loss: 0.564\n",
            "epoch: 8, step: 407, loss: 0.522\n",
            "epoch: 8, step: 408, loss: 0.633\n",
            "epoch: 8, step: 409, loss: 0.627\n",
            "epoch: 8, step: 410, loss: 0.564\n",
            "epoch: 8, step: 411, loss: 0.518\n",
            "epoch: 8, step: 412, loss: 0.506\n",
            "epoch: 8, step: 413, loss: 0.518\n",
            "epoch: 8, step: 414, loss: 0.568\n",
            "epoch: 8, step: 415, loss: 0.557\n",
            "epoch: 8, step: 416, loss: 0.509\n",
            "epoch: 8, step: 417, loss: 0.592\n",
            "epoch: 8, step: 418, loss: 0.644\n",
            "epoch: 8, step: 419, loss: 0.556\n",
            "epoch: 8, step: 420, loss: 0.570\n",
            "epoch: 8, step: 421, loss: 0.549\n",
            "epoch: 8, step: 422, loss: 0.530\n",
            "epoch: 8, step: 423, loss: 0.617\n",
            "epoch: 8, step: 424, loss: 0.612\n",
            "epoch: 8, step: 425, loss: 0.545\n",
            "epoch: 8, step: 426, loss: 0.575\n",
            "epoch: 8, step: 427, loss: 0.578\n",
            "epoch: 8, step: 428, loss: 0.614\n",
            "epoch: 8, step: 429, loss: 0.528\n",
            "epoch: 8, step: 430, loss: 0.727\n",
            "epoch: 8, step: 431, loss: 0.594\n",
            "epoch: 8, step: 432, loss: 0.571\n",
            "epoch: 8, step: 433, loss: 0.539\n",
            "epoch: 8, step: 434, loss: 0.549\n",
            "epoch: 8, step: 435, loss: 0.636\n",
            "epoch: 8, step: 436, loss: 0.558\n",
            "epoch: 8, step: 437, loss: 0.556\n",
            "epoch: 8, step: 438, loss: 0.624\n",
            "epoch: 8, step: 439, loss: 0.570\n",
            "epoch: 8, step: 440, loss: 0.629\n",
            "epoch: 8, step: 441, loss: 0.483\n",
            "epoch: 8, step: 442, loss: 0.514\n",
            "epoch: 8, step: 443, loss: 0.528\n",
            "epoch: 8, step: 444, loss: 0.540\n",
            "epoch: 8, step: 445, loss: 0.494\n",
            "epoch: 8, step: 446, loss: 0.512\n",
            "epoch: 8, step: 447, loss: 0.600\n",
            "epoch: 8, step: 448, loss: 0.534\n",
            "epoch: 8, step: 449, loss: 0.590\n",
            "epoch: 8, step: 450, loss: 0.569\n",
            "epoch: 8, step: 451, loss: 0.512\n",
            "epoch: 8, step: 452, loss: 0.539\n",
            "epoch: 8, step: 453, loss: 0.526\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Imh20S-r2Y-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval():\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for step , (src, tgt) in enumerate(val_dataloader):\n",
        "      outputs = model.predict(src)\n",
        "      outputs = torch.transpose(outputs, 1, 2)  # (batch, seq_len, vocab_size)  => (batch, vocab_size, seq_len)\n",
        "      loss = criterion(outputs, tgt[:, 1:])\n",
        "      print(step, loss)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35wDisSP2cp-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "6923746f-3bf0-467f-88a9-a8b7b36d5c62"
      },
      "source": [
        "eval()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor(2.8666, device='cuda:0')\n",
            "1 tensor(2.7830, device='cuda:0')\n",
            "2 tensor(3.1528, device='cuda:0')\n",
            "3 tensor(2.8343, device='cuda:0')\n",
            "4 tensor(2.9242, device='cuda:0')\n",
            "5 tensor(2.8963, device='cuda:0')\n",
            "6 tensor(3.1037, device='cuda:0')\n",
            "7 tensor(2.8479, device='cuda:0')\n",
            "8 tensor(3.0067, device='cuda:0')\n",
            "9 tensor(2.8770, device='cuda:0')\n",
            "10 tensor(3.0304, device='cuda:0')\n",
            "11 tensor(2.8544, device='cuda:0')\n",
            "12 tensor(2.8019, device='cuda:0')\n",
            "13 tensor(2.9889, device='cuda:0')\n",
            "14 tensor(2.8037, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vdYcUYIxIio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = CorpusDataset(val_src_corpus, val_trg_corpus, seq_len=SEQ_LEN)\n",
        "test_dataloader = DataLoader(test_dataset,batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aIr6V7PK2XF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict():\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for step , (src, tgt) in enumerate(test_dataloader):\n",
        "      if step > 10:\n",
        "        break\n",
        "      #outputs = model(src, tgt[:, :-1])\n",
        "      outputs = model.predict(src)\n",
        "      tokens = torch.argmax(outputs, dim=-1)\n",
        "      sents = []\n",
        "      for t in tokens:\n",
        "        sent = tokens2words(t, test_dataset, test_dataset.trg_vocab)\n",
        "        sents.append(sent)\n",
        "      for sent, tgt in zip(sents, tgt.cpu().numpy()):\n",
        "        print(step)\n",
        "        print('predition: ', sent)\n",
        "        target = tokens2words(tgt[1:], test_dataset, test_dataset.trg_vocab)\n",
        "        print('target   : ', target)\n",
        "        print()\n",
        "        break\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3a3f5Yt2VY7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "outputId": "35380ae1-72ac-45ea-e2c1-21dc1b8bb645"
      },
      "source": [
        "predict()\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "predition:  A man in a An and an white The at the middle yellow guitar. camera. all <eos>\n",
            "target   :  A young man in a green sweatshirt reads a newspaper on the beach. <eos>\n",
            "\n",
            "1\n",
            "predition:  A suit and a shorts, woman men up a side <eos>\n",
            "target   :  A bride and a groom at their wedding kissing <eos>\n",
            "\n",
            "2\n",
            "predition:  A blond during is ceiling. in the making of a brick <eos>\n",
            "target   :  A brown, black, and white dog barking up a tree. <eos>\n",
            "\n",
            "3\n",
            "predition:  A from Twp is floor Cami looking a shirt and shirt and an single in the wall <eos>\n",
            "target   :  There are multiple people going over a loop on an inverted roller coaster. <eos>\n",
            "\n",
            "4\n",
            "predition:  A black sitting men in front of a performing ready in front of a fallen stage <eos>\n",
            "target   :  Two men converse near a wall with graffiti on it. <eos>\n",
            "\n",
            "5\n",
            "predition:  A blond during is field. a barefoot while guitar. is The street of a circular <eos>\n",
            "target   :  A yellow dog carries a ball in its mouth on the beach. <eos>\n",
            "\n",
            "6\n",
            "predition:  A baseball during and talking in a city being woman snow. a luggage. in a drives <eos>\n",
            "target   :  A son and his parents are taking a group picture in a church. <eos>\n",
            "\n",
            "7\n",
            "predition:  A posing group in young top with young watches on the bowl <eos>\n",
            "target   :  Three kids wearing brown shirts and jeans jumping outdoors with leaves on the ground. <eos>\n",
            "\n",
            "8\n",
            "predition:  A appears is in a top of green and shirt and shirt is The at the green smiles <eos>\n",
            "target   :  A motel valet man wearing a trench coat pushing a load of luggage. <eos>\n",
            "\n",
            "9\n",
            "predition:  A girl with a shirt and shirt girl on adults paper is other into the purple <eos>\n",
            "target   :  A barefoot boy with a blue and white striped towel is standing on the beach. <eos>\n",
            "\n",
            "10\n",
            "predition:  A red of wearing in a above with two bench and people two drinks <eos>\n",
            "target   :  A group of people wait behind a barricade and look toward the right. <eos>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}